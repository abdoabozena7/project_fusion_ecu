{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f097d8",
   "metadata": {},
   "source": [
    "# 05 – Model Training and Ensemble Learning\n",
    "\n",
    "In this notebook we train multiple models on the preprocessed data and compare their performance.  We include logistic regression, random forest, gradient boosting.  We then build ensemble models such as stacking and voting using scikit‑learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436e891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_full: (40000, 14) sparse: True\n",
      "X_test: (10000, 14) sparse: True\n",
      "dense_allowed: True\n",
      "\n",
      "Training model 1: LogisticRegression\n",
      "Model LogisticRegression training finished\n",
      "[SAVED] d:\\projects\\Ai\\project_fusion_ecu\\models\\LogisticRegression_model.pkl\n",
      "\n",
      "Training model 2: RandomForest\n",
      "Model RandomForest training finished\n",
      "[SAVED] d:\\projects\\Ai\\project_fusion_ecu\\models\\RandomForest_model.pkl\n",
      "\n",
      "Training model 3: ExtraTrees\n",
      "Model ExtraTrees training finished\n",
      "[SAVED] d:\\projects\\Ai\\project_fusion_ecu\\models\\ExtraTrees_model.pkl\n",
      "\n",
      "Training model 4: Bagging_LR\n",
      "Model Bagging_LR training finished\n",
      "[SAVED] d:\\projects\\Ai\\project_fusion_ecu\\models\\Bagging_LR_model.pkl\n",
      "\n",
      "Training model 5: HistGradientBoosting\n",
      "Model HistGradientBoosting training finished\n",
      "[SAVED] d:\\projects\\Ai\\project_fusion_ecu\\models\\HistGradientBoosting_model.pkl\n",
      "\n",
      "Training model Voting_Soft\n",
      "Model Voting_Soft training finished\n",
      "[SAVED] d:\\projects\\Ai\\project_fusion_ecu\\models\\Voting_Soft_model.pkl\n",
      "\n",
      "Training model Stacking\n",
      "Model Stacking training finished\n",
      "[SAVED] d:\\projects\\Ai\\project_fusion_ecu\\models\\Stacking_model.pkl\n",
      "\n",
      "[VAL] BestThr_F1=0.4089  F1=0.7137\n",
      "[SAVED] d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\best_threshold.json\n",
      "\n",
      "Refit final model on full train...\n",
      "Final model refit finished\n",
      "\n",
      "Training model Stacking_Calibrated\n",
      "Model Stacking_Calibrated training finished\n",
      "[SAVED] d:\\projects\\Ai\\project_fusion_ecu\\models\\Stacking_Calibrated_model.pkl\n",
      "[SAVED] d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\feature_count.json n_features= 14\n",
      "\n",
      "DONE. Models folder should now contain multiple .pkl files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    BaggingClassifier,\n",
    "    VotingClassifier,\n",
    "    StackingClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# Utils\n",
    "def safe_predict_proba(model, X):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        p = model.predict_proba(X)\n",
    "        return p[:, 1]\n",
    "    scores = model.decision_function(X)\n",
    "    return 1.0 / (1.0 + np.exp(-scores))\n",
    "\n",
    "\n",
    "def can_dense(X):\n",
    "    rows, cols = X.shape\n",
    "    # float32 ~4 bytes\n",
    "    return rows * cols * 4 < 2.5e9\n",
    "\n",
    "\n",
    "def save_model(obj, path):\n",
    "    joblib.dump(obj, path)\n",
    "    print(\"[SAVED]\", path)\n",
    "\n",
    "\n",
    "def best_f1_threshold(y_true, p, steps=200):\n",
    "    # grid thresholds\n",
    "    thrs = np.linspace(0.01, 0.99, steps)\n",
    "    best_thr, best_f1 = 0.5, -1.0\n",
    "    for t in thrs:\n",
    "        pred = (p >= t).astype(int)\n",
    "        f1 = f1_score(y_true, pred, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thr = float(t)\n",
    "    return best_thr, float(best_f1)\n",
    "\n",
    "\n",
    "# Paths / Load processed data\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "processed_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "models_dir = os.path.join(project_root, \"models\")\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "X_train_full = sparse.load_npz(\n",
    "    os.path.join(processed_dir, \"X_train_processed.npz\")\n",
    ").tocsr()\n",
    "\n",
    "X_test = sparse.load_npz(\n",
    "    os.path.join(processed_dir, \"X_test_processed.npz\")\n",
    ").tocsr()\n",
    "\n",
    "y_train_full = (\n",
    "    pd.read_csv(os.path.join(processed_dir, \"y_train.csv\"))\n",
    "    .squeeze()\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "y_test = (\n",
    "    pd.read_csv(os.path.join(processed_dir, \"y_test.csv\"))\n",
    "    .squeeze()\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "print(\"X_train_full:\", X_train_full.shape, \"sparse:\", sparse.issparse(X_train_full))\n",
    "print(\"X_test:\", X_test.shape, \"sparse:\", sparse.issparse(X_test))\n",
    "\n",
    "# Temporal validation split\n",
    "n = X_train_full.shape[0]\n",
    "split_idx = int(n * 0.8)\n",
    "\n",
    "X_train = X_train_full[:split_idx]\n",
    "y_train = y_train_full.iloc[:split_idx].reset_index(drop=True)\n",
    "\n",
    "X_val = X_train_full[split_idx:]\n",
    "y_val = y_train_full.iloc[split_idx:].reset_index(drop=True)\n",
    "\n",
    "dense_allowed = can_dense(X_train_full)\n",
    "print(\"dense_allowed:\", dense_allowed)\n",
    "\n",
    "if dense_allowed:\n",
    "    X_train_dense = X_train.toarray().astype(np.float32)\n",
    "    X_val_dense = X_val.toarray().astype(np.float32)\n",
    "    X_test_dense = X_test.toarray().astype(np.float32)\n",
    "\n",
    "\n",
    "# Base Models\n",
    "base_models = {\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        max_iter=3000, n_jobs=-1, class_weight=\"balanced\"\n",
    "    ),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "    ),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=800,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\",\n",
    "    ),\n",
    "    \"Bagging_LR\": BaggingClassifier(\n",
    "        estimator=LogisticRegression(\n",
    "            max_iter=3000, n_jobs=-1, class_weight=\"balanced\"\n",
    "        ),\n",
    "        n_estimators=30,\n",
    "        max_samples=0.75,\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(\n",
    "        max_depth=6,\n",
    "        learning_rate=0.06,\n",
    "        max_iter=400,\n",
    "        random_state=42,\n",
    "    ),\n",
    "}\n",
    "\n",
    "requires_dense = {\"HistGradientBoosting\": True}\n",
    "\n",
    "trained = {}\n",
    "input_type = {}\n",
    "\n",
    "# Train base models \n",
    "for i, (name, model) in enumerate(base_models.items(), 1):\n",
    "    print(f\"\\nTraining model {i}: {name}\")\n",
    "\n",
    "    use_dense = requires_dense.get(name, False)\n",
    "    if use_dense and not dense_allowed:\n",
    "        print(f\"[SKIP] {name} requires dense, but dense is not allowed.\")\n",
    "        continue\n",
    "\n",
    "    Xtr = X_train_dense if use_dense else X_train\n",
    "    model.fit(Xtr, y_train)\n",
    "\n",
    "    trained[name] = model\n",
    "    input_type[name] = \"dense\" if use_dense else \"sparse\"\n",
    "    print(f\"Model {name} training finished\")\n",
    "\n",
    "    save_model(model, os.path.join(models_dir, f\"{name}_model.pkl\"))\n",
    "\n",
    "\n",
    "# Ensembles\n",
    "estimators = [(k, v) for k, v in trained.items()]\n",
    "ensemble_input = (\n",
    "    \"dense\" if any(input_type[k] == \"dense\" for k, _ in estimators) else \"sparse\"\n",
    ")\n",
    "Xtr_ens = X_train_dense if ensemble_input == \"dense\" else X_train\n",
    "Xval_ens = X_val_dense if ensemble_input == \"dense\" else X_val\n",
    "\n",
    "print(\"\\nTraining model Voting_Soft\")\n",
    "voting_soft = VotingClassifier(estimators=estimators, voting=\"soft\", n_jobs=-1)\n",
    "voting_soft.fit(Xtr_ens, y_train)\n",
    "print(\"Model Voting_Soft training finished\")\n",
    "save_model(voting_soft, os.path.join(models_dir, \"Voting_Soft_model.pkl\"))\n",
    "\n",
    "print(\"\\nTraining model Stacking\")\n",
    "stacking = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(max_iter=3000, n_jobs=-1, class_weight=\"balanced\"),\n",
    "    stack_method=\"predict_proba\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "stacking.fit(Xtr_ens, y_train)\n",
    "print(\"Model Stacking training finished\")\n",
    "save_model(stacking, os.path.join(models_dir, \"Stacking_model.pkl\"))\n",
    "\n",
    "# Evaluate on VAL to get BestThr_F1 \n",
    "try:\n",
    "    p_val = safe_predict_proba(stacking, Xval_ens)\n",
    "    best_thr, best_f1 = best_f1_threshold(y_val.values, p_val, steps=200)\n",
    "    print(f\"\\n[VAL] BestThr_F1={best_thr:.4f}  F1={best_f1:.4f}\")\n",
    "\n",
    "    best_thr_path = os.path.join(processed_dir, \"best_threshold.json\")\n",
    "    with open(best_thr_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(\n",
    "            {\"model\": \"Stacking\", \"best_threshold_f1\": best_thr, \"best_f1\": best_f1},\n",
    "            f,\n",
    "            ensure_ascii=False,\n",
    "            indent=2\n",
    "        )\n",
    "    print(\"[SAVED]\", best_thr_path)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"[WARN] Could not compute/save best_threshold.json:\", str(e)[:200])\n",
    "\n",
    "\n",
    "# Refit best model on TRAIN+VAL \n",
    "final_model = stacking\n",
    "final_input = ensemble_input\n",
    "\n",
    "X_trainval = X_train_full\n",
    "y_trainval = y_train_full.reset_index(drop=True)\n",
    "\n",
    "X_trainval_in = (\n",
    "    X_trainval.toarray().astype(np.float32)\n",
    "    if final_input == \"dense\"\n",
    "    else X_trainval\n",
    ")\n",
    "\n",
    "print(\"\\nRefit final model on full train...\")\n",
    "final_model.fit(X_trainval_in, y_trainval)\n",
    "print(\"Final model refit finished\")\n",
    "\n",
    "# Calibration on full train\n",
    "print(\"\\nTraining model Stacking_Calibrated\")\n",
    "calibrated = CalibratedClassifierCV(final_model, method=\"sigmoid\", cv=3)\n",
    "calibrated.fit(X_trainval_in, y_trainval)\n",
    "print(\"Model Stacking_Calibrated training finished\")\n",
    "save_model(calibrated, os.path.join(models_dir, \"Stacking_Calibrated_model.pkl\"))\n",
    "\n",
    "# Save meta to help Streamlit avoid feature mismatch\n",
    "try:\n",
    "    feature_count = int(X_train_full.shape[1])  \n",
    "    meta_path = os.path.join(processed_dir, \"feature_count.json\")\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"n_features\": feature_count}, f, indent=2)\n",
    "    print(\"[SAVED]\", meta_path, \"n_features=\", feature_count)\n",
    "except Exception as e:\n",
    "    print(\"[WARN] Could not save feature_count.json:\", str(e)[:200])\n",
    "\n",
    "print(\"\\nDONE. Models folder should now contain multiple .pkl files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9b8a2f",
   "metadata": {},
   "source": [
    "### Modifications Summary\n",
    "This training notebook has been updated to remove dependence on SMOTE-oversampled data.\n",
    "Processed feature matrices (`X_train_processed.npz`, `X_test_processed.npz`) and labels are loaded instead.\n",
    "Class imbalance is addressed via the `scale_pos_weight` parameter on the XGBoost model, computed from the training data.\n",
    "Evaluation metrics now focus exclusively on ROC-AUC, LogLoss, and PR-AUC. Accuracy and F1-score have been removed to provide more informative assessment for imbalanced data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
