{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f097d8",
   "metadata": {},
   "source": [
    "# 05 – Model Training and Ensemble Learning\n",
    "\n",
    "In this notebook we train multiple models on the preprocessed data and compare their performance.  We include logistic regression, random forest, gradient boosting and XGBoost.  We then build ensemble models such as stacking and voting using scikit‑learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5436e891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: d:\\projects\\Ai\\project_fusion_ecu\n",
      "Processed dir: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\n",
      "Models dir: d:\\projects\\Ai\\project_fusion_ecu\\models\n",
      "All required files exist.\n",
      "X_train: (160000, 19) type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "X_test : (40000, 19) type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "y_train: (160000,) pos_rate: 0.04765625\n",
      "y_test : (40000,) pos_rate: 0.0457\n",
      "Loaded scale_pos_weight: 19.9836\n",
      "X_train_dense: (160000, 19) dtype: float32\n",
      "X_test_dense : (40000, 19) dtype: float32\n",
      "\n",
      "Training: LogisticRegression\n",
      "\n",
      "Training: RandomForest\n",
      "\n",
      "Training: ExtraTrees\n",
      "\n",
      "Training: Bagging_LR\n",
      "\n",
      "Training: HistGradientBoosting\n",
      "\n",
      "Training: XGBoost_Tuned\n",
      "\n",
      "Base models performance:\n",
      "                  Model   ROC_AUC   LogLoss    PR_AUC\n",
      "3            Bagging_LR  0.546442  0.184862  0.056800\n",
      "0    LogisticRegression  0.546306  0.184884  0.056705\n",
      "4  HistGradientBoosting  0.552093  0.184951  0.054692\n",
      "5         XGBoost_Tuned  0.539574  0.650461  0.053649\n",
      "2            ExtraTrees  0.535406  0.185486  0.051955\n",
      "1          RandomForest  0.524178  0.188247  0.050326\n",
      "\n",
      "Best-threshold (by F1) summary:\n",
      "                  Model  BestThr_F1  F1_at_best  Precision_at_best  \\\n",
      "4  HistGradientBoosting    0.050743    0.099013           0.058266   \n",
      "3            Bagging_LR    0.052741    0.098476           0.063218   \n",
      "0    LogisticRegression    0.052955    0.097906           0.063627   \n",
      "5         XGBoost_Tuned    0.524795    0.095901           0.057740   \n",
      "2            ExtraTrees    0.049262    0.094536           0.053479   \n",
      "1          RandomForest    0.040662    0.090131           0.048387   \n",
      "\n",
      "   Recall_at_best  \n",
      "4        0.329322  \n",
      "3        0.222648  \n",
      "0        0.212254  \n",
      "5        0.282823  \n",
      "2        0.407002  \n",
      "1        0.656455  \n",
      "\n",
      "Training: Voting_Soft (fusion)\n",
      "\n",
      "Training: Voting_WeightedSoft (fusion)\n",
      "Weighted voting models: ['LogisticRegression', 'Bagging_LR', 'HistGradientBoosting', 'XGBoost_Tuned']\n",
      "Weights (normalized): [0.25560452859431415, 0.2560325376205561, 0.24653198386692177, 0.24183094991820792]\n",
      "\n",
      "Training: Stacking (fusion)\n",
      "\n",
      "All models (base + ensembles) performance:\n",
      "                  Model   ROC_AUC   LogLoss    PR_AUC\n",
      "3            Bagging_LR  0.546442  0.184862  0.056800\n",
      "0    LogisticRegression  0.546306  0.184884  0.056705\n",
      "7   Voting_WeightedSoft  0.544281  0.241319  0.055548\n",
      "6           Voting_Soft  0.544259  0.244188  0.055514\n",
      "4  HistGradientBoosting  0.552093  0.184951  0.054692\n",
      "8              Stacking  0.541503  0.185304  0.054063\n",
      "5         XGBoost_Tuned  0.539574  0.650461  0.053649\n",
      "2            ExtraTrees  0.535406  0.185486  0.051955\n",
      "1          RandomForest  0.524178  0.188247  0.050326\n",
      "\n",
      "All models best-threshold summary:\n",
      "                  Model  BestThr_F1  F1_at_best  Precision_at_best  \\\n",
      "4  HistGradientBoosting    0.050743    0.099013           0.058266   \n",
      "3            Bagging_LR    0.052741    0.098476           0.063218   \n",
      "6           Voting_Soft    0.171889    0.098003           0.061213   \n",
      "0    LogisticRegression    0.052955    0.097906           0.063627   \n",
      "7   Voting_WeightedSoft    0.167785    0.097656           0.060843   \n",
      "5         XGBoost_Tuned    0.524795    0.095901           0.057740   \n",
      "8              Stacking    0.049561    0.094792           0.054113   \n",
      "2            ExtraTrees    0.049262    0.094536           0.053479   \n",
      "1          RandomForest    0.040662    0.090131           0.048387   \n",
      "\n",
      "   Recall_at_best  \n",
      "4        0.329322  \n",
      "3        0.222648  \n",
      "6        0.245624  \n",
      "0        0.212254  \n",
      "7        0.247265  \n",
      "5        0.282823  \n",
      "8        0.381838  \n",
      "2        0.407002  \n",
      "1        0.656455  \n",
      "\n",
      "Best model by PR_AUC: Bagging_LR\n",
      "Best model input type: sparse\n",
      "Training calibrated best model (sigmoid, CV=3)...\n",
      "\n",
      "All models (including calibrated) performance:\n",
      "                   Model   ROC_AUC   LogLoss    PR_AUC\n",
      "3             Bagging_LR  0.546442  0.184862  0.056800\n",
      "9  Bagging_LR_Calibrated  0.547645  0.184917  0.056796\n",
      "0     LogisticRegression  0.546306  0.184884  0.056705\n",
      "7    Voting_WeightedSoft  0.544281  0.241319  0.055548\n",
      "6            Voting_Soft  0.544259  0.244188  0.055514\n",
      "4   HistGradientBoosting  0.552093  0.184951  0.054692\n",
      "8               Stacking  0.541503  0.185304  0.054063\n",
      "5          XGBoost_Tuned  0.539574  0.650461  0.053649\n",
      "2             ExtraTrees  0.535406  0.185486  0.051955\n",
      "1           RandomForest  0.524178  0.188247  0.050326\n",
      "\n",
      "All models best-threshold summary (including calibrated):\n",
      "                   Model  BestThr_F1  F1_at_best  Precision_at_best  \\\n",
      "4   HistGradientBoosting    0.050743    0.099013           0.058266   \n",
      "9  Bagging_LR_Calibrated    0.051528    0.098815           0.062096   \n",
      "3             Bagging_LR    0.052741    0.098476           0.063218   \n",
      "6            Voting_Soft    0.171889    0.098003           0.061213   \n",
      "0     LogisticRegression    0.052955    0.097906           0.063627   \n",
      "7    Voting_WeightedSoft    0.167785    0.097656           0.060843   \n",
      "5          XGBoost_Tuned    0.524795    0.095901           0.057740   \n",
      "8               Stacking    0.049561    0.094792           0.054113   \n",
      "2             ExtraTrees    0.049262    0.094536           0.053479   \n",
      "1           RandomForest    0.040662    0.090131           0.048387   \n",
      "\n",
      "   Recall_at_best  \n",
      "4        0.329322  \n",
      "9        0.241794  \n",
      "3        0.222648  \n",
      "6        0.245624  \n",
      "0        0.212254  \n",
      "7        0.247265  \n",
      "5        0.282823  \n",
      "8        0.381838  \n",
      "2        0.407002  \n",
      "1        0.656455  \n",
      "\n",
      "Precision@K (ranking proxy):\n",
      "                    Model     K  Precision@K\n",
      "0              Bagging_LR   500       0.1020\n",
      "4   Bagging_LR_Calibrated   500       0.1000\n",
      "8      LogisticRegression   500       0.0980\n",
      "16            Voting_Soft   500       0.0940\n",
      "12    Voting_WeightedSoft   500       0.0920\n",
      "20   HistGradientBoosting   500       0.0840\n",
      "28          XGBoost_Tuned   500       0.0840\n",
      "24               Stacking   500       0.0820\n",
      "32             ExtraTrees   500       0.0780\n",
      "36           RandomForest   500       0.0540\n",
      "1              Bagging_LR  1000       0.0890\n",
      "5   Bagging_LR_Calibrated  1000       0.0890\n",
      "9      LogisticRegression  1000       0.0890\n",
      "13    Voting_WeightedSoft  1000       0.0780\n",
      "17            Voting_Soft  1000       0.0780\n",
      "25               Stacking  1000       0.0740\n",
      "29          XGBoost_Tuned  1000       0.0730\n",
      "21   HistGradientBoosting  1000       0.0680\n",
      "33             ExtraTrees  1000       0.0650\n",
      "37           RandomForest  1000       0.0580\n",
      "10     LogisticRegression  2000       0.0760\n",
      "6   Bagging_LR_Calibrated  2000       0.0755\n",
      "2              Bagging_LR  2000       0.0750\n",
      "14    Voting_WeightedSoft  2000       0.0710\n",
      "18            Voting_Soft  2000       0.0705\n",
      "26               Stacking  2000       0.0650\n",
      "22   HistGradientBoosting  2000       0.0620\n",
      "30          XGBoost_Tuned  2000       0.0615\n",
      "38           RandomForest  2000       0.0595\n",
      "34             ExtraTrees  2000       0.0570\n",
      "3              Bagging_LR  5000       0.0652\n",
      "7   Bagging_LR_Calibrated  5000       0.0644\n",
      "11     LogisticRegression  5000       0.0638\n",
      "23   HistGradientBoosting  5000       0.0614\n",
      "19            Voting_Soft  5000       0.0610\n",
      "15    Voting_WeightedSoft  5000       0.0606\n",
      "27               Stacking  5000       0.0586\n",
      "31          XGBoost_Tuned  5000       0.0584\n",
      "35             ExtraTrees  5000       0.0564\n",
      "39           RandomForest  5000       0.0564\n",
      "Saved LogisticRegression model to: d:\\projects\\Ai\\project_fusion_ecu\\models\\LogisticRegression_model.pkl\n",
      "Saved RandomForest model to: d:\\projects\\Ai\\project_fusion_ecu\\models\\RandomForest_model.pkl\n",
      "Saved ExtraTrees model to: d:\\projects\\Ai\\project_fusion_ecu\\models\\ExtraTrees_model.pkl\n",
      "Saved Bagging_LR model to: d:\\projects\\Ai\\project_fusion_ecu\\models\\Bagging_LR_model.pkl\n",
      "Saved HistGradientBoosting model to: d:\\projects\\Ai\\project_fusion_ecu\\models\\HistGradientBoosting_model.pkl\n",
      "Saved XGBoost_Tuned model to: d:\\projects\\Ai\\project_fusion_ecu\\models\\XGBoost_Tuned_model.pkl\n",
      "Saved Voting_Soft model to: d:\\projects\\Ai\\project_fusion_ecu\\models\\Voting_Soft_model.pkl\n",
      "Saved Voting_WeightedSoft model to: d:\\projects\\Ai\\project_fusion_ecu\\models\\Voting_WeightedSoft_model.pkl\n",
      "Saved Stacking model to: d:\\projects\\Ai\\project_fusion_ecu\\models\\Stacking_model.pkl\n",
      "Saved Bagging_LR_Calibrated model to: d:\\projects\\Ai\\project_fusion_ecu\\models\\Bagging_LR_Calibrated_model.pkl\n",
      "[Training + ensembles (including weighted voting) + calibration completed successfully.]\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 05_model_training.ipynb  (Model Training + Ensembles)\n",
    "# - Base models\n",
    "# - Bagging (LR)  -> Bagging\n",
    "# - Boosting (HistGB + XGB) -> Boosting\n",
    "# - Soft Voting + Weighted Soft Voting (fusion)\n",
    "# - Stacking (fusion)\n",
    "# - Calibration for best PR_AUC\n",
    "# ============================================\n",
    "\n",
    "# ============================================\n",
    "# 0. Imports and configuration\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    BaggingClassifier,\n",
    "    VotingClassifier,\n",
    "    StackingClassifier,\n",
    ")\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    log_loss,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 1. Load processed data and labels\n",
    "# ============================================\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "processed_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "models_dir = os.path.join(project_root, \"models\")\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"Processed dir:\", processed_dir)\n",
    "print(\"Models dir:\", models_dir)\n",
    "\n",
    "required = [\n",
    "    \"X_train_processed.npz\",\n",
    "    \"X_test_processed.npz\",\n",
    "    \"y_train.csv\",\n",
    "    \"y_test.csv\",\n",
    "    \"preprocessor.joblib\",\n",
    "    \"scale_pos_weight.txt\",\n",
    "]\n",
    "\n",
    "for fname in required:\n",
    "    p = os.path.join(processed_dir, fname)\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Required file not found: {p}\")\n",
    "print(\"All required files exist.\")\n",
    "\n",
    "X_train = sparse.load_npz(os.path.join(processed_dir, \"X_train_processed.npz\")).tocsr()\n",
    "X_test  = sparse.load_npz(os.path.join(processed_dir, \"X_test_processed.npz\")).tocsr()\n",
    "y_train = pd.read_csv(os.path.join(processed_dir, \"y_train.csv\")).squeeze().astype(int)\n",
    "y_test  = pd.read_csv(os.path.join(processed_dir, \"y_test.csv\")).squeeze().astype(int)\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"type:\", type(X_train))\n",
    "print(\"X_test :\", X_test.shape,  \"type:\", type(X_test))\n",
    "print(\"y_train:\", y_train.shape, \"pos_rate:\", float(y_train.mean()))\n",
    "print(\"y_test :\", y_test.shape,  \"pos_rate:\", float(y_test.mean()))\n",
    "\n",
    "with open(os.path.join(processed_dir, \"scale_pos_weight.txt\"), \"r\") as f:\n",
    "    scale_pos_weight = float(f.read().strip())\n",
    "print(f\"Loaded scale_pos_weight: {scale_pos_weight:.4f}\")\n",
    "\n",
    "# Prepare dense copies (needed for HistGradientBoosting and any ensemble including it)\n",
    "# Use float32 to reduce memory\n",
    "X_train_dense = X_train.toarray().astype(np.float32)\n",
    "X_test_dense  = X_test.toarray().astype(np.float32)\n",
    "\n",
    "print(\"X_train_dense:\", X_train_dense.shape, \"dtype:\", X_train_dense.dtype)\n",
    "print(\"X_test_dense :\", X_test_dense.shape,  \"dtype:\", X_test_dense.dtype)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 2. Utility: evaluation helpers\n",
    "# ============================================\n",
    "\n",
    "def evaluate_probas(y_true, probas, name=\"Model\"):\n",
    "    auc = roc_auc_score(y_true, probas)\n",
    "    ll = log_loss(y_true, probas)\n",
    "    pr_auc = average_precision_score(y_true, probas)\n",
    "    return {\"Model\": name, \"ROC_AUC\": float(auc), \"LogLoss\": float(ll), \"PR_AUC\": float(pr_auc)}\n",
    "\n",
    "def best_threshold_by_f1(y_true, probas):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, probas)\n",
    "    precision = precision[:-1]\n",
    "    recall = recall[:-1]\n",
    "    f1 = (2 * precision * recall) / (precision + recall + 1e-12)\n",
    "    best_idx = int(np.nanargmax(f1))\n",
    "    return float(thresholds[best_idx]), float(f1[best_idx]), float(precision[best_idx]), float(recall[best_idx])\n",
    "\n",
    "def summarize_threshold_metrics(y_true, probas, name=\"Model\"):\n",
    "    thr, best_f1, p, r = best_threshold_by_f1(y_true, probas)\n",
    "    preds = (probas >= thr).astype(int)\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"BestThr_F1\": thr,\n",
    "        \"F1_at_best\": best_f1,\n",
    "        \"Precision_at_best\": float(precision_score(y_true, preds, zero_division=0)),\n",
    "        \"Recall_at_best\": float(recall_score(y_true, preds, zero_division=0)),\n",
    "    }\n",
    "\n",
    "def precision_at_k(y_true, probas, k):\n",
    "    k = int(k)\n",
    "    idx = np.argsort(-probas)[:k]\n",
    "    if hasattr(y_true, \"iloc\"):\n",
    "        return float(y_true.iloc[idx].mean())\n",
    "    return float(np.mean(y_true[idx]))\n",
    "\n",
    "def safe_predict_proba(model, X):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        scores = model.decision_function(X)\n",
    "        return 1.0 / (1.0 + np.exp(-scores))\n",
    "    raise AttributeError(f\"Model {model.__class__.__name__} has no predict_proba/decision_function.\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 3. Define base models (Bagging + Boosting included)\n",
    "# ============================================\n",
    "\n",
    "base_models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=3000, n_jobs=-1),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=350,\n",
    "        max_depth=None,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=600,\n",
    "        max_depth=None,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    "    \"Bagging_LR\": BaggingClassifier(\n",
    "        estimator=LogisticRegression(max_iter=3000, n_jobs=-1),\n",
    "        n_estimators=25,\n",
    "        max_samples=0.75,\n",
    "        max_features=1.0,\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(\n",
    "        max_depth=6,\n",
    "        learning_rate=0.08,\n",
    "        max_iter=300,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    \"XGBoost_Tuned\": XGBClassifier(\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=1.0,\n",
    "        gamma=0.0,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        tree_method=\"hist\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Which models require dense input\n",
    "requires_dense = {\n",
    "    \"HistGradientBoosting\": True,\n",
    "}\n",
    "# default False for others\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 4. Train and evaluate base models\n",
    "# ============================================\n",
    "\n",
    "results = []\n",
    "threshold_results = []\n",
    "trained_models = {}\n",
    "model_input_type = {}  # \"sparse\" or \"dense\"\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    print(f\"\\nTraining: {name}\")\n",
    "\n",
    "    use_dense = bool(requires_dense.get(name, False))\n",
    "    Xtr = X_train_dense if use_dense else X_train\n",
    "    Xte = X_test_dense  if use_dense else X_test\n",
    "\n",
    "    model.fit(Xtr, y_train)\n",
    "    probas = safe_predict_proba(model, Xte)\n",
    "\n",
    "    results.append(evaluate_probas(y_test, probas, name=name))\n",
    "    threshold_results.append(summarize_threshold_metrics(y_test, probas, name=name))\n",
    "\n",
    "    trained_models[name] = model\n",
    "    model_input_type[name] = \"dense\" if use_dense else \"sparse\"\n",
    "\n",
    "base_df = pd.DataFrame(results).sort_values(\"PR_AUC\", ascending=False)\n",
    "thr_df = pd.DataFrame(threshold_results).sort_values(\"F1_at_best\", ascending=False)\n",
    "\n",
    "print(\"\\nBase models performance:\")\n",
    "print(base_df)\n",
    "\n",
    "print(\"\\nBest-threshold (by F1) summary:\")\n",
    "print(thr_df)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 5. Fusion: Soft Voting + Weighted Soft Voting\n",
    "# ============================================\n",
    "\n",
    "voting_candidates = [\"LogisticRegression\", \"Bagging_LR\", \"HistGradientBoosting\", \"XGBoost_Tuned\"]\n",
    "\n",
    "estimators = []\n",
    "for mname in voting_candidates:\n",
    "    if mname in trained_models:\n",
    "        estimators.append((mname, trained_models[mname]))\n",
    "\n",
    "# If any estimator needs dense, train the whole ensemble on dense\n",
    "ensemble_needs_dense = any(model_input_type.get(mname, \"sparse\") == \"dense\" for mname, _ in estimators)\n",
    "Xtr_ens = X_train_dense if ensemble_needs_dense else X_train\n",
    "Xte_ens = X_test_dense  if ensemble_needs_dense else X_test\n",
    "\n",
    "# 5.1 Soft Voting (equal weights)\n",
    "print(\"\\nTraining: Voting_Soft (fusion)\")\n",
    "voting_soft = VotingClassifier(\n",
    "    estimators=estimators,\n",
    "    voting=\"soft\",\n",
    "    weights=None,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "voting_soft.fit(Xtr_ens, y_train)\n",
    "probas_vote = safe_predict_proba(voting_soft, Xte_ens)\n",
    "\n",
    "results.append(evaluate_probas(y_test, probas_vote, name=\"Voting_Soft\"))\n",
    "threshold_results.append(summarize_threshold_metrics(y_test, probas_vote, name=\"Voting_Soft\"))\n",
    "trained_models[\"Voting_Soft\"] = voting_soft\n",
    "model_input_type[\"Voting_Soft\"] = \"dense\" if ensemble_needs_dense else \"sparse\"\n",
    "\n",
    "# 5.2 Weighted Soft Voting (weights derived from PR_AUC of base models)\n",
    "print(\"\\nTraining: Voting_WeightedSoft (fusion)\")\n",
    "\n",
    "pr_map = {row[\"Model\"]: float(row[\"PR_AUC\"]) for _, row in base_df.iterrows()}\n",
    "\n",
    "raw_weights = []\n",
    "used_names = []\n",
    "for mname, _ in estimators:\n",
    "    w = pr_map.get(mname, 0.0)\n",
    "    raw_weights.append(w)\n",
    "    used_names.append(mname)\n",
    "\n",
    "raw_weights = np.array(raw_weights, dtype=float)\n",
    "\n",
    "if np.all(raw_weights <= 0):\n",
    "    weights = None\n",
    "    print(\"WARNING: All computed weights were zero; fallback to equal weights.\")\n",
    "else:\n",
    "    raw_weights = raw_weights + 1e-6\n",
    "    weights = (raw_weights / raw_weights.sum()).tolist()\n",
    "\n",
    "print(\"Weighted voting models:\", used_names)\n",
    "print(\"Weights (normalized):\", weights)\n",
    "\n",
    "voting_weighted = VotingClassifier(\n",
    "    estimators=estimators,\n",
    "    voting=\"soft\",\n",
    "    weights=weights,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "voting_weighted.fit(Xtr_ens, y_train)\n",
    "probas_vote_w = safe_predict_proba(voting_weighted, Xte_ens)\n",
    "\n",
    "results.append(evaluate_probas(y_test, probas_vote_w, name=\"Voting_WeightedSoft\"))\n",
    "threshold_results.append(summarize_threshold_metrics(y_test, probas_vote_w, name=\"Voting_WeightedSoft\"))\n",
    "trained_models[\"Voting_WeightedSoft\"] = voting_weighted\n",
    "model_input_type[\"Voting_WeightedSoft\"] = \"dense\" if ensemble_needs_dense else \"sparse\"\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 6. Fusion: Stacking\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nTraining: Stacking (fusion)\")\n",
    "stacking = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(max_iter=3000, n_jobs=-1),\n",
    "    stack_method=\"predict_proba\",\n",
    "    passthrough=False,\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    ")\n",
    "stacking.fit(Xtr_ens, y_train)\n",
    "probas_stack = safe_predict_proba(stacking, Xte_ens)\n",
    "\n",
    "results.append(evaluate_probas(y_test, probas_stack, name=\"Stacking\"))\n",
    "threshold_results.append(summarize_threshold_metrics(y_test, probas_stack, name=\"Stacking\"))\n",
    "trained_models[\"Stacking\"] = stacking\n",
    "model_input_type[\"Stacking\"] = \"dense\" if ensemble_needs_dense else \"sparse\"\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 7. Summaries\n",
    "# ============================================\n",
    "\n",
    "all_df = pd.DataFrame(results).sort_values(\"PR_AUC\", ascending=False)\n",
    "all_thr_df = pd.DataFrame(threshold_results).sort_values(\"F1_at_best\", ascending=False)\n",
    "\n",
    "print(\"\\nAll models (base + ensembles) performance:\")\n",
    "print(all_df)\n",
    "\n",
    "print(\"\\nAll models best-threshold summary:\")\n",
    "print(all_thr_df)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 8. Calibrate best model by PR_AUC\n",
    "# ============================================\n",
    "\n",
    "best_name = all_df.iloc[0][\"Model\"]\n",
    "best_model = trained_models[best_name]\n",
    "best_input = model_input_type.get(best_name, \"sparse\")\n",
    "\n",
    "print(f\"\\nBest model by PR_AUC: {best_name}\")\n",
    "print(f\"Best model input type: {best_input}\")\n",
    "\n",
    "Xtr_cal = X_train_dense if best_input == \"dense\" else X_train\n",
    "Xte_cal = X_test_dense  if best_input == \"dense\" else X_test\n",
    "\n",
    "print(\"Training calibrated best model (sigmoid, CV=3)...\")\n",
    "calibrated = CalibratedClassifierCV(best_model, method=\"sigmoid\", cv=3)\n",
    "calibrated.fit(Xtr_cal, y_train)\n",
    "probas_cal = calibrated.predict_proba(Xte_cal)[:, 1]\n",
    "\n",
    "cal_name = f\"{best_name}_Calibrated\"\n",
    "results.append(evaluate_probas(y_test, probas_cal, name=cal_name))\n",
    "threshold_results.append(summarize_threshold_metrics(y_test, probas_cal, name=cal_name))\n",
    "trained_models[cal_name] = calibrated\n",
    "model_input_type[cal_name] = best_input\n",
    "\n",
    "all_df2 = pd.DataFrame(results).sort_values(\"PR_AUC\", ascending=False)\n",
    "all_thr_df2 = pd.DataFrame(threshold_results).sort_values(\"F1_at_best\", ascending=False)\n",
    "\n",
    "print(\"\\nAll models (including calibrated) performance:\")\n",
    "print(all_df2)\n",
    "\n",
    "print(\"\\nAll models best-threshold summary (including calibrated):\")\n",
    "print(all_thr_df2)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 9. Optional: Precision@K ranking proxy\n",
    "# ============================================\n",
    "\n",
    "Ks = [500, 1000, 2000, 5000]\n",
    "rank_rows = []\n",
    "\n",
    "for name in all_df2[\"Model\"].tolist():\n",
    "    m = trained_models.get(name)\n",
    "    if m is None:\n",
    "        continue\n",
    "\n",
    "    inp = model_input_type.get(name, \"sparse\")\n",
    "    Xte_rank = X_test_dense if inp == \"dense\" else X_test\n",
    "\n",
    "    if name == cal_name:\n",
    "        probas = probas_cal\n",
    "    else:\n",
    "        probas = safe_predict_proba(m, Xte_rank)\n",
    "\n",
    "    for k in Ks:\n",
    "        k = min(k, len(y_test))\n",
    "        rank_rows.append({\n",
    "            \"Model\": name,\n",
    "            \"K\": k,\n",
    "            \"Precision@K\": precision_at_k(y_test, probas, k),\n",
    "        })\n",
    "\n",
    "rank_df = pd.DataFrame(rank_rows)\n",
    "print(\"\\nPrecision@K (ranking proxy):\")\n",
    "print(rank_df.sort_values([\"K\", \"Precision@K\"], ascending=[True, False]))\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 10. Save models\n",
    "# ============================================\n",
    "\n",
    "save_list = [\n",
    "    \"LogisticRegression\",\n",
    "    \"RandomForest\",\n",
    "    \"ExtraTrees\",\n",
    "    \"Bagging_LR\",\n",
    "    \"HistGradientBoosting\",\n",
    "    \"XGBoost_Tuned\",\n",
    "    \"Voting_Soft\",\n",
    "    \"Voting_WeightedSoft\",\n",
    "    \"Stacking\",\n",
    "    cal_name,\n",
    "]\n",
    "\n",
    "for name in save_list:\n",
    "    model = trained_models.get(name)\n",
    "    if model is None:\n",
    "        continue\n",
    "    path = os.path.join(models_dir, f\"{name}_model.pkl\")\n",
    "    joblib.dump(model, path)\n",
    "    print(f\"Saved {name} model to: {path}\")\n",
    "\n",
    "print(\"[Training + ensembles (including weighted voting) + calibration completed successfully.]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9b8a2f",
   "metadata": {},
   "source": [
    "### Modifications Summary\n",
    "This training notebook has been updated to remove dependence on SMOTE-oversampled data.\n",
    "Processed feature matrices (`X_train_processed.npz`, `X_test_processed.npz`) and labels are loaded instead.\n",
    "Class imbalance is addressed via the `scale_pos_weight` parameter on the XGBoost model, computed from the training data.\n",
    "Evaluation metrics now focus exclusively on ROC-AUC, LogLoss, and PR-AUC. Accuracy and F1-score have been removed to provide more informative assessment for imbalanced data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
