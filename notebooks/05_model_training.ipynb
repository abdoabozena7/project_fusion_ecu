{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f097d8",
   "metadata": {},
   "source": [
    "# 05 – Model Training and Ensemble Learning\n",
    "\n",
    "In this notebook we train multiple models on the preprocessed data and compare their performance.  We include logistic regression, random forest, gradient boosting and XGBoost.  We then build ensemble models such as stacking and voting using scikit‑learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5436e891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_full: (40000, 29)\n",
      "X_test      : (10000, 29)\n",
      "pos_rate train_full: 0.051525 test: 0.0506\n",
      "scale_pos_weight: 18.40805434255216\n",
      "\n",
      "Temporal VAL split:\n",
      "X_train: (32000, 29) pos_rate: 0.0515625\n",
      "X_val  : (8000, 29) pos_rate: 0.051375\n",
      "\n",
      "Dense conversion allowed: True\n",
      "\n",
      "Training (Temporal VAL): LogisticRegression\n",
      "\n",
      "Training (Temporal VAL): RandomForest\n",
      "\n",
      "Training (Temporal VAL): ExtraTrees\n",
      "\n",
      "Training (Temporal VAL): Bagging_LR\n",
      "\n",
      "Training (Temporal VAL): HistGradientBoosting\n",
      "\n",
      "Training (Temporal VAL): XGBoost_Tuned\n",
      "\n",
      "VAL performance (temporal):\n",
      "                  Model   ROC_AUC   LogLoss    PR_AUC\n",
      "4  HistGradientBoosting  0.550186  0.201382  0.067160\n",
      "1          RandomForest  0.552556  0.325918  0.060533\n",
      "5         XGBoost_Tuned  0.523871  0.294011  0.059590\n",
      "2            ExtraTrees  0.524061  0.525319  0.056851\n",
      "0    LogisticRegression  0.490578  0.676817  0.049854\n",
      "3            Bagging_LR  0.492193  0.671473  0.049130\n",
      "\n",
      "VAL best-threshold summary:\n",
      "                  Model  BestThr_F1  F1_at_best  Precision_at_best  \\\n",
      "1          RandomForest    0.225024    0.110994           0.062259   \n",
      "4  HistGradientBoosting    0.056328    0.110874           0.070990   \n",
      "5         XGBoost_Tuned    0.016219    0.102594           0.054545   \n",
      "2            ExtraTrees    0.435441    0.101471           0.059766   \n",
      "3            Bagging_LR    0.436707    0.099328           0.052342   \n",
      "0    LogisticRegression    0.432766    0.099155           0.052204   \n",
      "\n",
      "   Recall_at_best  \n",
      "1        0.510949  \n",
      "4        0.253041  \n",
      "5        0.861314  \n",
      "2        0.335766  \n",
      "3        0.970803  \n",
      "0        0.985401  \n",
      "\n",
      "Selected best by VAL PR_AUC: HistGradientBoosting | input: dense\n",
      "\n",
      "Training ensemble: Voting_Soft (Temporal VAL)\n",
      "Voting models: ['LogisticRegression', 'Bagging_LR', 'HistGradientBoosting', 'XGBoost_Tuned']\n",
      "Weights: [0.22085434996136705, 0.21764697440138164, 0.29751696729134003, 0.26398170834591134]\n",
      "\n",
      "Training ensemble: Voting_WeightedSoft (Temporal VAL)\n",
      "\n",
      "Training ensemble: Stacking (Temporal VAL)\n",
      "\n",
      "All models (base + ensembles) VAL performance:\n",
      "                  Model   ROC_AUC   LogLoss    PR_AUC\n",
      "8              Stacking  0.555242  0.698018  0.067246\n",
      "4  HistGradientBoosting  0.550186  0.201382  0.067160\n",
      "7   Voting_WeightedSoft  0.522836  0.361992  0.061724\n",
      "6           Voting_Soft  0.521464  0.390983  0.061366\n",
      "1          RandomForest  0.552556  0.325918  0.060533\n",
      "5         XGBoost_Tuned  0.523871  0.294011  0.059590\n",
      "2            ExtraTrees  0.524061  0.525319  0.056851\n",
      "0    LogisticRegression  0.490578  0.676817  0.049854\n",
      "3            Bagging_LR  0.492193  0.671473  0.049130\n",
      "\n",
      "FINAL selected (by temporal VAL PR_AUC): Stacking | input: dense\n",
      "\n",
      "Refitting best model on TRAIN+VAL then evaluating on TEST...\n",
      "\n",
      "TEST metrics (honest):\n",
      "           Model   ROC_AUC   LogLoss    PR_AUC\n",
      "0  Stacking_TEST  0.498384  0.691393  0.051633\n",
      "\n",
      "TEST best-threshold summary:\n",
      "           Model  BestThr_F1  F1_at_best  Precision_at_best  Recall_at_best\n",
      "0  Stacking_TEST    0.495425    0.097645           0.053006        0.618577\n",
      "\n",
      "Calibrating best model (sigmoid, CV=3) on TRAIN+VAL only...\n",
      "\n",
      "Calibrated TEST metrics:\n",
      "                      Model   ROC_AUC  LogLoss    PR_AUC\n",
      "0  Stacking_Calibrated_TEST  0.507988  0.20032  0.052728\n",
      "\n",
      "Calibrated TEST best-threshold summary:\n",
      "                      Model  BestThr_F1  F1_at_best  Precision_at_best  \\\n",
      "0  Stacking_Calibrated_TEST    0.049853    0.098125           0.052114   \n",
      "\n",
      "   Recall_at_best  \n",
      "0        0.837945  \n",
      "\n",
      "Saved best model: d:\\projects\\Ai\\project_fusion_ecu\\models\\Stacking_model.pkl\n",
      "Saved calibrated model: d:\\projects\\Ai\\project_fusion_ecu\\models\\Stacking_Calibrated_model.pkl\n",
      "\n",
      "[Training completed successfully - temporal VAL selection + honest TEST evaluation.]\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 05_model_training_temporal_val.ipynb\n",
    "# - Temporal VAL split (mimics future TEST)\n",
    "# - Robust XGBoost early stopping (old/new versions)\n",
    "# - Base models + Voting + Stacking + Calibration\n",
    "# - Honest TEST evaluation\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    BaggingClassifier,\n",
    "    VotingClassifier,\n",
    "    StackingClassifier,\n",
    ")\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    log_loss,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def evaluate_probas(y_true, probas, name=\"Model\"):\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"ROC_AUC\": float(roc_auc_score(y_true, probas)),\n",
    "        \"LogLoss\": float(log_loss(y_true, probas)),\n",
    "        \"PR_AUC\": float(average_precision_score(y_true, probas)),\n",
    "    }\n",
    "\n",
    "def best_threshold_by_f1(y_true, probas):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, probas)\n",
    "    precision = precision[:-1]\n",
    "    recall = recall[:-1]\n",
    "    f1 = (2 * precision * recall) / (precision + recall + 1e-12)\n",
    "    best_idx = int(np.nanargmax(f1))\n",
    "    return float(thresholds[best_idx]), float(f1[best_idx]), float(precision[best_idx]), float(recall[best_idx])\n",
    "\n",
    "def summarize_threshold_metrics(y_true, probas, name=\"Model\"):\n",
    "    thr, best_f1, p, r = best_threshold_by_f1(y_true, probas)\n",
    "    preds = (probas >= thr).astype(int)\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"BestThr_F1\": thr,\n",
    "        \"F1_at_best\": best_f1,\n",
    "        \"Precision_at_best\": float(precision_score(y_true, preds, zero_division=0)),\n",
    "        \"Recall_at_best\": float(recall_score(y_true, preds, zero_division=0)),\n",
    "    }\n",
    "\n",
    "def safe_predict_proba(model, X):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        p = model.predict_proba(X)\n",
    "        if p.ndim == 2 and p.shape[1] >= 2:\n",
    "            return p[:, 1]\n",
    "        return p.reshape(-1)\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        scores = model.decision_function(X)\n",
    "        return 1.0 / (1.0 + np.exp(-scores))\n",
    "    raise AttributeError(f\"{model.__class__.__name__} has no predict_proba/decision_function.\")\n",
    "\n",
    "def xgb_fit_with_early_stop_robust(model, Xtr, ytr, Xva, yva):\n",
    "    \"\"\"\n",
    "    Compatibility layer for XGBoost:\n",
    "    - Some versions don't accept early_stopping_rounds in sklearn API.\n",
    "    - We'll try callbacks if available, else fit without early stopping.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "        # callbacks exists in most versions\n",
    "        cb = [xgb.callback.EarlyStopping(rounds=200, save_best=True)]\n",
    "        model.fit(Xtr, ytr, eval_set=[(Xva, yva)], verbose=False, callbacks=cb)\n",
    "        return\n",
    "    except Exception:\n",
    "        # fallback: plain fit\n",
    "        model.fit(Xtr, ytr)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load processed data\n",
    "# -----------------------------\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "processed_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "models_dir = os.path.join(project_root, \"models\")\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "required = [\n",
    "    \"X_train_processed.npz\",\n",
    "    \"X_test_processed.npz\",\n",
    "    \"y_train.csv\",\n",
    "    \"y_test.csv\",\n",
    "    \"scale_pos_weight.txt\",\n",
    "]\n",
    "\n",
    "for fname in required:\n",
    "    p = os.path.join(processed_dir, fname)\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Required file not found: {p}\")\n",
    "\n",
    "X_train_full = sparse.load_npz(os.path.join(processed_dir, \"X_train_processed.npz\")).tocsr()\n",
    "X_test = sparse.load_npz(os.path.join(processed_dir, \"X_test_processed.npz\")).tocsr()\n",
    "y_train_full = pd.read_csv(os.path.join(processed_dir, \"y_train.csv\")).squeeze().astype(int)\n",
    "y_test = pd.read_csv(os.path.join(processed_dir, \"y_test.csv\")).squeeze().astype(int)\n",
    "\n",
    "with open(os.path.join(processed_dir, \"scale_pos_weight.txt\"), \"r\") as f:\n",
    "    scale_pos_weight = float(f.read().strip())\n",
    "\n",
    "print(\"X_train_full:\", X_train_full.shape)\n",
    "print(\"X_test      :\", X_test.shape)\n",
    "print(\"pos_rate train_full:\", float(y_train_full.mean()), \"test:\", float(y_test.mean()))\n",
    "print(\"scale_pos_weight:\", scale_pos_weight)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Temporal VAL split (last 20% as validation)\n",
    "# -----------------------------\n",
    "n = X_train_full.shape[0]\n",
    "split_idx = int(n * 0.8)\n",
    "\n",
    "X_train = X_train_full[:split_idx]\n",
    "y_train = y_train_full.iloc[:split_idx].reset_index(drop=True)\n",
    "\n",
    "X_val = X_train_full[split_idx:]\n",
    "y_val = y_train_full.iloc[split_idx:].reset_index(drop=True)\n",
    "\n",
    "print(\"\\nTemporal VAL split:\")\n",
    "print(\"X_train:\", X_train.shape, \"pos_rate:\", float(y_train.mean()))\n",
    "print(\"X_val  :\", X_val.shape, \"pos_rate:\", float(y_val.mean()))\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Dense conversion if safe\n",
    "# -----------------------------\n",
    "def can_dense(X):\n",
    "    # avoid memory explosion\n",
    "    rows, cols = X.shape\n",
    "    approx_bytes = rows * cols * 4  # float32\n",
    "    return approx_bytes < 2.5e9  # ~2.5GB\n",
    "\n",
    "dense_allowed = can_dense(X_train_full)\n",
    "print(\"\\nDense conversion allowed:\", dense_allowed)\n",
    "\n",
    "if dense_allowed:\n",
    "    X_train_dense = X_train.toarray().astype(np.float32)\n",
    "    X_val_dense   = X_val.toarray().astype(np.float32)\n",
    "    X_test_dense  = X_test.toarray().astype(np.float32)\n",
    "else:\n",
    "    X_train_dense = X_val_dense = X_test_dense = None\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Define models\n",
    "# -----------------------------\n",
    "base_models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=3000, n_jobs=-1, class_weight=\"balanced\"),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "    ),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=800,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\",\n",
    "    ),\n",
    "    \"Bagging_LR\": BaggingClassifier(\n",
    "        estimator=LogisticRegression(max_iter=3000, n_jobs=-1, class_weight=\"balanced\"),\n",
    "        n_estimators=30,\n",
    "        max_samples=0.75,\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(\n",
    "        max_depth=6,\n",
    "        learning_rate=0.06,\n",
    "        max_iter=400,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    \"XGBoost_Tuned\": XGBClassifier(\n",
    "        n_estimators=3000,            # large + early stop\n",
    "        learning_rate=0.02,\n",
    "        max_depth=6,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        reg_alpha=0.0,\n",
    "        gamma=0.0,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        tree_method=\"hist\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "requires_dense = {\"HistGradientBoosting\": True}\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Train base models (select by VAL PR_AUC)\n",
    "# -----------------------------\n",
    "results_val = []\n",
    "thr_val = []\n",
    "trained = {}\n",
    "input_type = {}\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    print(f\"\\nTraining (Temporal VAL): {name}\")\n",
    "\n",
    "    use_dense = bool(requires_dense.get(name, False))\n",
    "    if use_dense:\n",
    "        if not dense_allowed:\n",
    "            print(f\"[SKIP] {name} needs dense but dense conversion not allowed.\")\n",
    "            continue\n",
    "        Xtr, Xva = X_train_dense, X_val_dense\n",
    "    else:\n",
    "        Xtr, Xva = X_train, X_val\n",
    "\n",
    "    # XGB early stop robust\n",
    "    if name == \"XGBoost_Tuned\":\n",
    "        xgb_fit_with_early_stop_robust(model, Xtr, y_train, Xva, y_val)\n",
    "    else:\n",
    "        model.fit(Xtr, y_train)\n",
    "\n",
    "    probas_val = safe_predict_proba(model, Xva)\n",
    "    results_val.append(evaluate_probas(y_val, probas_val, name=name))\n",
    "    thr_val.append(summarize_threshold_metrics(y_val, probas_val, name=name))\n",
    "\n",
    "    trained[name] = model\n",
    "    input_type[name] = \"dense\" if use_dense else \"sparse\"\n",
    "\n",
    "val_df = pd.DataFrame(results_val).sort_values(\"PR_AUC\", ascending=False)\n",
    "val_thr_df = pd.DataFrame(thr_val).sort_values(\"F1_at_best\", ascending=False)\n",
    "\n",
    "print(\"\\nVAL performance (temporal):\")\n",
    "print(val_df)\n",
    "\n",
    "print(\"\\nVAL best-threshold summary:\")\n",
    "print(val_thr_df)\n",
    "\n",
    "best_name = val_df.iloc[0][\"Model\"]\n",
    "print(f\"\\nSelected best by VAL PR_AUC: {best_name} | input: {input_type[best_name]}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Ensembles\n",
    "# -----------------------------\n",
    "voting_candidates = [\"LogisticRegression\", \"Bagging_LR\", \"HistGradientBoosting\", \"XGBoost_Tuned\"]\n",
    "estimators = [(m, trained[m]) for m in voting_candidates if m in trained]\n",
    "\n",
    "ensemble_needs_dense = any(input_type.get(m, \"sparse\") == \"dense\" for m, _ in estimators)\n",
    "\n",
    "def X_for(inp, split):\n",
    "    if inp == \"dense\":\n",
    "        if not dense_allowed:\n",
    "            raise RuntimeError(\"Dense required but not allowed.\")\n",
    "        if split == \"train\": return X_train_dense\n",
    "        if split == \"val\":   return X_val_dense\n",
    "        if split == \"test\":  return X_test_dense\n",
    "        raise ValueError(split)\n",
    "    else:\n",
    "        if split == \"train\": return X_train\n",
    "        if split == \"val\":   return X_val\n",
    "        if split == \"test\":  return X_test\n",
    "        raise ValueError(split)\n",
    "\n",
    "ens_input = \"dense\" if ensemble_needs_dense else \"sparse\"\n",
    "Xtr_ens = X_for(ens_input, \"train\")\n",
    "Xva_ens = X_for(ens_input, \"val\")\n",
    "\n",
    "# Voting soft\n",
    "print(\"\\nTraining ensemble: Voting_Soft (Temporal VAL)\")\n",
    "voting_soft = VotingClassifier(estimators=estimators, voting=\"soft\", n_jobs=-1)\n",
    "voting_soft.fit(Xtr_ens, y_train)\n",
    "probas_vote = safe_predict_proba(voting_soft, Xva_ens)\n",
    "\n",
    "# Weighted soft (use VAL PR_AUC weights)\n",
    "pr_map = {row[\"Model\"]: float(row[\"PR_AUC\"]) for _, row in val_df.iterrows()}\n",
    "raw_weights = np.array([pr_map.get(m, 0.0) for m, _ in estimators], dtype=float)\n",
    "weights = None\n",
    "if np.any(raw_weights > 0):\n",
    "    raw_weights = raw_weights + 1e-6\n",
    "    weights = (raw_weights / raw_weights.sum()).tolist()\n",
    "\n",
    "print(\"Voting models:\", [m for m, _ in estimators])\n",
    "print(\"Weights:\", weights)\n",
    "\n",
    "print(\"\\nTraining ensemble: Voting_WeightedSoft (Temporal VAL)\")\n",
    "voting_weighted = VotingClassifier(estimators=estimators, voting=\"soft\", weights=weights, n_jobs=-1)\n",
    "voting_weighted.fit(Xtr_ens, y_train)\n",
    "probas_vote_w = safe_predict_proba(voting_weighted, Xva_ens)\n",
    "\n",
    "# Stacking\n",
    "print(\"\\nTraining ensemble: Stacking (Temporal VAL)\")\n",
    "stacking = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(max_iter=3000, n_jobs=-1, class_weight=\"balanced\"),\n",
    "    stack_method=\"predict_proba\",\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    ")\n",
    "stacking.fit(Xtr_ens, y_train)\n",
    "probas_stack = safe_predict_proba(stacking, Xva_ens)\n",
    "\n",
    "# Add ensemble results\n",
    "results_val2 = results_val.copy()\n",
    "thr_val2 = thr_val.copy()\n",
    "\n",
    "for nm, pr in [\n",
    "    (\"Voting_Soft\", probas_vote),\n",
    "    (\"Voting_WeightedSoft\", probas_vote_w),\n",
    "    (\"Stacking\", probas_stack),\n",
    "]:\n",
    "    results_val2.append(evaluate_probas(y_val, pr, name=nm))\n",
    "    thr_val2.append(summarize_threshold_metrics(y_val, pr, name=nm))\n",
    "\n",
    "trained[\"Voting_Soft\"] = voting_soft\n",
    "trained[\"Voting_WeightedSoft\"] = voting_weighted\n",
    "trained[\"Stacking\"] = stacking\n",
    "\n",
    "input_type[\"Voting_Soft\"] = ens_input\n",
    "input_type[\"Voting_WeightedSoft\"] = ens_input\n",
    "input_type[\"Stacking\"] = ens_input\n",
    "\n",
    "val_all_df = pd.DataFrame(results_val2).sort_values(\"PR_AUC\", ascending=False)\n",
    "print(\"\\nAll models (base + ensembles) VAL performance:\")\n",
    "print(val_all_df)\n",
    "\n",
    "final_best = val_all_df.iloc[0][\"Model\"]\n",
    "final_input = input_type[final_best]\n",
    "print(f\"\\nFINAL selected (by temporal VAL PR_AUC): {final_best} | input: {final_input}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Refit best on TRAIN+VAL then TEST\n",
    "# -----------------------------\n",
    "print(\"\\nRefitting best model on TRAIN+VAL then evaluating on TEST...\")\n",
    "\n",
    "X_trainval = X_train_full\n",
    "y_trainval = y_train_full.reset_index(drop=True)\n",
    "\n",
    "if final_input == \"dense\":\n",
    "    if not dense_allowed:\n",
    "        raise RuntimeError(\"Dense required but not allowed.\")\n",
    "    X_trainval_in = X_train_full.toarray().astype(np.float32)\n",
    "    X_test_in = X_test_dense\n",
    "else:\n",
    "    X_trainval_in = X_trainval\n",
    "    X_test_in = X_test\n",
    "\n",
    "best_model = trained[final_best]\n",
    "\n",
    "# Refit\n",
    "if final_best == \"XGBoost_Tuned\":\n",
    "    # for refit, train without early stop (or could early stop using VAL again)\n",
    "    best_model.fit(X_trainval_in, y_trainval)\n",
    "else:\n",
    "    best_model.fit(X_trainval_in, y_trainval)\n",
    "\n",
    "probas_test = safe_predict_proba(best_model, X_test_in)\n",
    "test_df = pd.DataFrame([evaluate_probas(y_test, probas_test, name=f\"{final_best}_TEST\")])\n",
    "test_thr = pd.DataFrame([summarize_threshold_metrics(y_test, probas_test, name=f\"{final_best}_TEST\")])\n",
    "\n",
    "print(\"\\nTEST metrics (honest):\")\n",
    "print(test_df)\n",
    "\n",
    "print(\"\\nTEST best-threshold summary:\")\n",
    "print(test_thr)\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Calibration (optional)\n",
    "# -----------------------------\n",
    "print(\"\\nCalibrating best model (sigmoid, CV=3) on TRAIN+VAL only...\")\n",
    "calibrated = CalibratedClassifierCV(best_model, method=\"sigmoid\", cv=3)\n",
    "calibrated.fit(X_trainval_in, y_trainval)\n",
    "probas_cal = calibrated.predict_proba(X_test_in)[:, 1]\n",
    "\n",
    "cal_df = pd.DataFrame([evaluate_probas(y_test, probas_cal, name=f\"{final_best}_Calibrated_TEST\")])\n",
    "cal_thr = pd.DataFrame([summarize_threshold_metrics(y_test, probas_cal, name=f\"{final_best}_Calibrated_TEST\")])\n",
    "\n",
    "print(\"\\nCalibrated TEST metrics:\")\n",
    "print(cal_df)\n",
    "print(\"\\nCalibrated TEST best-threshold summary:\")\n",
    "print(cal_thr)\n",
    "\n",
    "# -----------------------------\n",
    "# 9) Save best + calibrated\n",
    "# -----------------------------\n",
    "best_path = os.path.join(models_dir, f\"{final_best}_model.pkl\")\n",
    "cal_path  = os.path.join(models_dir, f\"{final_best}_Calibrated_model.pkl\")\n",
    "\n",
    "joblib.dump(best_model, best_path)\n",
    "joblib.dump(calibrated, cal_path)\n",
    "\n",
    "print(\"\\nSaved best model:\", best_path)\n",
    "print(\"Saved calibrated model:\", cal_path)\n",
    "print(\"\\n[Training completed successfully - temporal VAL selection + honest TEST evaluation.]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9b8a2f",
   "metadata": {},
   "source": [
    "### Modifications Summary\n",
    "This training notebook has been updated to remove dependence on SMOTE-oversampled data.\n",
    "Processed feature matrices (`X_train_processed.npz`, `X_test_processed.npz`) and labels are loaded instead.\n",
    "Class imbalance is addressed via the `scale_pos_weight` parameter on the XGBoost model, computed from the training data.\n",
    "Evaluation metrics now focus exclusively on ROC-AUC, LogLoss, and PR-AUC. Accuracy and F1-score have been removed to provide more informative assessment for imbalanced data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
