{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3111b333",
   "metadata": {},
   "source": [
    "# 02 â€“ Data Cleaning (PySpark)\n",
    "\n",
    "In this notebook we clean and preprocess the data. We load the raw CSV files using PySpark (limiting to 1 million rows), drop duplicates, handle missing values, and perform outlier detection. Cleaned data are saved to the `data/processed` directory for later use.\n",
    "\n",
    "Steps:\n",
    "1. Load data from `data/raw` using PySpark.\n",
    "2. Drop duplicate rows.\n",
    "3. Fill missing values using appropriate strategies (median for numeric, mode for categorical).\n",
    "4. Convert timestamp columns to datetime if needed.\n",
    "5. Save cleaned data as Parquet for efficient downstream processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b18c8da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n",
      "Project root: d:\\projects\\Ai\\project_fusion_ecu\n",
      "Raw data dir: d:\\projects\\Ai\\project_fusion_ecu\\data\\raw\n",
      "Processed data dir: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\n",
      "raw_sample      -> d:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\raw_sample.csv | exists: True\n",
      "ad_feature      -> d:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\ad_feature.csv | exists: True\n",
      "user_profile    -> d:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\user_profile.csv | exists: True\n",
      "behavior_log    -> d:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\behavior_log.csv | exists: True\n",
      "\n",
      "Loading up to 1,000,000 rows from each file...\n",
      "User rows:      1000000\n",
      "Ad rows:        846811\n",
      "Click rows:     1000000\n",
      "Behavior rows:  1000000\n",
      "\n",
      "Dropping exact duplicate rows...\n",
      "User rows after dropDuplicates: 1000000\n",
      "Ad rows after dropDuplicates:   846811\n",
      "Click rows after dropDuplicates: 1000000\n",
      "Behavior rows after dropDuplicates: 930695\n",
      "\n",
      "Filling missing values (median for numeric, mode for categorical)...\n",
      "Missing value imputation completed.\n",
      "\n",
      "Adding safe timestamp string columns...\n",
      "\n",
      "Converting Spark DataFrames to Pandas for saving... This may take some time for 1M rows each.\n",
      "Saving cleaned data as CSV (sufficient for the project and easy to use later).\n",
      "\n",
      "Saved cleaned CSVs into: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\n",
      "\n",
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# 0. Imports and Spark session\n",
    "# ============================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "from pyspark.sql.types import NumericType\n",
    "import os\n",
    "\n",
    "# Create a reasonably sized local Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"CTR_Project_Fusion_Ensemble\")\n",
    "        .master(\"local[4]\")                     # use 4 cores (good balance for your laptop)\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "        .config(\"spark.driver.memory\", \"6g\")    # keep some RAM for the OS and VS Code\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "# ============================\n",
    "# 1. Define project and data paths\n",
    "# ============================\n",
    "\n",
    "# Assuming this notebook lives in: D:/projects/Ai/project_fusion_ecu/notebooks\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "raw_dir = os.path.join(project_root, \"data\", \"raw\")\n",
    "processed_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"Raw data dir:\", raw_dir)\n",
    "print(\"Processed data dir:\", processed_dir)\n",
    "\n",
    "# Expected file names (change here if your actual names differ)\n",
    "file_names = {\n",
    "    \"raw_sample\": \"raw_sample.csv\",\n",
    "    \"ad_feature\": \"ad_feature.csv\",\n",
    "    \"user_profile\": \"user_profile.csv\",\n",
    "    # If your file is actually named 'raw_behavior_log.csv', change the next line accordingly:\n",
    "    \"behavior_log\": \"behavior_log.csv\",\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# 2. Verify that required files exist\n",
    "# ============================\n",
    "\n",
    "missing = []\n",
    "for key, fname in file_names.items():\n",
    "    path = os.path.join(raw_dir, fname)\n",
    "    exists = os.path.exists(path)\n",
    "    print(f\"{key:15s} -> {path} | exists: {exists}\")\n",
    "    if not exists:\n",
    "        missing.append(path)\n",
    "\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        \"The following required files are missing. \"\n",
    "        \"Please copy them into data/raw with the exact names:\\n\"\n",
    "        + \"\\n\".join(missing)\n",
    "    )\n",
    "\n",
    "# ============================\n",
    "# 3. Load up to 1M rows per file\n",
    "# ============================\n",
    "\n",
    "print(\"\\nLoading up to 1,000,000 rows from each file...\")\n",
    "\n",
    "user_df = (\n",
    "    spark.read.csv(\n",
    "        os.path.join(raw_dir, file_names[\"user_profile\"]),\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "    )\n",
    "    .limit(1_000_000)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "ad_df = (\n",
    "    spark.read.csv(\n",
    "        os.path.join(raw_dir, file_names[\"ad_feature\"]),\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "    )\n",
    "    .limit(1_000_000)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "click_df = (\n",
    "    spark.read.csv(\n",
    "        os.path.join(raw_dir, file_names[\"raw_sample\"]),\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "    )\n",
    "    .limit(1_000_000)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "behavior_df = (\n",
    "    spark.read.csv(\n",
    "        os.path.join(raw_dir, file_names[\"behavior_log\"]),\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "    )\n",
    "    .limit(1_000_000)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "print(\"User rows:     \", user_df.count())\n",
    "print(\"Ad rows:       \", ad_df.count())\n",
    "print(\"Click rows:    \", click_df.count())\n",
    "print(\"Behavior rows: \", behavior_df.count())\n",
    "\n",
    "# ============================\n",
    "# 4. Drop duplicate rows\n",
    "# ============================\n",
    "\n",
    "print(\"\\nDropping exact duplicate rows...\")\n",
    "\n",
    "user_df = user_df.dropDuplicates()\n",
    "ad_df = ad_df.dropDuplicates()\n",
    "click_df = click_df.dropDuplicates()\n",
    "behavior_df = behavior_df.dropDuplicates()\n",
    "\n",
    "print(\"User rows after dropDuplicates:\", user_df.count())\n",
    "print(\"Ad rows after dropDuplicates:  \", ad_df.count())\n",
    "print(\"Click rows after dropDuplicates:\", click_df.count())\n",
    "print(\"Behavior rows after dropDuplicates:\", behavior_df.count())\n",
    "\n",
    "# ============================\n",
    "# 5. Helper function: fill missing values\n",
    "#    - Numeric columns: median\n",
    "#    - Categorical columns: mode (most frequent value)\n",
    "# ============================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def fill_missing(df, id_cols=None):\n",
    "    \"\"\"\n",
    "    Fill missing values in a Spark DataFrame.\n",
    "    - Numeric columns: filled with median\n",
    "    - Non-numeric columns: filled with mode (most frequent value)\n",
    "    id_cols: columns that should not be imputed (e.g., IDs)\n",
    "    \"\"\"\n",
    "    if id_cols is None:\n",
    "        id_cols = []\n",
    "\n",
    "    # Work on a copy-like reference\n",
    "    result = df\n",
    "    for col_name, dtype in result.dtypes:\n",
    "        if col_name in id_cols:\n",
    "            continue\n",
    "\n",
    "        field = result.schema[col_name].dataType\n",
    "\n",
    "        # Numeric column: use median\n",
    "        if isinstance(field, NumericType):\n",
    "            non_null = result.filter(F.col(col_name).isNotNull())\n",
    "            if non_null.limit(1).count() == 0:\n",
    "                # No non-null values at all, fill with 0 as safe default\n",
    "                median_val = 0\n",
    "            else:\n",
    "                median_val = non_null.approxQuantile(col_name, [0.5], 0.01)[0]\n",
    "            result = result.fillna({col_name: median_val})\n",
    "\n",
    "        # Categorical/text column: use mode (most frequent value)\n",
    "        else:\n",
    "            non_null = result.filter(F.col(col_name).isNotNull())\n",
    "            if non_null.limit(1).count() == 0:\n",
    "                # Column is entirely null -> skip\n",
    "                continue\n",
    "            mode_row = (\n",
    "                non_null.groupBy(col_name)\n",
    "                .count()\n",
    "                .orderBy(F.desc(\"count\"))\n",
    "                .first()\n",
    "            )\n",
    "            if mode_row is not None:\n",
    "                mode_val = mode_row[0]\n",
    "                result = result.fillna({col_name: mode_val})\n",
    "\n",
    "    return result\n",
    "\n",
    "# ============================\n",
    "# 6. Apply missing value imputation\n",
    "# ============================\n",
    "\n",
    "print(\"\\nFilling missing values (median for numeric, mode for categorical)...\")\n",
    "\n",
    "user_df = fill_missing(user_df, id_cols=[\"userid\", \"user\"])\n",
    "ad_df = fill_missing(ad_df, id_cols=[\"adgroup_id\"])\n",
    "click_df = fill_missing(click_df, id_cols=[\"user\", \"adgroup_id\"])\n",
    "behavior_df = fill_missing(behavior_df, id_cols=[\"user\", \"nick\"])\n",
    "\n",
    "print(\"Missing value imputation completed.\")\n",
    "\n",
    "# ============================\n",
    "# 7. Safe timestamp handling\n",
    "#    - For Windows, toPandas() with TimestampType can fail if values are out of range.\n",
    "#    - We convert Unix time to a STRING column instead of Spark TimestampType.\n",
    "# ============================\n",
    "\n",
    "print(\"\\nAdding safe timestamp string columns...\")\n",
    "\n",
    "if \"time_stamp\" in click_df.columns:\n",
    "    click_df = click_df.withColumn(\n",
    "        \"time_stamp_str\",\n",
    "        from_unixtime(col(\"time_stamp\").cast(\"bigint\")).cast(\"string\"),\n",
    "    )\n",
    "\n",
    "if \"time_stamp\" in behavior_df.columns:\n",
    "    behavior_df = behavior_df.withColumn(\n",
    "        \"time_stamp_str\",\n",
    "        from_unixtime(col(\"time_stamp\").cast(\"bigint\")).cast(\"string\"),\n",
    "    )\n",
    "\n",
    "# ============================\n",
    "# 8. Convert to Pandas and save as CSV (no Hadoop / winutils issues)\n",
    "# ============================\n",
    "\n",
    "print(\"\\nConverting Spark DataFrames to Pandas for saving... \"\n",
    "      \"This may take some time for 1M rows each.\")\n",
    "\n",
    "user_pdf = user_df.toPandas()\n",
    "ad_pdf = ad_df.toPandas()\n",
    "click_pdf = click_df.toPandas()\n",
    "behavior_pdf = behavior_df.toPandas()\n",
    "\n",
    "print(\"Saving cleaned data as CSV (sufficient for the project and easy to use later).\")\n",
    "\n",
    "user_pdf.to_csv(os.path.join(processed_dir, \"user_profile_clean.csv\"), index=False)\n",
    "ad_pdf.to_csv(os.path.join(processed_dir, \"ad_feature_clean.csv\"), index=False)\n",
    "click_pdf.to_csv(os.path.join(processed_dir, \"raw_sample_clean.csv\"), index=False)\n",
    "behavior_pdf.to_csv(os.path.join(processed_dir, \"behavior_log_clean.csv\"), index=False)\n",
    "\n",
    "print(\"\\nSaved cleaned CSVs into:\", processed_dir)\n",
    "\n",
    "# ============================\n",
    "# 9. Stop Spark session\n",
    "# ============================\n",
    "\n",
    "spark.stop()\n",
    "print(\"\\nSpark session stopped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
