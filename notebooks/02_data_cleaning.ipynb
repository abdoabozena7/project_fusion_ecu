{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3111b333",
   "metadata": {},
   "source": [
    "# 02 – Data Cleaning (PySpark)\n",
    "\n",
    "In this notebook we clean and preprocess the data. We load the raw CSV files using PySpark (limiting to 1 million rows), drop duplicates, handle missing values, and perform outlier detection. Cleaned data are saved to the `data/processed` directory for later use.\n",
    "\n",
    "Steps:\n",
    "1. Load data from `data/raw` using PySpark.\n",
    "2. Drop duplicate rows.\n",
    "3. Fill missing values using appropriate strategies (median for numeric, mode for categorical).\n",
    "4. Convert timestamp columns to datetime if needed.\n",
    "5. Save cleaned data as Parquet for efficient downstream processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b18c8da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n",
      "\n",
      "Loading randomized samples...\n",
      "+----------+\n",
      "|click_rate|\n",
      "+----------+\n",
      "|   0.05148|\n",
      "+----------+\n",
      "\n",
      "\n",
      "Saving cleaned CSV files...\n",
      "Spark write failed, using Pandas fallback: An error occurred while calling o1823.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotF\n",
      "Saved with Pandas fallback: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\user_profile_clean.csv\n",
      "Spark write failed, using Pandas fallback: An error occurred while calling o1833.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotF\n",
      "Saved with Pandas fallback: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\ad_feature_clean.csv\n",
      "Spark write failed, using Pandas fallback: An error occurred while calling o1843.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotF\n",
      "Saved with Pandas fallback: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\raw_sample_clean.csv\n",
      "Spark write failed, using Pandas fallback: An error occurred while calling o1852.csv.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotF\n",
      "Saved with Pandas fallback: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\behavior_log_clean.csv\n",
      "\n",
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\n",
    "1) Read raw CSV files from data/raw\n",
    "2) Take a RANDOM sample up to 500K rows (efficient, no heavy shuffle)\n",
    "3) Remove duplicate rows\n",
    "4) Normalize join keys (user, adgroup_id)\n",
    "5) Handle missing values:\n",
    "   - Numeric  -> median\n",
    "   - Categorical -> \"UNKNOWN\"\n",
    "   - Add *_missing flags for important columns\n",
    "6) Add readable timestamp columns (if time_stamp exists)\n",
    "7) Run basic sanity checks\n",
    "8) Save cleaned CSVs into data/processed\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "from pyspark.sql.types import NumericType\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "SAMPLE_ROWS = 500_000\n",
    "SEED = 42\n",
    "\n",
    "# Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"CTR_Project_Cleaning\")\n",
    "        .master(\"local[4]\")\n",
    "        .config(\"spark.driver.memory\", \"6g\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "raw_dir = os.path.join(project_root, \"data\", \"raw\")\n",
    "processed_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "file_names = {\n",
    "    \"raw_sample\": \"raw_sample.csv\",\n",
    "    \"ad_feature\": \"ad_feature.csv\",\n",
    "    \"user_profile\": \"user_profile.csv\",\n",
    "    \"behavior_log\": \"behavior_log.csv\",\n",
    "}\n",
    "\n",
    "def safe_read_csv(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "    return spark.read.csv(path, header=True, inferSchema=True)\n",
    "\n",
    "def load_random_sample_limit(path, limit_rows=SAMPLE_ROWS, seed=SEED):\n",
    "\n",
    "    df = safe_read_csv(path)\n",
    "    total = df.count()\n",
    "\n",
    "    if total == 0:\n",
    "        return df\n",
    "\n",
    "    if total <= limit_rows:\n",
    "        return df\n",
    "\n",
    "    frac = min(1.0, (limit_rows / float(total)) * 1.25)\n",
    "    return df.sample(False, frac, seed).limit(limit_rows)\n",
    "\n",
    "def split_cols(df):\n",
    "    num_cols, cat_cols = [], []\n",
    "    for c in df.columns:\n",
    "        if isinstance(df.schema[c].dataType, NumericType):\n",
    "            num_cols.append(c)\n",
    "        else:\n",
    "            cat_cols.append(c)\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def fill_numeric_with_median(df, numeric_cols, skip_cols=None, rel_err=0.01):\n",
    "    skip_cols = skip_cols or set()\n",
    "    out = df\n",
    "\n",
    "    for c in numeric_cols:\n",
    "        if c in skip_cols or c not in out.columns:\n",
    "            continue\n",
    "\n",
    "        has_any = out.select(F.max(F.col(c).isNotNull().cast(\"int\"))).collect()[0][0]\n",
    "        med = 0 if not has_any else out.approxQuantile(c, [0.5], rel_err)[0]\n",
    "        out = out.fillna({c: med})\n",
    "\n",
    "    return out\n",
    "\n",
    "def fill_categorical_unknown(df, cat_cols, unknown=\"UNKNOWN\"):\n",
    "    fill_map = {c: unknown for c in cat_cols if c in df.columns}\n",
    "    return df.fillna(fill_map) if fill_map else df\n",
    "\n",
    "def add_missing_flags(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(f\"{c}_missing\", F.col(c).isNull().cast(\"int\"))\n",
    "    return df\n",
    "\n",
    "def normalize_keys(user_df, ad_df, click_df, behavior_df):\n",
    "    # user_profile: userid -> user\n",
    "    if \"userid\" in user_df.columns and \"user\" not in user_df.columns:\n",
    "        user_df = user_df.withColumn(\"user\", F.col(\"userid\").cast(\"int\")).drop(\"userid\")\n",
    "    elif \"userid\" in user_df.columns:\n",
    "        user_df = user_df.drop(\"userid\")\n",
    "\n",
    "    # click\n",
    "    if \"user\" in click_df.columns:\n",
    "        click_df = click_df.withColumn(\"user\", F.col(\"user\").cast(\"int\"))\n",
    "    if \"adgroup_id\" in click_df.columns:\n",
    "        click_df = click_df.withColumn(\"adgroup_id\", F.col(\"adgroup_id\").cast(\"int\"))\n",
    "\n",
    "    # ad\n",
    "    if \"adgroup_id\" in ad_df.columns:\n",
    "        ad_df = ad_df.withColumn(\"adgroup_id\", F.col(\"adgroup_id\").cast(\"int\"))\n",
    "\n",
    "    # behavior\n",
    "    if \"user\" in behavior_df.columns:\n",
    "        behavior_df = behavior_df.withColumn(\"user\", F.col(\"user\").cast(\"int\"))\n",
    "\n",
    "    return user_df, ad_df, click_df, behavior_df\n",
    "\n",
    "# Helpers – timestamp\n",
    "def add_timestamp_strings(click_df, behavior_df):\n",
    "    if \"time_stamp\" in click_df.columns:\n",
    "        click_df = click_df.withColumn(\n",
    "            \"time_stamp_str\",\n",
    "            from_unixtime(col(\"time_stamp\").cast(\"bigint\"))\n",
    "        )\n",
    "    if \"time_stamp\" in behavior_df.columns:\n",
    "        behavior_df = behavior_df.withColumn(\n",
    "            \"time_stamp_str\",\n",
    "            from_unixtime(col(\"time_stamp\").cast(\"bigint\"))\n",
    "        )\n",
    "    return click_df, behavior_df\n",
    "\n",
    "# Helpers – safe save (Windows friendly)\n",
    "def save_df_safely(df, out_csv_path, chunk_rows=200_000):\n",
    "\n",
    "    tmp_dir = out_csv_path + \"_tmp\"\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(tmp_dir):\n",
    "            shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(tmp_dir)\n",
    "\n",
    "        part_file = next(\n",
    "            os.path.join(tmp_dir, f)\n",
    "            for f in os.listdir(tmp_dir)\n",
    "            if f.startswith(\"part-\") and f.endswith(\".csv\")\n",
    "        )\n",
    "\n",
    "        if os.path.exists(out_csv_path):\n",
    "            os.remove(out_csv_path)\n",
    "\n",
    "        shutil.move(part_file, out_csv_path)\n",
    "        shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "        print(f\"Saved with Spark: {out_csv_path}\")\n",
    "        return\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Spark write failed, using Pandas fallback:\", str(e)[:120])\n",
    "\n",
    "    # Pandas fallback (chunked)\n",
    "    cols = df.columns\n",
    "    buffer = []\n",
    "    wrote_header = False\n",
    "\n",
    "    if os.path.exists(out_csv_path):\n",
    "        os.remove(out_csv_path)\n",
    "\n",
    "    for row in df.toLocalIterator():\n",
    "        buffer.append([row[c] for c in cols])\n",
    "        if len(buffer) >= chunk_rows:\n",
    "            pd.DataFrame(buffer, columns=cols).to_csv(\n",
    "                out_csv_path,\n",
    "                mode=\"a\" if wrote_header else \"w\",\n",
    "                header=not wrote_header,\n",
    "                index=False,\n",
    "            )\n",
    "            wrote_header = True\n",
    "            buffer = []\n",
    "\n",
    "    if buffer:\n",
    "        pd.DataFrame(buffer, columns=cols).to_csv(\n",
    "            out_csv_path,\n",
    "            mode=\"a\" if wrote_header else \"w\",\n",
    "            header=not wrote_header,\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "    print(f\"Saved with Pandas fallback: {out_csv_path}\")\n",
    "\n",
    "# Load data\n",
    "print(\"\\nLoading randomized samples...\")\n",
    "\n",
    "user_df = load_random_sample_limit(os.path.join(raw_dir, file_names[\"user_profile\"]))\n",
    "ad_df = load_random_sample_limit(os.path.join(raw_dir, file_names[\"ad_feature\"]))\n",
    "click_df = load_random_sample_limit(os.path.join(raw_dir, file_names[\"raw_sample\"]))\n",
    "behavior_df = load_random_sample_limit(os.path.join(raw_dir, file_names[\"behavior_log\"]))\n",
    "\n",
    "# Remove duplicates\n",
    "user_df = user_df.dropDuplicates()\n",
    "ad_df = ad_df.dropDuplicates()\n",
    "click_df = click_df.dropDuplicates()\n",
    "behavior_df = behavior_df.dropDuplicates()\n",
    "\n",
    "# Normalize keys\n",
    "user_df, ad_df, click_df, behavior_df = normalize_keys(\n",
    "    user_df, ad_df, click_df, behavior_df\n",
    ")\n",
    "\n",
    "# Missing value handling\n",
    "user_num, user_cat = split_cols(user_df)\n",
    "user_df = add_missing_flags(user_df, [\"age_level\", \"shopping_level\", \"occupation\", \"final_gender_code\"])\n",
    "user_df = fill_numeric_with_median(user_df, user_num, skip_cols={\"user\"})\n",
    "user_df = fill_categorical_unknown(user_df, user_cat)\n",
    "\n",
    "ad_num, ad_cat = split_cols(ad_df)\n",
    "ad_df = add_missing_flags(ad_df, [\"price\", \"cate_id\", \"campaign_id\", \"customer\"])\n",
    "ad_df = fill_numeric_with_median(ad_df, ad_num, skip_cols={\"adgroup_id\"})\n",
    "ad_df = fill_categorical_unknown(ad_df, ad_cat)\n",
    "\n",
    "click_num, click_cat = split_cols(click_df)\n",
    "click_df = add_missing_flags(click_df, [\"pid\"])\n",
    "click_df = fill_numeric_with_median(click_df, click_num, skip_cols={\"user\", \"adgroup_id\"})\n",
    "click_df = fill_categorical_unknown(click_df, click_cat)\n",
    "\n",
    "beh_num, beh_cat = split_cols(behavior_df)\n",
    "behavior_df = fill_numeric_with_median(behavior_df, beh_num, skip_cols={\"user\"})\n",
    "behavior_df = fill_categorical_unknown(behavior_df, beh_cat)\n",
    "\n",
    "# Timestamp strings\n",
    "click_df, behavior_df = add_timestamp_strings(click_df, behavior_df)\n",
    "\n",
    "# Sanity checks\n",
    "if \"clk\" in click_df.columns:\n",
    "    click_df.select(F.mean(\"clk\").alias(\"click_rate\")).show()\n",
    "\n",
    "# Save cleaned CSVs\n",
    "print(\"\\nSaving cleaned CSV files...\")\n",
    "\n",
    "save_df_safely(user_df, os.path.join(processed_dir, \"user_profile_clean.csv\"))\n",
    "save_df_safely(ad_df, os.path.join(processed_dir, \"ad_feature_clean.csv\"))\n",
    "save_df_safely(click_df, os.path.join(processed_dir, \"raw_sample_clean.csv\"))\n",
    "save_df_safely(behavior_df, os.path.join(processed_dir, \"behavior_log_clean.csv\"))\n",
    "\n",
    "spark.stop()\n",
    "print(\"\\nSpark session stopped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
