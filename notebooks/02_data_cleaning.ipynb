{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3111b333",
   "metadata": {},
   "source": [
    "# 02 â€“ Data Cleaning (PySpark)\n",
    "\n",
    "In this notebook we clean and preprocess the data. We load the raw CSV files using PySpark (limiting to 1 million rows), drop duplicates, handle missing values, and perform outlier detection. Cleaned data are saved to the `data/processed` directory for later use.\n",
    "\n",
    "Steps:\n",
    "1. Load data from `data/raw` using PySpark.\n",
    "2. Drop duplicate rows.\n",
    "3. Fill missing values using appropriate strategies (median for numeric, mode for categorical).\n",
    "4. Convert timestamp columns to datetime if needed.\n",
    "5. Save cleaned data as Parquet for efficient downstream processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b18c8da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n",
      "Project root: d:\\projects\\Ai\\project_fusion_ecu\n",
      "Raw data dir: d:\\projects\\Ai\\project_fusion_ecu\\data\\raw\n",
      "Processed data dir: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\n",
      "raw_sample      -> d:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\raw_sample.csv | exists: True\n",
      "ad_feature      -> d:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\ad_feature.csv | exists: True\n",
      "user_profile    -> d:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\user_profile.csv | exists: True\n",
      "behavior_log    -> d:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\behavior_log.csv | exists: True\n",
      "\n",
      "Loading randomized sample (up to 500k rows each)...\n",
      "User rows:      500000\n",
      "Ad rows:        500000\n",
      "Click rows:     500000\n",
      "Behavior rows:  500000\n",
      "\n",
      "Dropping exact duplicate rows...\n",
      "User rows after dropDuplicates:     500000\n",
      "Ad rows after dropDuplicates:       500000\n",
      "Click rows after dropDuplicates:    500000\n",
      "Behavior rows after dropDuplicates: 499964\n",
      "\n",
      "Normalizing join keys types...\n",
      "\n",
      "Handling missing values safely...\n",
      "Missing handling done.\n",
      "\n",
      "Adding safe timestamp string columns...\n",
      "\n",
      "Sanity checks:\n",
      "+--------+\n",
      "|clk_rate|\n",
      "+--------+\n",
      "|0.051986|\n",
      "+--------+\n",
      "\n",
      "+--------------+-----------------+\n",
      "|user_null_rate|adgroup_null_rate|\n",
      "+--------------+-----------------+\n",
      "|           0.0|              0.0|\n",
      "+--------------+-----------------+\n",
      "\n",
      "\n",
      "Converting Spark DataFrames to Pandas for saving... (can take time)\n",
      "Saving cleaned data as CSV...\n",
      "\n",
      "Saved cleaned CSVs into: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\n",
      "\n",
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Clean + Prepare (Spark) - FIXED VERSION\n",
    "# ============================\n",
    "# Fixes that directly impact CTR modeling quality:\n",
    "# 1) Avoid non-random limit() sampling -> use random sampling with seed\n",
    "# 2) Normalize join keys types NOW (user/adgroup_id) to prevent join-enrichment loss later\n",
    "# 3) Safer missing handling:\n",
    "#    - Categorical: fill with \"UNKNOWN\" (do NOT mode-impute)\n",
    "#    - Numeric: fill with median + add missing indicator flags where useful\n",
    "# 4) Keep time_stamp as integer (for future time-based split). Also store time_stamp_str for Windows safety.\n",
    "# 5) Still saves as CSV to data/processed (same outputs as your pipeline expects)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "from pyspark.sql.types import NumericType\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "# ============================\n",
    "# 0. Spark session\n",
    "# ============================\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"CTR_Project_Cleaning_Fixed\")\n",
    "        .master(\"local[4]\")\n",
    "        .config(\"spark.driver.memory\", \"6g\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "# ============================\n",
    "# 1. Paths\n",
    "# ============================\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "raw_dir = os.path.join(project_root, \"data\", \"raw\")\n",
    "processed_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"Raw data dir:\", raw_dir)\n",
    "print(\"Processed data dir:\", processed_dir)\n",
    "\n",
    "file_names = {\n",
    "    \"raw_sample\": \"raw_sample.csv\",\n",
    "    \"ad_feature\": \"ad_feature.csv\",\n",
    "    \"user_profile\": \"user_profile.csv\",\n",
    "    \"behavior_log\": \"behavior_log.csv\",\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# 2. Verify files\n",
    "# ============================\n",
    "missing = []\n",
    "for key, fname in file_names.items():\n",
    "    path = os.path.join(raw_dir, fname)\n",
    "    exists = os.path.exists(path)\n",
    "    print(f\"{key:15s} -> {path} | exists: {exists}\")\n",
    "    if not exists:\n",
    "        missing.append(path)\n",
    "\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing required files:\\n\" + \"\\n\".join(missing)\n",
    "    )\n",
    "\n",
    "# ============================\n",
    "# 3. Load data\n",
    "#    IMPORTANT: avoid .limit() for modeling datasets unless randomized\n",
    "# ============================\n",
    "SAMPLE_ROWS = 500_000\n",
    "SEED = 42\n",
    "\n",
    "def load_random_limit(path):\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    # Randomize then take SAMPLE_ROWS\n",
    "    return df.orderBy(F.rand(seed=SEED)).limit(SAMPLE_ROWS).cache()\n",
    "\n",
    "print(\"\\nLoading randomized sample (up to 500k rows each)...\")\n",
    "\n",
    "user_df = load_random_limit(os.path.join(raw_dir, file_names[\"user_profile\"]))\n",
    "ad_df = load_random_limit(os.path.join(raw_dir, file_names[\"ad_feature\"]))\n",
    "click_df = load_random_limit(os.path.join(raw_dir, file_names[\"raw_sample\"]))\n",
    "behavior_df = load_random_limit(os.path.join(raw_dir, file_names[\"behavior_log\"]))\n",
    "\n",
    "print(\"User rows:     \", user_df.count())\n",
    "print(\"Ad rows:       \", ad_df.count())\n",
    "print(\"Click rows:    \", click_df.count())\n",
    "print(\"Behavior rows: \", behavior_df.count())\n",
    "\n",
    "# ============================\n",
    "# 4. Drop exact duplicates\n",
    "# ============================\n",
    "print(\"\\nDropping exact duplicate rows...\")\n",
    "\n",
    "user_df = user_df.dropDuplicates()\n",
    "ad_df = ad_df.dropDuplicates()\n",
    "click_df = click_df.dropDuplicates()\n",
    "behavior_df = behavior_df.dropDuplicates()\n",
    "\n",
    "print(\"User rows after dropDuplicates:    \", user_df.count())\n",
    "print(\"Ad rows after dropDuplicates:      \", ad_df.count())\n",
    "print(\"Click rows after dropDuplicates:   \", click_df.count())\n",
    "print(\"Behavior rows after dropDuplicates:\", behavior_df.count())\n",
    "\n",
    "# ============================\n",
    "# 5. Normalize key columns + schemas (critical for later joins)\n",
    "# ============================\n",
    "print(\"\\nNormalizing join keys types...\")\n",
    "\n",
    "# user_profile has userid (double) in your schema -> create \"user\" int\n",
    "if \"userid\" in user_df.columns and \"user\" not in user_df.columns:\n",
    "    user_df = user_df.withColumn(\"user\", F.col(\"userid\").cast(\"int\")).drop(\"userid\")\n",
    "elif \"userid\" in user_df.columns and \"user\" in user_df.columns:\n",
    "    # if both exist, keep user and drop userid\n",
    "    user_df = user_df.drop(\"userid\")\n",
    "\n",
    "# click_df user/adgroup_id are int already in your schema, but cast defensively\n",
    "if \"user\" in click_df.columns:\n",
    "    click_df = click_df.withColumn(\"user\", F.col(\"user\").cast(\"int\"))\n",
    "if \"adgroup_id\" in click_df.columns:\n",
    "    click_df = click_df.withColumn(\"adgroup_id\", F.col(\"adgroup_id\").cast(\"int\"))\n",
    "\n",
    "# ad_df adgroup_id is double in your schema -> cast to int\n",
    "if \"adgroup_id\" in ad_df.columns:\n",
    "    ad_df = ad_df.withColumn(\"adgroup_id\", F.col(\"adgroup_id\").cast(\"int\"))\n",
    "\n",
    "# behavior_df user is int, cast defensively\n",
    "if \"user\" in behavior_df.columns:\n",
    "    behavior_df = behavior_df.withColumn(\"user\", F.col(\"user\").cast(\"int\"))\n",
    "\n",
    "# Optional: unify brand types\n",
    "# ad_df brand is string; behavior_df brand is integer (different meaning), keep as-is.\n",
    "\n",
    "# ============================\n",
    "# 6. Missing value handling (safer for CTR)\n",
    "#    - Categorical: fill with \"UNKNOWN\" (do NOT mode-impute)\n",
    "#    - Numeric: median + (optional) missing flags for selected numeric cols\n",
    "# ============================\n",
    "print(\"\\nHandling missing values safely...\")\n",
    "\n",
    "def fill_numeric_with_median(df, numeric_cols, skip_cols=None):\n",
    "    if skip_cols is None:\n",
    "        skip_cols = set()\n",
    "    out = df\n",
    "    for c in numeric_cols:\n",
    "        if c in skip_cols or c not in out.columns:\n",
    "            continue\n",
    "        nn = out.filter(F.col(c).isNotNull())\n",
    "        if nn.limit(1).count() == 0:\n",
    "            med = 0\n",
    "        else:\n",
    "            med = nn.approxQuantile(c, [0.5], 0.01)[0]\n",
    "        out = out.fillna({c: med})\n",
    "    return out\n",
    "\n",
    "def fill_categorical_unknown(df, cat_cols, unknown=\"UNKNOWN\"):\n",
    "    out = df\n",
    "    fill_map = {}\n",
    "    for c in cat_cols:\n",
    "        if c in out.columns:\n",
    "            fill_map[c] = unknown\n",
    "    if fill_map:\n",
    "        out = out.fillna(fill_map)\n",
    "    return out\n",
    "\n",
    "def add_missing_flags(df, cols):\n",
    "    out = df\n",
    "    for c in cols:\n",
    "        if c in out.columns:\n",
    "            out = out.withColumn(f\"{c}_missing\", F.col(c).isNull().cast(\"int\"))\n",
    "    return out\n",
    "\n",
    "# Identify numeric/categorical columns automatically, but keep control\n",
    "def split_cols(df):\n",
    "    num_cols = []\n",
    "    cat_cols = []\n",
    "    for c, dtype in df.dtypes:\n",
    "        field = df.schema[c].dataType\n",
    "        if isinstance(field, NumericType):\n",
    "            num_cols.append(c)\n",
    "        else:\n",
    "            cat_cols.append(c)\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "# USER\n",
    "user_num, user_cat = split_cols(user_df)\n",
    "user_df = add_missing_flags(user_df, [\"age_level\", \"shopping_level\", \"occupation\", \"final_gender_code\"])\n",
    "user_df = fill_numeric_with_median(user_df, user_num, skip_cols={\"user\"})\n",
    "user_df = fill_categorical_unknown(user_df, user_cat, unknown=\"UNKNOWN\")\n",
    "\n",
    "# AD\n",
    "ad_num, ad_cat = split_cols(ad_df)\n",
    "ad_df = add_missing_flags(ad_df, [\"price\", \"cate_id\", \"campaign_id\", \"customer\"])\n",
    "ad_df = fill_numeric_with_median(ad_df, ad_num, skip_cols={\"adgroup_id\"})\n",
    "ad_df = fill_categorical_unknown(ad_df, ad_cat, unknown=\"UNKNOWN\")\n",
    "\n",
    "# CLICK\n",
    "click_num, click_cat = split_cols(click_df)\n",
    "# keep clk/nonclk/user/adgroup_id; fill other numeric if any\n",
    "click_df = add_missing_flags(click_df, [\"pid\"])\n",
    "click_df = fill_numeric_with_median(click_df, click_num, skip_cols={\"user\", \"adgroup_id\"})\n",
    "click_df = fill_categorical_unknown(click_df, click_cat, unknown=\"UNKNOWN\")\n",
    "\n",
    "# BEHAVIOR\n",
    "beh_num, beh_cat = split_cols(behavior_df)\n",
    "behavior_df = fill_numeric_with_median(behavior_df, beh_num, skip_cols={\"user\"})\n",
    "behavior_df = fill_categorical_unknown(behavior_df, beh_cat, unknown=\"UNKNOWN\")\n",
    "\n",
    "print(\"Missing handling done.\")\n",
    "\n",
    "# ============================\n",
    "# 7. Safe timestamp columns\n",
    "#    - keep time_stamp as integer for time splits later\n",
    "#    - add time_stamp_str for Windows / CSV friendliness\n",
    "# ============================\n",
    "print(\"\\nAdding safe timestamp string columns...\")\n",
    "\n",
    "if \"time_stamp\" in click_df.columns:\n",
    "    click_df = click_df.withColumn(\n",
    "        \"time_stamp_str\",\n",
    "        from_unixtime(col(\"time_stamp\").cast(\"bigint\")).cast(\"string\"),\n",
    "    )\n",
    "\n",
    "if \"time_stamp\" in behavior_df.columns:\n",
    "    behavior_df = behavior_df.withColumn(\n",
    "        \"time_stamp_str\",\n",
    "        from_unixtime(col(\"time_stamp\").cast(\"bigint\")).cast(\"string\"),\n",
    "    )\n",
    "\n",
    "# ============================\n",
    "# 8. Quick sanity checks that affect modeling\n",
    "#    - label distribution\n",
    "#    - join key null rates\n",
    "# ============================\n",
    "print(\"\\nSanity checks:\")\n",
    "\n",
    "if \"clk\" in click_df.columns:\n",
    "    click_df.select(F.mean(\"clk\").alias(\"clk_rate\")).show()\n",
    "\n",
    "click_df.select(\n",
    "    F.mean(F.col(\"user\").isNull().cast(\"int\")).alias(\"user_null_rate\"),\n",
    "    F.mean(F.col(\"adgroup_id\").isNull().cast(\"int\")).alias(\"adgroup_null_rate\"),\n",
    ").show()\n",
    "\n",
    "# ============================\n",
    "# 9. Convert to Pandas and save CSV\n",
    "#    (keeps your downstream notebook compatible)\n",
    "# ============================\n",
    "print(\"\\nConverting Spark DataFrames to Pandas for saving... (can take time)\")\n",
    "\n",
    "user_pdf = user_df.toPandas()\n",
    "ad_pdf = ad_df.toPandas()\n",
    "click_pdf = click_df.toPandas()\n",
    "behavior_pdf = behavior_df.toPandas()\n",
    "\n",
    "print(\"Saving cleaned data as CSV...\")\n",
    "\n",
    "user_pdf.to_csv(os.path.join(processed_dir, \"user_profile_clean.csv\"), index=False)\n",
    "ad_pdf.to_csv(os.path.join(processed_dir, \"ad_feature_clean.csv\"), index=False)\n",
    "click_pdf.to_csv(os.path.join(processed_dir, \"raw_sample_clean.csv\"), index=False)\n",
    "behavior_pdf.to_csv(os.path.join(processed_dir, \"behavior_log_clean.csv\"), index=False)\n",
    "\n",
    "print(\"\\nSaved cleaned CSVs into:\", processed_dir)\n",
    "\n",
    "# ============================\n",
    "# 10. Stop Spark\n",
    "# ============================\n",
    "spark.stop()\n",
    "print(\"\\nSpark session stopped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
