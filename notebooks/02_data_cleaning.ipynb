{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3fcf74",
   "metadata": {},
   "source": [
    "# 02 – Data Cleaning (PySpark)\n",
    "\n",
    "This notebook performs data cleaning steps using Spark.  We remove duplicate records, handle missing values and convert timestamps to human–readable formats.  The cleaned DataFrames are saved to the `data/processed` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "import os\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName('CTR_Data_Cleaning').getOrCreate()\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "raw_dir = os.path.join('..', 'data', 'raw')\n",
    "processed_dir = os.path.join('..', 'data', 'processed')\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Load up to 1M rows from each dataset\n",
    "user_df = spark.read.csv(os.path.join(raw_dir, 'user_profile.csv'), header=True, inferSchema=True).limit(1_000_000)\n",
    "ad_df = spark.read.csv(os.path.join(raw_dir, 'ad_feature.csv'), header=True, inferSchema=True).limit(1_000_000)\n",
    "click_df = spark.read.csv(os.path.join(raw_dir, 'raw_sample.csv'), header=True, inferSchema=True).limit(1_000_000)\n",
    "behaviour_df = spark.read.csv(os.path.join(raw_dir, 'behavior_log.csv'), header=True, inferSchema=True).limit(1_000_000)\n",
    "\n",
    "# Drop duplicate rows\n",
    "user_df = user_df.dropDuplicates()\n",
    "ad_df = ad_df.dropDuplicates()\n",
    "click_df = click_df.dropDuplicates()\n",
    "behaviour_df = behaviour_df.dropDuplicates()\n",
    "\n",
    "# Fill missing numeric values with median and categorical with mode (simple heuristic)\n",
    "# For demonstration, we fill numeric nulls with -1 and categorical nulls with 'unknown'\n",
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "def fill_missing_values(df):\n",
    "    for column, dtype in df.dtypes:\n",
    "        if isinstance(df.schema[column].dataType, NumericType):\n",
    "            df = df.na.fill({column: -1})\n",
    "        else:\n",
    "            df = df.na.fill({column: 'unknown'})\n",
    "    return df\n",
    "\n",
    "user_df = fill_missing_values(user_df)\n",
    "ad_df = fill_missing_values(ad_df)\n",
    "click_df = fill_missing_values(click_df)\n",
    "behaviour_df = fill_missing_values(behaviour_df)\n",
    "\n",
    "# Convert timestamp columns (assumed to be Unix epoch in seconds) to timestamps\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "click_df = click_df.withColumn('time_stamp', to_timestamp(col('time_stamp')))\n",
    "behaviour_df = behaviour_df.withColumn('time_stamp', to_timestamp(col('time_stamp')))\n",
    "\n",
    "# Save cleaned DataFrames to processed directory in parquet format\n",
    "user_df.write.mode('overwrite').parquet(os.path.join(processed_dir, 'user_profile_clean.parquet'))\n",
    "ad_df.write.mode('overwrite').parquet(os.path.join(processed_dir, 'ad_feature_clean.parquet'))\n",
    "click_df.write.mode('overwrite').parquet(os.path.join(processed_dir, 'raw_sample_clean.parquet'))\n",
    "behaviour_df.write.mode('overwrite').parquet(os.path.join(processed_dir, 'behavior_log_clean.parquet'))\n",
    "\n",
    "print('Cleaning complete – cleaned files written to data/processed.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
