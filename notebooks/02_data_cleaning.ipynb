{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a4e18a",
   "metadata": {},
   "source": [
    "# 02 â€“ Data Cleaning (PySpark)\n",
    "\n",
    "This notebook performs data cleaning and basic preprocessing on the CTR dataset using PySpark.  \n",
    "It is designed to be **robust to missing files** and to work with up to **1M rows per source file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84243fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or get Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"CTR_Data_Cleaning\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25c3eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: d:\\projects\\Ai\\project_fusion_ecu\n",
      "Raw data dir: d:\\projects\\Ai\\project_fusion_ecu\\data\\raw\n",
      "Processed data dir: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\n",
      "user_profile    -> d:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\user_profile.csv  | exists: False\n",
      "ad_feature      -> d:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\ad_feature.csv  | exists: False\n",
      "raw_sample      -> d:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\raw_sample.csv  | exists: False\n",
      "behavior_log    -> d:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\behavior_log.csv  | exists: False\n",
      "\n",
      "WARNING: No data files found in `data/raw`. Please add at least one of:\n",
      "  - user_profile.csv\n",
      "  - ad_feature.csv\n",
      "  - raw_sample.csv\n",
      "  - behavior_log.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Resolve project root assuming this notebook lives in `notebooks/`\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "raw_dir = os.path.join(project_root, \"data\", \"raw\")\n",
    "processed_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"Raw data dir:\", raw_dir)\n",
    "print(\"Processed data dir:\", processed_dir)\n",
    "\n",
    "# Expected file names\n",
    "files = {\n",
    "    \"user_profile\": \"user_profile.csv\",\n",
    "    \"ad_feature\": \"ad_feature.csv\",\n",
    "    \"raw_sample\": \"raw_sample.csv\",\n",
    "    \"behavior_log\": \"behavior_log.csv\",  # change here if your file name is different\n",
    "}\n",
    "\n",
    "available_files = {}\n",
    "for key, fname in files.items():\n",
    "    path = os.path.join(raw_dir, fname)\n",
    "    exists = os.path.exists(path)\n",
    "    print(f\"{key:15s} -> {path}  | exists: {exists}\")\n",
    "    if exists:\n",
    "        available_files[key] = fname\n",
    "\n",
    "if not available_files:\n",
    "    print(\"\\nWARNING: No data files found in `data/raw`. Please add at least one of:\")\n",
    "    for key, fname in files.items():\n",
    "        print(f\"  - {fname}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5812511a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data files were loaded. Please add data files to `data/raw`.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load up to 1M rows from each dataset (only if available)\n",
    "dataframes = {}\n",
    "\n",
    "if \"user_profile\" in available_files:\n",
    "    user_path = os.path.join(raw_dir, available_files[\"user_profile\"])\n",
    "    user_df = spark.read.csv(user_path, header=True, inferSchema=True).limit(1_000_000).cache()\n",
    "    dataframes[\"user\"] = user_df\n",
    "    print(\"User profile rows:\", user_df.count())\n",
    "\n",
    "if \"ad_feature\" in available_files:\n",
    "    ad_path = os.path.join(raw_dir, available_files[\"ad_feature\"])\n",
    "    ad_df = spark.read.csv(ad_path, header=True, inferSchema=True).limit(1_000_000).cache()\n",
    "    dataframes[\"ad\"] = ad_df\n",
    "    print(\"Ad feature rows:\", ad_df.count())\n",
    "\n",
    "if \"raw_sample\" in available_files:\n",
    "    click_path = os.path.join(raw_dir, available_files[\"raw_sample\"])\n",
    "    click_df = spark.read.csv(click_path, header=True, inferSchema=True).limit(1_000_000).cache()\n",
    "    dataframes[\"click\"] = click_df\n",
    "    print(\"Click log rows:\", click_df.count())\n",
    "\n",
    "if \"behavior_log\" in available_files:\n",
    "    behaviour_path = os.path.join(raw_dir, available_files[\"behavior_log\"])\n",
    "    behaviour_df = spark.read.csv(behaviour_path, header=True, inferSchema=True).limit(1_000_000).cache()\n",
    "    dataframes[\"behaviour\"] = behaviour_df\n",
    "    print(\"Behavior log rows:\", behaviour_df.count())\n",
    "\n",
    "if not dataframes:\n",
    "    print(\"No data files were loaded. Please add data files to `data/raw`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db043c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping duplicates:\n"
     ]
    }
   ],
   "source": [
    "# Drop exact duplicates from available dataframes\n",
    "for key, df in dataframes.items():\n",
    "    dataframes[key] = df.dropDuplicates()\n",
    "\n",
    "print(\"After dropping duplicates:\")\n",
    "for key, df in dataframes.items():\n",
    "    print(f\"{key:15s} rows: {df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e783321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null handling complete.\n"
     ]
    }
   ],
   "source": [
    "# Convert timestamp columns if present\n",
    "if \"click\" in dataframes and \"time_stamp\" in dataframes[\"click\"].columns:\n",
    "    dataframes[\"click\"] = dataframes[\"click\"].withColumn(\n",
    "        \"time_stamp_ts\",\n",
    "        F.to_timestamp(F.col(\"time_stamp\").cast(\"double\"))\n",
    "    )\n",
    "\n",
    "if \"behaviour\" in dataframes and \"time_stamp\" in dataframes[\"behaviour\"].columns:\n",
    "    dataframes[\"behaviour\"] = dataframes[\"behaviour\"].withColumn(\n",
    "        \"time_stamp_ts\",\n",
    "        F.to_timestamp(F.col(\"time_stamp\").cast(\"double\"))\n",
    "    )\n",
    "\n",
    "# Simple missing value handling: fill numeric nulls with median and string nulls with mode\n",
    "def fill_nulls(df, id_cols=None):\n",
    "    if id_cols is None:\n",
    "        id_cols = []\n",
    "    cols = [c for c in df.columns if c not in id_cols]\n",
    "    result = df\n",
    "    for c in cols:\n",
    "        dtype = dict(result.dtypes)[c]\n",
    "        if dtype in (\"int\", \"bigint\", \"double\", \"float\", \"long\", \"decimal\"):\n",
    "            median_val = result.approxQuantile(c, [0.5], 0.01)[0] if result.filter(F.col(c).isNotNull()).count() > 0 else 0\n",
    "            result = result.fillna({c: median_val})\n",
    "        else:\n",
    "            mode_row = result.groupBy(c).count().orderBy(F.desc(\"count\")).first()\n",
    "            if mode_row is not None:\n",
    "                result = result.fillna({c: mode_row[0]})\n",
    "    return result\n",
    "\n",
    "# Clean available dataframes with conditional ID columns\n",
    "cleaned_dataframes = {}\n",
    "if \"user\" in dataframes:\n",
    "    cleaned_dataframes[\"user\"] = fill_nulls(dataframes[\"user\"], id_cols=[\"userid\", \"user\", \"nick\"])\n",
    "\n",
    "if \"ad\" in dataframes:\n",
    "    cleaned_dataframes[\"ad\"] = fill_nulls(dataframes[\"ad\"], id_cols=[\"adgroup_id\"])\n",
    "\n",
    "if \"click\" in dataframes:\n",
    "    cleaned_dataframes[\"click\"] = fill_nulls(dataframes[\"click\"], id_cols=[\"user\", \"adgroup_id\"])\n",
    "\n",
    "if \"behaviour\" in dataframes:\n",
    "    cleaned_dataframes[\"behaviour\"] = fill_nulls(dataframes[\"behaviour\"], id_cols=[\"user\", \"nick\"])\n",
    "\n",
    "print(\"Null handling complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3070a62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned data as Parquet (recommended for Spark) and CSV (for backup or inspection)\n",
    "\n",
    "if \"user\" in cleaned_dataframes:\n",
    "    cleaned_dataframes[\"user\"].write.mode(\"overwrite\").parquet(os.path.join(processed_dir, \"user_profile_clean.parquet\"))\n",
    "    cleaned_dataframes[\"user\"].limit(50_000).coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(os.path.join(processed_dir, \"user_profile_clean_sample_csv\"))\n",
    "\n",
    "if \"ad\" in cleaned_dataframes:\n",
    "    cleaned_dataframes[\"ad\"].write.mode(\"overwrite\").parquet(os.path.join(processed_dir, \"ad_feature_clean.parquet\"))\n",
    "    cleaned_dataframes[\"ad\"].limit(50_000).coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(os.path.join(processed_dir, \"ad_feature_clean_sample_csv\"))\n",
    "\n",
    "if \"click\" in cleaned_dataframes:\n",
    "    cleaned_dataframes[\"click\"].write.mode(\"overwrite\").parquet(os.path.join(processed_dir, \"raw_sample_clean.parquet\"))\n",
    "    cleaned_dataframes[\"click\"].limit(50_000).coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(os.path.join(processed_dir, \"raw_sample_clean_sample_csv\"))\n",
    "\n",
    "if \"behaviour\" in cleaned_dataframes:\n",
    "    cleaned_dataframes[\"behaviour\"].write.mode(\"overwrite\").parquet(os.path.join(processed_dir, \"behavior_log_clean.parquet\"))\n",
    "    cleaned_dataframes[\"behaviour\"].limit(50_000).coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(os.path.join(processed_dir, \"behavior_log_clean_sample_csv\"))\n",
    "\n",
    "print(\"Cleaned data saved to:\", processed_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
