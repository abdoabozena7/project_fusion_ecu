{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c312a7",
   "metadata": {},
   "source": [
    "# 06 – Model Evaluation\n",
    "\n",
    "In this notebook we compare the performance of individual and ensemble models using a variety of classification metrics.  We plot ROC curves and visualise confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve\n",
    "\n",
    "# Load test data\n",
    "processed_dir = os.path.join(os.path.pardir, 'data', 'processed')\n",
    "X_test = pd.read_csv(os.path.join(processed_dir, 'X_test.csv'))\n",
    "y_test = pd.read_csv(os.path.join(processed_dir, 'y_test.csv')).squeeze()\n",
    "\n",
    "# Load preprocessor\n",
    "preprocessor = joblib.load(os.path.join(processed_dir, 'preprocessor.pkl'))\n",
    "\n",
    "# Load trained models\n",
    "models_dir = os.path.join(os.path.pardir, 'models')\n",
    "model_files = [f for f in os.listdir(models_dir) if f.endswith('_model.pkl')]\n",
    "\n",
    "results = []\n",
    "roc_curves = {}\n",
    "\n",
    "for file in model_files:\n",
    "    name = file.replace('_model.pkl', '')\n",
    "    model = joblib.load(os.path.join(models_dir, file))\n",
    "    # Predictions and probabilities\n",
    "    preds = model.predict(X_test)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probas = model.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, probas)\n",
    "        fpr, tpr, _ = roc_curve(y_test, probas)\n",
    "        roc_curves[name] = (fpr, tpr, auc)\n",
    "    else:\n",
    "        auc = float('nan')\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    precision = precision_score(y_test, preds)\n",
    "    recall = recall_score(y_test, preds)\n",
    "    results.append({'Model': name, 'Accuracy': acc, 'F1': f1, 'Precision': precision, 'Recall': recall, 'AUC': auc})\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).sort_values(by='AUC', ascending=False)\n",
    "print(results_df)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "for name, (fpr, tpr, auc) in roc_curves.items():\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix for the best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = joblib.load(os.path.join(models_dir, f'{best_model_name}_model.pkl'))\n",
    "best_preds = best_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, best_preds)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix – {best_model_name}')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
