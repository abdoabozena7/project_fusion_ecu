{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "359026a3",
   "metadata": {},
   "source": [
    "# 06 – Model Evaluation (PySpark)\n",
    "\n",
    "This notebook evaluates the trained models on the test data using several metrics.  We compute area under the ROC curve (AUC) and accuracy, and derive a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassificationModel, LogisticRegressionModel, GBTClassificationModel\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.appName('CTR_Model_Evaluation').getOrCreate()\n",
    "processed_dir = os.path.join('..', 'data', 'processed')\n",
    "models_dir = os.path.join('..', 'models')\n",
    "\n",
    "# Load test data\n",
    "test_df = spark.read.parquet(os.path.join(processed_dir, 'test_df.parquet'))\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='clk', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "\n",
    "models_to_evaluate = {\n",
    "    'LogisticRegression': LogisticRegressionModel.load(os.path.join(models_dir, 'LogisticRegression_spark_model')),\n",
    "    'RandomForest': RandomForestClassificationModel.load(os.path.join(models_dir, 'RandomForest_spark_model')),\n",
    "    'GBT': GBTClassificationModel.load(os.path.join(models_dir, 'GBT_spark_model'))\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models_to_evaluate.items():\n",
    "    predictions = model.transform(test_df)\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    # Compute accuracy\n",
    "    accuracy = predictions.filter(predictions['prediction'] == predictions['clk']).count() / float(predictions.count())\n",
    "    results.append((name, auc, accuracy))\n",
    "    print(f\"Model: {name} – AUC: {auc:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix for the best model\n",
    "best_model_name, best_auc, _ = max(results, key=lambda x: x[1])\n",
    "best_model = models_to_evaluate[best_model_name]\n",
    "best_predictions = best_model.transform(test_df)\n",
    "# Convert predictions to RDD of (prediction, label)\n",
    "pred_and_labels = best_predictions.select('prediction', 'clk').rdd.map(lambda row: (row.prediction, row.clk))\n",
    "metrics = MulticlassMetrics(pred_and_labels)\n",
    "cm = metrics.confusionMatrix().toArray()\n",
    "\n",
    "print(f\"\n",
    "Confusion matrix for {best_model_name}:\n",
    "\", cm)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
