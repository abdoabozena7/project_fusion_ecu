{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36b9bff",
   "metadata": {},
   "source": [
    "# 01 – Data Overview (PySpark)\n",
    "\n",
    "This notebook introduces the Taobao CTR dataset. We will load up to **500k rows** from each CSV file using PySpark (to handle large data volumes) and inspect the schema and basic statistics. \n",
    "\n",
    "**Note on Kaggle download**: If you have Kaggle API credentials set up, you can automatically download the dataset. Uncomment the lines in the code cell below and ensure your `kaggle.json` credentials file is in the correct location (`~/.kaggle`). Otherwise, place the CSV files manually into `data/raw`. \n",
    "\n",
    "* Data sources:\n",
    "  * `raw_sample.csv` – user impressions and clicks\n",
    "  * `ad_feature.csv` – advertisement metadata\n",
    "  * `user_profile.csv` – user demographics\n",
    "  * `behavior_log.csv` – user behaviour logs (page view, cart, favourite, purchase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f607956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n",
      "Raw data directory: D:\\projects\\Ai\\project_fusion_ecu\\data\\raw\n",
      "raw_sample      -> D:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\raw_sample.csv | exists: True\n",
      "ad_feature      -> D:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\ad_feature.csv | exists: True\n",
      "user_profile    -> D:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\user_profile.csv | exists: True\n",
      "behavior_log    -> D:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\behavior_log.csv | exists: True\n",
      "Loaded dataframes (random sample):\n",
      "user_df rows:      265613\n",
      "ad_df rows:        211615\n",
      "click_df rows:     500000\n",
      "behavior_df rows:  500000\n",
      "\n",
      "Schema for User Profile:\n",
      "root\n",
      " |-- userid : double (nullable = true)\n",
      " |--  cms_segid: double (nullable = true)\n",
      " |--  cms_group_id: double (nullable = true)\n",
      " |--  final_gender_code: double (nullable = true)\n",
      " |--  age_level: double (nullable = true)\n",
      " |--  pvalue_level: string (nullable = true)\n",
      " |--  shopping_level: double (nullable = true)\n",
      " |--  occupation: double (nullable = true)\n",
      " |--  new_user_class_level: string (nullable = true)\n",
      "\n",
      "\n",
      "Schema for Ad Feature:\n",
      "root\n",
      " |-- adgroup_id: double (nullable = true)\n",
      " |--  cate_id: double (nullable = true)\n",
      " |--  campaign_id: double (nullable = true)\n",
      " |--  customer: double (nullable = true)\n",
      " |--  brand : string (nullable = true)\n",
      " |--  price: double (nullable = true)\n",
      "\n",
      "\n",
      "Schema for Click Log:\n",
      "root\n",
      " |-- user: integer (nullable = true)\n",
      " |-- time_stamp: integer (nullable = true)\n",
      " |-- adgroup_id: integer (nullable = true)\n",
      " |-- pid: string (nullable = true)\n",
      " |-- nonclk: integer (nullable = true)\n",
      " |-- clk: integer (nullable = true)\n",
      "\n",
      "\n",
      "Schema for Behavior Log:\n",
      "root\n",
      " |-- user: integer (nullable = true)\n",
      " |-- time_stamp: integer (nullable = true)\n",
      " |-- btag: string (nullable = true)\n",
      " |-- cate: integer (nullable = true)\n",
      " |-- brand: integer (nullable = true)\n",
      "\n",
      "\n",
      "Numeric columns in click_df: ['user', 'time_stamp', 'adgroup_id', 'nonclk', 'clk']\n",
      "+-------+------------------+-------------------+-----------------+-----------------+-------------------+\n",
      "|summary|              user|         time_stamp|       adgroup_id|           nonclk|                clk|\n",
      "+-------+------------------+-------------------+-----------------+-----------------+-------------------+\n",
      "|  count|            500000|             500000|           500000|           500000|             500000|\n",
      "|   mean|     577328.160264|1.494351764478228E9|    175821.116976|          0.95075|            0.04925|\n",
      "| stddev|329603.07425514236| 198649.23953193123|90660.27693700096|0.216389766738319|0.21638976673831906|\n",
      "|    min|                 1|         1494000000|               11|                0|                  0|\n",
      "|    max|           1141726|         1494691174|           326071|                1|                  1|\n",
      "+-------+------------------+-------------------+-----------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Start Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"CTR_Data_Overview\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", 200) \n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\") # Reduce log verbosity\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "# Define project and data paths\n",
    "project_root = r\"D:\\projects\\Ai\\project_fusion_ecu\"\n",
    "\n",
    "raw_dir = os.path.join(project_root, \"data\", \"raw\")\n",
    "print(\"Raw data directory:\", raw_dir)\n",
    "\n",
    "# Expected file names\n",
    "file_names = {\n",
    "    \"raw_sample\": \"raw_sample.csv\",\n",
    "    \"ad_feature\": \"ad_feature.csv\",\n",
    "    \"user_profile\": \"user_profile.csv\",\n",
    "    \"behavior_log\": \"behavior_log.csv\",\n",
    "}\n",
    "\n",
    "# Verify that files exist\n",
    "missing = []\n",
    "for key, fname in file_names.items():\n",
    "    path = os.path.join(raw_dir, fname)\n",
    "    exists = os.path.exists(path)\n",
    "    print(f\"{key:15s} -> {path} | exists: {exists}\")\n",
    "    if not exists:\n",
    "        missing.append(fname)\n",
    "\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing files in data/raw: \"\n",
    "        + \", \".join(missing)\n",
    "        + \"\\nPlease copy them into \"\n",
    "        + raw_dir\n",
    "        + \" with the exact same names.\"\n",
    "    )\n",
    "\n",
    "# Load RANDOM sample \n",
    "TARGET_ROWS = 500_000\n",
    "\n",
    "SAMPLE_FRACTION = 0.25 \n",
    "SEED = 42\n",
    "\n",
    "def read_random_sample_csv(path, target_rows=500_000, frac=0.25, seed=42, cache_df=True):\n",
    "    df = (\n",
    "        spark.read.csv(path, header=True, inferSchema=True)\n",
    "        .sample(withReplacement=False, fraction=frac, seed=seed)  # random sampling\n",
    "        .limit(target_rows) \n",
    "    )\n",
    "    if cache_df:  #due to lazy evaluation\n",
    "        df = df.cache()\n",
    "    return df\n",
    "\n",
    "user_df = read_random_sample_csv(\n",
    "    os.path.join(raw_dir, file_names[\"user_profile\"]),\n",
    "    target_rows=TARGET_ROWS,\n",
    "    frac=SAMPLE_FRACTION,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "ad_df = read_random_sample_csv(\n",
    "    os.path.join(raw_dir, file_names[\"ad_feature\"]),\n",
    "    target_rows=TARGET_ROWS,\n",
    "    frac=SAMPLE_FRACTION,\n",
    "    seed=SEED + 1, # different seed for different samples\n",
    ")\n",
    "\n",
    "click_df = read_random_sample_csv(\n",
    "    os.path.join(raw_dir, file_names[\"raw_sample\"]),\n",
    "    target_rows=TARGET_ROWS,\n",
    "    frac=SAMPLE_FRACTION,\n",
    "    seed=SEED + 2,\n",
    ")\n",
    "\n",
    "behavior_df = read_random_sample_csv(\n",
    "    os.path.join(raw_dir, file_names[\"behavior_log\"]),\n",
    "    target_rows=TARGET_ROWS,\n",
    "    frac=SAMPLE_FRACTION,\n",
    "    seed=SEED + 3,\n",
    ")\n",
    "\n",
    "print(\"Loaded dataframes (random sample):\")\n",
    "print(\"user_df rows:     \", user_df.count())\n",
    "print(\"ad_df rows:       \", ad_df.count())\n",
    "print(\"click_df rows:    \", click_df.count())\n",
    "print(\"behavior_df rows: \", behavior_df.count())\n",
    "\n",
    "# Inspect schemas\n",
    "for name, df in [\n",
    "    (\"User Profile\", user_df),\n",
    "    (\"Ad Feature\", ad_df),\n",
    "    (\"Click Log\", click_df),\n",
    "    (\"Behavior Log\", behavior_df),\n",
    "]:\n",
    "    print(f\"\\nSchema for {name}:\")\n",
    "    df.printSchema()\n",
    "\n",
    "# Basic statistics for numeric columns in click_df\n",
    "numeric_cols = [c for c, dtype in click_df.dtypes if dtype in (\"int\", \"bigint\", \"double\", \"float\")]\n",
    "print(\"\\nNumeric columns in click_df:\", numeric_cols)\n",
    "\n",
    "if numeric_cols:\n",
    "    click_df.describe(numeric_cols).show()\n",
    "else:\n",
    "    print(\"No numeric columns found in click_df.\")\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
