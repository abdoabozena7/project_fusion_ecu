{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36b9bff",
   "metadata": {},
   "source": [
    "# 01 – Data Overview (PySpark)\n",
    "\n",
    "This notebook introduces the Taobao CTR dataset. We will load up to **1 million rows** from each CSV file using PySpark (to handle large data volumes) and inspect the schema and basic statistics. \n",
    "\n",
    "**Note on Kaggle download**: If you have Kaggle API credentials set up, you can automatically download the dataset. Uncomment the lines in the code cell below and ensure your `kaggle.json` credentials file is in the correct location (`~/.kaggle`). Otherwise, place the CSV files manually into `data/raw`. \n",
    "\n",
    "* Data sources:\n",
    "  * `raw_sample.csv` – user impressions and clicks\n",
    "  * `ad_feature.csv` – advertisement metadata\n",
    "  * `user_profile.csv` – user demographics\n",
    "  * `behavior_log.csv` – user behaviour logs (page view, cart, favourite, purchase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1dabf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uncomment and configure the following if Kaggle API is available:\n",
    "# from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "# api = KaggleApi()\n",
    "# api.authenticate()\n",
    "# api.dataset_download_files('t/t', path='../data/raw', unzip=True)\n",
    "# The above will download the dataset into the raw data folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f607956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n",
      "Raw data directory: D:\\projects\\Ai\\project_fusion_ecu\\data\\raw\n",
      "raw_sample      -> D:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\raw_sample.csv | exists: True\n",
      "ad_feature      -> D:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\ad_feature.csv | exists: True\n",
      "user_profile    -> D:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\user_profile.csv | exists: True\n",
      "behavior_log    -> D:\\projects\\Ai\\project_fusion_ecu\\data\\raw\\behavior_log.csv | exists: True\n",
      "Loaded dataframes:\n",
      "user_df rows:      1000000\n",
      "ad_df rows:        846811\n",
      "click_df rows:     1000000\n",
      "behavior_df rows:  1000000\n",
      "\n",
      "Schema for User Profile:\n",
      "root\n",
      " |-- userid: integer (nullable = true)\n",
      " |-- cms_segid: integer (nullable = true)\n",
      " |-- cms_group_id: integer (nullable = true)\n",
      " |-- final_gender_code: integer (nullable = true)\n",
      " |-- age_level: integer (nullable = true)\n",
      " |-- pvalue_level: integer (nullable = true)\n",
      " |-- shopping_level: integer (nullable = true)\n",
      " |-- occupation: integer (nullable = true)\n",
      " |-- new_user_class_level : integer (nullable = true)\n",
      "\n",
      "\n",
      "Schema for Ad Feature:\n",
      "root\n",
      " |-- adgroup_id: double (nullable = true)\n",
      " |--  cate_id: double (nullable = true)\n",
      " |--  campaign_id: double (nullable = true)\n",
      " |--  customer: double (nullable = true)\n",
      " |--  brand : string (nullable = true)\n",
      " |--  price: double (nullable = true)\n",
      "\n",
      "\n",
      "Schema for Click Log:\n",
      "root\n",
      " |-- user: integer (nullable = true)\n",
      " |-- time_stamp: integer (nullable = true)\n",
      " |-- adgroup_id: integer (nullable = true)\n",
      " |-- pid: string (nullable = true)\n",
      " |-- nonclk: integer (nullable = true)\n",
      " |-- clk: integer (nullable = true)\n",
      "\n",
      "\n",
      "Schema for Behavior Log:\n",
      "root\n",
      " |-- user: integer (nullable = true)\n",
      " |-- time_stamp: integer (nullable = true)\n",
      " |-- btag: string (nullable = true)\n",
      " |-- cate: integer (nullable = true)\n",
      " |-- brand: integer (nullable = true)\n",
      "\n",
      "\n",
      "Numeric columns in click_df: ['user', 'time_stamp', 'adgroup_id', 'nonclk', 'clk']\n",
      "+-------+-----------------+-------------------+-----------------+-------------------+-------------------+\n",
      "|summary|             user|         time_stamp|       adgroup_id|             nonclk|                clk|\n",
      "+-------+-----------------+-------------------+-----------------+-------------------+-------------------+\n",
      "|  count|          1000000|            1000000|          1000000|            1000000|            1000000|\n",
      "|   mean|    578308.071826|1.494354575113817E9|     98206.100232|           0.950507|           0.049493|\n",
      "| stddev|329660.9347728377| 198616.57548744223|48714.72158102842|0.21689511288751806|0.21689511288751814|\n",
      "|    min|                1|         1494000000|                1|                  0|                  0|\n",
      "|    max|          1141726|         1494691175|           176356|                  1|                  1|\n",
      "+-------+-----------------+-------------------+-----------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# ============================\n",
    "# 1. Start Spark session\n",
    "# ============================\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"CTR_Data_Overview\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "# ============================\n",
    "# 2. Define project and data paths\n",
    "# ============================\n",
    "\n",
    "# Option A: hard-coded project root (recommended for now)\n",
    "project_root = r\"D:\\projects\\Ai\\project_fusion_ecu\"\n",
    "\n",
    "# Option B (alternative): if this notebook lives inside `project_fusion_ecu/notebooks`\n",
    "# project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "raw_dir = os.path.join(project_root, \"data\", \"raw\")\n",
    "print(\"Raw data directory:\", raw_dir)\n",
    "\n",
    "# ============================\n",
    "# 3. Expected file names\n",
    "# ============================\n",
    "file_names = {\n",
    "    \"raw_sample\": \"raw_sample.csv\",\n",
    "    \"ad_feature\": \"ad_feature.csv\",\n",
    "    \"user_profile\": \"user_profile.csv\",\n",
    "    # If your file is named `raw_behavior_log.csv` change the next line accordingly\n",
    "    \"behavior_log\": \"behavior_log.csv\",\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# 4. Verify that files exist\n",
    "# ============================\n",
    "missing = []\n",
    "for key, fname in file_names.items():\n",
    "    path = os.path.join(raw_dir, fname)\n",
    "    exists = os.path.exists(path)\n",
    "    print(f\"{key:15s} -> {path} | exists: {exists}\")\n",
    "    if not exists:\n",
    "        missing.append(fname)\n",
    "\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing files in data/raw: \"\n",
    "        + \", \".join(missing)\n",
    "        + \"\\nPlease copy them into \"\n",
    "        + raw_dir\n",
    "        + \" with the exact same names.\"\n",
    "    )\n",
    "\n",
    "# ============================\n",
    "# 5. Load up to 1M rows per file\n",
    "# ============================\n",
    "user_df = (\n",
    "    spark.read.csv(\n",
    "        os.path.join(raw_dir, file_names[\"user_profile\"]),\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "    )\n",
    "    .limit(1_000_000)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "ad_df = (\n",
    "    spark.read.csv(\n",
    "        os.path.join(raw_dir, file_names[\"ad_feature\"]),\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "    )\n",
    "    .limit(1_000_000)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "click_df = (\n",
    "    spark.read.csv(\n",
    "        os.path.join(raw_dir, file_names[\"raw_sample\"]),\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "    )\n",
    "    .limit(1_000_000)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "behavior_df = (\n",
    "    spark.read.csv(\n",
    "        os.path.join(raw_dir, file_names[\"behavior_log\"]),\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "    )\n",
    "    .limit(1_000_000)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "print(\"Loaded dataframes:\")\n",
    "print(\"user_df rows:     \", user_df.count())\n",
    "print(\"ad_df rows:       \", ad_df.count())\n",
    "print(\"click_df rows:    \", click_df.count())\n",
    "print(\"behavior_df rows: \", behavior_df.count())\n",
    "\n",
    "# ============================\n",
    "# 6. Inspect schemas\n",
    "# ============================\n",
    "for name, df in [\n",
    "    (\"User Profile\", user_df),\n",
    "    (\"Ad Feature\", ad_df),\n",
    "    (\"Click Log\", click_df),\n",
    "    (\"Behavior Log\", behavior_df),\n",
    "]:\n",
    "    print(f\"\\nSchema for {name}:\")\n",
    "    df.printSchema()\n",
    "\n",
    "# ============================\n",
    "# 7. Basic statistics for numeric columns in click_df\n",
    "# ============================\n",
    "numeric_cols = [c for c, dtype in click_df.dtypes if dtype in (\"int\", \"bigint\", \"double\")]\n",
    "print(\"\\nNumeric columns in click_df:\", numeric_cols)\n",
    "\n",
    "if numeric_cols:\n",
    "    click_df.describe(numeric_cols).show()\n",
    "else:\n",
    "    print(\"No numeric columns found in click_df.\")\n",
    "\n",
    "# ============================\n",
    "# 8. Stop Spark session (optional)\n",
    "# ============================\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
