{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa32c5a",
   "metadata": {},
   "source": [
    "# 04 – Data Preprocessing (PySpark ML)\n",
    "\n",
    "This notebook prepares the fused data for machine learning.  We encode categorical features, scale numerical features, and split the data into training and testing sets.  We also handle class imbalance by oversampling the minority class using the `imbalanced-learn` library after converting to Pandas.  Our goal is to create a balanced and well‑structured dataset for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d11776df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded fused: (50000, 29)\n",
      "target: label | user: user | time: time_stamp\n",
      "Dropped null rows: 0\n",
      "Working df: (50000, 29) pos_rate: 0.05134\n",
      "Removed by name blacklist: ['nonclk', 'clk', 'time_stamp_str']\n",
      "Temporal split cutoff: 1494682302\n",
      "train: (40000, 26) pos: 0.051525\n",
      "test : (10000, 26) pos: 0.0506\n",
      "X_train raw: (40000, 23) X_test raw: (10000, 23)\n",
      "categorical: 14 numeric: 9\n",
      "scale_pos_weight: 18.40805434255216\n",
      "Encoded shapes: (40000, 29) (10000, 29)\n",
      "Saved leakage report: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\leakage_report.csv\n",
      "\n",
      "[LEAKAGE GUARD] No obvious leaked features found by exact/AUC scans.\n",
      "After leakage guard: (40000, 29) (10000, 29)\n",
      "Saved importances: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\feature_importances.csv\n",
      "\n",
      "Saved:\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\X_train_processed.npz\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\X_test_processed.npz\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\y_train.csv\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\y_test.csv\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\top_features.csv\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\scale_pos_weight.txt\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\preprocessor.joblib\n",
      "\n",
      "[Preprocessing done with leakage guard.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy import sparse\n",
    "from joblib import dump\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ==========================================================\n",
    "# 1) Paths + Load fused data\n",
    "# ==========================================================\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "processed_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "Path(processed_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fused_path = os.path.join(processed_dir, \"fused_data.csv\")\n",
    "if not os.path.exists(fused_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing: {fused_path}\\nRun fusion notebook first.\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(fused_path)\n",
    "print(\"Loaded fused:\", df.shape)\n",
    "\n",
    "# ==========================================================\n",
    "# 2) Target + user + time\n",
    "# ==========================================================\n",
    "target_col = \"label\" if \"label\" in df.columns else (\"clk\" if \"clk\" in df.columns else None)\n",
    "if target_col is None:\n",
    "    raise ValueError('No target column found (\"label\" or \"clk\").')\n",
    "\n",
    "user_col = next((c for c in [\"user\", \"userid\", \"nick\"] if c in df.columns), None)\n",
    "if user_col is None:\n",
    "    raise ValueError('No user ID column found (\"user\" or \"userid\" or \"nick\").')\n",
    "\n",
    "time_col = next((c for c in [\"time_stamp\", \"timestamp\", \"date_time\", \"datetime\", \"time\"] if c in df.columns), None)\n",
    "\n",
    "print(\"target:\", target_col, \"| user:\", user_col, \"| time:\", time_col)\n",
    "\n",
    "# drop nulls\n",
    "drop_cols = [user_col, target_col] + ([time_col] if time_col else [])\n",
    "before = len(df)\n",
    "df = df.dropna(subset=drop_cols)\n",
    "print(\"Dropped null rows:\", before - len(df))\n",
    "\n",
    "df[target_col] = df[target_col].astype(int)\n",
    "\n",
    "# ==========================================================\n",
    "# 3) Cap rows (optional)\n",
    "# ==========================================================\n",
    "MAX_ROWS = 200_000\n",
    "if time_col is not None:\n",
    "    df = df.sort_values(time_col).reset_index(drop=True)\n",
    "    if len(df) > MAX_ROWS:\n",
    "        df = df.iloc[-MAX_ROWS:].copy()\n",
    "        print(f\"Temporal tail kept: {MAX_ROWS}\")\n",
    "else:\n",
    "    if len(df) > MAX_ROWS:\n",
    "        df = df.sample(n=MAX_ROWS, random_state=42).copy()\n",
    "        print(f\"Random sample kept: {MAX_ROWS}\")\n",
    "\n",
    "print(\"Working df:\", df.shape, \"pos_rate:\", df[target_col].mean())\n",
    "\n",
    "# ==========================================================\n",
    "# 4) Remove obvious leakage columns (names)\n",
    "# ==========================================================\n",
    "# Strong rule: anything that smells like target/after-click aggregates is removed\n",
    "name_blacklist_substrings = [\n",
    "    \"label\", \"clk\", \"click\", \"nonclk\", \"noclk\", \"impression\", \"ctr\", \"conversion\",\n",
    "    \"time_stamp_str\", \"is_click\", \"clicked\"\n",
    "]\n",
    "removed = []\n",
    "for c in list(df.columns):\n",
    "    if c == target_col:\n",
    "        continue\n",
    "    low = c.lower()\n",
    "    if any(s in low for s in name_blacklist_substrings):\n",
    "        # allow time_col to remain for split only\n",
    "        if time_col is not None and c == time_col:\n",
    "            continue\n",
    "        # allow user_col to remain for drop later\n",
    "        if c == user_col:\n",
    "            continue\n",
    "        df.drop(columns=[c], inplace=True)\n",
    "        removed.append(c)\n",
    "\n",
    "if removed:\n",
    "    print(\"Removed by name blacklist:\", removed)\n",
    "\n",
    "# ==========================================================\n",
    "# 5) Temporal split 80/20 (fallback stratified)\n",
    "# ==========================================================\n",
    "if time_col is not None:\n",
    "    df = df.sort_values(time_col).reset_index(drop=True)\n",
    "    split_idx = int(len(df) * 0.8)\n",
    "    train_df = df.iloc[:split_idx].copy()\n",
    "    test_df  = df.iloc[split_idx:].copy()\n",
    "    print(\"Temporal split cutoff:\", df.iloc[split_idx][time_col])\n",
    "else:\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=0.2, stratify=df[target_col], random_state=42\n",
    "    )\n",
    "    print(\"Stratified split (no time col).\")\n",
    "\n",
    "print(\"train:\", train_df.shape, \"pos:\", train_df[target_col].mean())\n",
    "print(\"test :\", test_df.shape,  \"pos:\", test_df[target_col].mean())\n",
    "\n",
    "# ==========================================================\n",
    "# 6) Build X/y\n",
    "# ==========================================================\n",
    "def build_xy(sub_df: pd.DataFrame):\n",
    "    y = sub_df[target_col].astype(int)\n",
    "    X = sub_df.drop(columns=[target_col], errors=\"ignore\")\n",
    "\n",
    "    # time only for splitting, not as a feature\n",
    "    if time_col is not None and time_col in X.columns:\n",
    "        X = X.drop(columns=[time_col], errors=\"ignore\")\n",
    "\n",
    "    # user id removed from features\n",
    "    if user_col in X.columns:\n",
    "        X = X.drop(columns=[user_col], errors=\"ignore\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = build_xy(train_df)\n",
    "X_test,  y_test  = build_xy(test_df)\n",
    "\n",
    "print(\"X_train raw:\", X_train.shape, \"X_test raw:\", X_test.shape)\n",
    "\n",
    "# ==========================================================\n",
    "# 7) Feature typing\n",
    "# ==========================================================\n",
    "force_categorical = set([\n",
    "    \"adgroup_id\", \"pid\", \"campaign_id\", \"customer\", \"cate_id\", \"brand\",\n",
    "    \"cms_segid\", \"cms_group_id\", \"final_gender_code\", \"age_level\",\n",
    "    \"shopping_level\", \"occupation\", \"pvalue_level\", \"new_user_class_level\",\n",
    "])\n",
    "force_categorical = {c for c in force_categorical if c in X_train.columns}\n",
    "\n",
    "categorical_cols = list(X_train.select_dtypes(include=[\"object\", \"category\"]).columns)\n",
    "for c in force_categorical:\n",
    "    if c not in categorical_cols:\n",
    "        categorical_cols.append(c)\n",
    "\n",
    "numeric_cols = [c for c in X_train.columns if c not in categorical_cols]\n",
    "\n",
    "print(\"categorical:\", len(categorical_cols), \"numeric:\", len(numeric_cols))\n",
    "\n",
    "# ==========================================================\n",
    "# 8) Missing indicators + log1p\n",
    "# ==========================================================\n",
    "key_missing_cols = [c for c in [\"price\", \"age_level\", \"final_gender_code\", \"shopping_level\", \"pvalue_level\", \"cms_segid\"] if c in X_train.columns]\n",
    "for c in key_missing_cols:\n",
    "    X_train[f\"is_missing_{c}\"] = X_train[c].isna().astype(int)\n",
    "    X_test[f\"is_missing_{c}\"]  = X_test[c].isna().astype(int)\n",
    "    if f\"is_missing_{c}\" not in numeric_cols:\n",
    "        numeric_cols.append(f\"is_missing_{c}\")\n",
    "\n",
    "for c in [\"pv\", \"cart\", \"fav\", \"buy\"]:\n",
    "    if c in X_train.columns:\n",
    "        X_train[c] = pd.to_numeric(X_train[c], errors=\"coerce\")\n",
    "        X_test[c]  = pd.to_numeric(X_test[c], errors=\"coerce\")\n",
    "        X_train[c] = np.log1p(X_train[c])\n",
    "        X_test[c]  = np.log1p(X_test[c])\n",
    "\n",
    "# ==========================================================\n",
    "# 9) scale_pos_weight\n",
    "# ==========================================================\n",
    "pos = int(y_train.sum())\n",
    "neg = int(len(y_train) - pos)\n",
    "scale_pos_weight = (neg / pos) if pos > 0 else 1.0\n",
    "print(\"scale_pos_weight:\", scale_pos_weight)\n",
    "\n",
    "# ==========================================================\n",
    "# 10) Encoding (NO OHE)\n",
    "# ==========================================================\n",
    "HIGH_CARD_THRESHOLD = 30\n",
    "SMOOTHING = 50\n",
    "N_SPLITS_TE = 5\n",
    "RANDOM_STATE = 42\n",
    "global_mean = float(y_train.mean())\n",
    "\n",
    "X_train_enc = X_train.copy()\n",
    "X_test_enc  = X_test.copy()\n",
    "\n",
    "for c in categorical_cols:\n",
    "    X_train_enc[c] = X_train_enc[c].astype(\"string\")\n",
    "    X_test_enc[c]  = X_test_enc[c].astype(\"string\")\n",
    "\n",
    "high_card_cols, low_card_cols = [], []\n",
    "for c in categorical_cols:\n",
    "    nunq = X_train_enc[c].nunique(dropna=True)\n",
    "    (high_card_cols if nunq > HIGH_CARD_THRESHOLD else low_card_cols).append(c)\n",
    "\n",
    "# low-card label encoding\n",
    "low_card_mappings = {}\n",
    "for c in low_card_cols:\n",
    "    cats = X_train_enc[c].dropna().unique().tolist()\n",
    "    cats = sorted([str(x) for x in cats])\n",
    "    mapping = {cat: i for i, cat in enumerate(cats)}\n",
    "    low_card_mappings[c] = mapping\n",
    "    X_train_enc[c] = X_train_enc[c].map(mapping).fillna(-1).astype(int)\n",
    "    X_test_enc[c]  = X_test_enc[c].map(mapping).fillna(-1).astype(int)\n",
    "\n",
    "# high-card target encoding (OOF)\n",
    "high_card_mappings = {}\n",
    "if high_card_cols:\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS_TE, shuffle=True, random_state=RANDOM_STATE)\n",
    "    for c in high_card_cols:\n",
    "        oof = pd.Series(index=X_train_enc.index, dtype=float)\n",
    "\n",
    "        for tr_idx, val_idx in skf.split(X_train_enc, y_train):\n",
    "            tr_x = X_train_enc.iloc[tr_idx]\n",
    "            tr_y = y_train.iloc[tr_idx]\n",
    "            stats = pd.DataFrame({c: tr_x[c], \"y\": tr_y.values}).groupby(c)[\"y\"].agg([\"count\", \"mean\"])\n",
    "\n",
    "            enc_map = {}\n",
    "            for cat, row in stats.iterrows():\n",
    "                cnt = float(row[\"count\"])\n",
    "                mu  = float(row[\"mean\"])\n",
    "                enc_val = (cnt * mu + SMOOTHING * global_mean) / (cnt + SMOOTHING)\n",
    "                enc_map[str(cat)] = enc_val\n",
    "\n",
    "            val_x = X_train_enc.iloc[val_idx]\n",
    "            oof.iloc[val_idx] = val_x[c].map(enc_map).astype(float)\n",
    "\n",
    "        X_train_enc[c] = oof.fillna(global_mean).astype(float)\n",
    "\n",
    "        stats_full = pd.DataFrame({c: X_train[c].astype(\"string\"), \"y\": y_train.values}).groupby(c)[\"y\"].agg([\"count\", \"mean\"])\n",
    "        final_map = {}\n",
    "        for cat, row in stats_full.iterrows():\n",
    "            cnt = float(row[\"count\"])\n",
    "            mu  = float(row[\"mean\"])\n",
    "            enc_val = (cnt * mu + SMOOTHING * global_mean) / (cnt + SMOOTHING)\n",
    "            final_map[str(cat)] = enc_val\n",
    "\n",
    "        high_card_mappings[c] = {\"mapping\": final_map, \"global_mean\": global_mean, \"smoothing\": SMOOTHING}\n",
    "        X_test_enc[c] = X_test[c].astype(\"string\").map(final_map).fillna(global_mean).astype(float)\n",
    "\n",
    "# now everything numeric\n",
    "for c in categorical_cols:\n",
    "    if c not in numeric_cols:\n",
    "        numeric_cols.append(c)\n",
    "\n",
    "numeric_cols = [c for c in numeric_cols if c in X_train_enc.columns]\n",
    "\n",
    "# force float\n",
    "X_train_enc[numeric_cols] = X_train_enc[numeric_cols].apply(pd.to_numeric, errors=\"coerce\").astype(np.float64)\n",
    "X_test_enc[numeric_cols]  = X_test_enc[numeric_cols].apply(pd.to_numeric, errors=\"coerce\").astype(np.float64)\n",
    "\n",
    "# impute + scale\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "scaler  = StandardScaler()\n",
    "\n",
    "Xtr_num = scaler.fit_transform(imputer.fit_transform(X_train_enc[numeric_cols]))\n",
    "Xte_num = scaler.transform(imputer.transform(X_test_enc[numeric_cols]))\n",
    "\n",
    "X_train_enc.loc[:, numeric_cols] = Xtr_num.astype(np.float64)\n",
    "X_test_enc.loc[:, numeric_cols]  = Xte_num.astype(np.float64)\n",
    "\n",
    "# align columns\n",
    "X_test_enc = X_test_enc.reindex(columns=X_train_enc.columns, fill_value=0.0)\n",
    "\n",
    "print(\"Encoded shapes:\", X_train_enc.shape, X_test_enc.shape)\n",
    "\n",
    "# ==========================================================\n",
    "# 11) LEAKAGE GUARD (critical)\n",
    "# ==========================================================\n",
    "# 11.1 exact equality checks (y or 1-y)\n",
    "leak_cols_exact = []\n",
    "y_arr = y_train.values.astype(int)\n",
    "\n",
    "for c in X_train_enc.columns:\n",
    "    col = X_train_enc[c].values\n",
    "    # skip nan checks by safe conversion\n",
    "    col_num = np.array(col, dtype=float)\n",
    "    if np.all(np.isfinite(col_num)):\n",
    "        # try rounding for \"almost binary\"\n",
    "        col_bin = np.round(col_num).astype(int)\n",
    "        if col_bin.shape[0] == y_arr.shape[0]:\n",
    "            if np.array_equal(col_bin, y_arr) or np.array_equal(1 - col_bin, y_arr):\n",
    "                leak_cols_exact.append(c)\n",
    "\n",
    "# 11.2 single-feature AUC scan on a holdout split inside TRAIN\n",
    "# if a single feature yields AUC ~ 1, it's almost surely leakage\n",
    "Xtr_sub, Xva_sub, ytr_sub, yva_sub = train_test_split(\n",
    "    X_train_enc, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "leak_auc_rows = []\n",
    "leak_cols_auc = []\n",
    "for c in X_train_enc.columns:\n",
    "    x = Xva_sub[c].values.astype(float)\n",
    "    if np.nanstd(x) < 1e-12:\n",
    "        continue\n",
    "    try:\n",
    "        auc = roc_auc_score(yva_sub, x)\n",
    "        auc = max(auc, 1.0 - auc)  # make it symmetric\n",
    "        leak_auc_rows.append((c, float(auc)))\n",
    "        if auc > 0.999:  # near-perfect from single feature -> leakage\n",
    "            leak_cols_auc.append(c)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "leakage_report = pd.DataFrame(leak_auc_rows, columns=[\"feature\", \"single_feature_auc\"]).sort_values(\"single_feature_auc\", ascending=False)\n",
    "leak_report_path = os.path.join(processed_dir, \"leakage_report.csv\")\n",
    "leakage_report.to_csv(leak_report_path, index=False)\n",
    "print(\"Saved leakage report:\", leak_report_path)\n",
    "\n",
    "leak_all = sorted(set(leak_cols_exact + leak_cols_auc))\n",
    "if leak_all:\n",
    "    print(\"\\n[LEAKAGE GUARD] Dropping leaked features:\", leak_all)\n",
    "    X_train_enc = X_train_enc.drop(columns=leak_all, errors=\"ignore\")\n",
    "    X_test_enc  = X_test_enc.drop(columns=leak_all, errors=\"ignore\")\n",
    "else:\n",
    "    print(\"\\n[LEAKAGE GUARD] No obvious leaked features found by exact/AUC scans.\")\n",
    "\n",
    "# final align\n",
    "X_test_enc = X_test_enc.reindex(columns=X_train_enc.columns, fill_value=0.0)\n",
    "print(\"After leakage guard:\", X_train_enc.shape, X_test_enc.shape)\n",
    "\n",
    "# ==========================================================\n",
    "# 12) Feature importance (optional, keep all)\n",
    "# ==========================================================\n",
    "importance_model = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    tree_method=\"hist\",\n",
    ")\n",
    "importance_model.fit(X_train_enc, y_train)\n",
    "\n",
    "imp_df = pd.DataFrame({\"feature\": X_train_enc.columns, \"importance\": importance_model.feature_importances_}).sort_values(\"importance\", ascending=False)\n",
    "imp_path = os.path.join(processed_dir, \"feature_importances.csv\")\n",
    "imp_df.to_csv(imp_path, index=False)\n",
    "print(\"Saved importances:\", imp_path)\n",
    "\n",
    "top_features = imp_df[\"feature\"].tolist()\n",
    "X_train_final = X_train_enc[top_features]\n",
    "X_test_final  = X_test_enc[top_features]\n",
    "\n",
    "# ==========================================================\n",
    "# 13) Save outputs\n",
    "# ==========================================================\n",
    "X_train_path = os.path.join(processed_dir, \"X_train_processed.npz\")\n",
    "X_test_path  = os.path.join(processed_dir, \"X_test_processed.npz\")\n",
    "y_train_path = os.path.join(processed_dir, \"y_train.csv\")\n",
    "y_test_path  = os.path.join(processed_dir, \"y_test.csv\")\n",
    "top_features_path = os.path.join(processed_dir, \"top_features.csv\")\n",
    "scale_pos_weight_path = os.path.join(processed_dir, \"scale_pos_weight.txt\")\n",
    "preprocessor_path = os.path.join(processed_dir, \"preprocessor.joblib\")\n",
    "\n",
    "sparse.save_npz(X_train_path, sparse.csr_matrix(X_train_final.values))\n",
    "sparse.save_npz(X_test_path,  sparse.csr_matrix(X_test_final.values))\n",
    "\n",
    "pd.Series(y_train, name=target_col).to_csv(y_train_path, index=False)\n",
    "pd.Series(y_test, name=target_col).to_csv(y_test_path, index=False)\n",
    "\n",
    "with open(scale_pos_weight_path, \"w\") as f:\n",
    "    f.write(str(scale_pos_weight))\n",
    "\n",
    "pd.DataFrame({\"feature\": top_features}).to_csv(top_features_path, index=False)\n",
    "\n",
    "preprocessor = {\n",
    "    \"target_col\": target_col,\n",
    "    \"user_col_removed\": user_col,\n",
    "    \"time_col_used_for_split\": time_col,\n",
    "    \"categorical_cols_original\": categorical_cols,\n",
    "    \"numeric_cols\": numeric_cols,\n",
    "    \"high_card_cols\": high_card_cols,\n",
    "    \"low_card_cols\": low_card_cols,\n",
    "    \"high_card_mappings\": high_card_mappings,\n",
    "    \"low_card_mappings\": low_card_mappings,\n",
    "    \"global_mean\": global_mean,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"top_features\": top_features,\n",
    "    \"leakage_dropped_features\": leak_all,\n",
    "    \"leakage_report_path\": leak_report_path,\n",
    "    \"imputer_statistics\": imputer.statistics_.tolist(),\n",
    "    \"scaler_mean\": scaler.mean_.tolist(),\n",
    "    \"scaler_scale\": scaler.scale_.tolist(),\n",
    "}\n",
    "dump(preprocessor, preprocessor_path)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" -\", X_train_path)\n",
    "print(\" -\", X_test_path)\n",
    "print(\" -\", y_train_path)\n",
    "print(\" -\", y_test_path)\n",
    "print(\" -\", top_features_path)\n",
    "print(\" -\", scale_pos_weight_path)\n",
    "print(\" -\", preprocessor_path)\n",
    "print(\"\\n[Preprocessing done with leakage guard.]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b8847",
   "metadata": {},
   "source": [
    "### Modifications Summary\n",
    "This notebook has been updated to remove SMOTE-based oversampling and One-Hot encoding.\n",
    "Instead, class imbalance is handled by computing `scale_pos_weight` (negative/positive ratio) on the training data.\n",
    "Categorical features are encoded using **Target Encoding** for high-cardinality columns and **Label Encoding** for low-cardinality columns.\n",
    "Numeric features are imputed with the median and standardised.\n",
    "Feature importances are computed with an XGBoost model using the calculated `scale_pos_weight` and the top 50 features are selected.\n",
    "The processed datasets, label files, selected features, feature importances, scale position weight, and a preprocessor mapping are saved to the `data/processed` directory for subsequent notebooks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
