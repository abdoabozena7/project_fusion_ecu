{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84469e2a",
   "metadata": {},
   "source": [
    "# 04 – Data Preprocessing (PySpark)\n",
    "\n",
    "Here we prepare the fused data for machine‑learning.  Using Spark’s ML library we encode categorical variables, assemble features into a single vector, scale numeric columns and perform a train/test split.  We store the resulting pipeline and data splits for subsequent modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a71f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.appName('CTR_Preprocessing').getOrCreate()\n",
    "processed_dir = os.path.join('..', 'data', 'processed')\n",
    "\n",
    "# Load fused dataset from previous notebook (we saved as Parquet)\n",
    "full_df = spark.read.parquet(os.path.join(processed_dir, 'hybrid_fusion.parquet')) if os.path.exists(os.path.join(processed_dir, 'hybrid_fusion.parquet')) else None\n",
    "if full_df is None:\n",
    "    # If not saved yet, run EDA notebook to produce hybrid_fusion.parquet or change path accordingly\n",
    "    raise FileNotFoundError('Hybrid fused dataset not found.  Please run the EDA notebook to generate it.')\n",
    "\n",
    "# Identify target and feature columns\n",
    "target_col = 'clk'\n",
    "\n",
    "# Remove target from features\n",
    "data = full_df.dropna(subset=[target_col])\n",
    "feature_cols = [c for c in data.columns if c != target_col]\n",
    "\n",
    "# Infer categorical and numeric columns\n",
    "categorical_cols = [c for c, dtype in data.dtypes if dtype == 'string' and c != target_col]\n",
    "numeric_cols = [c for c, dtype in data.dtypes if dtype != 'string' and c != target_col]\n",
    "\n",
    "# Index and encode categorical variables\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f'{c}_indexed', handleInvalid='keep') for c in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=f'{c}_indexed', outputCol=f'{c}_ohe') for c in categorical_cols]\n",
    "\n",
    "# Assemble numeric and encoded categorical features\n",
    "assembler_inputs = [f'{c}_ohe' for c in categorical_cols] + numeric_cols\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol='raw_features')\n",
    "\n",
    "# Scale numeric features inside the vector\n",
    "scaler = StandardScaler(inputCol='raw_features', outputCol='features')\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\n",
    "\n",
    "# Fit pipeline and transform data\n",
    "pipeline_model = pipeline.fit(data)\n",
    "processed_df = pipeline_model.transform(data).select('features', target_col)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_df, test_df = processed_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Save processed data and pipeline\n",
    "processed_df.write.mode('overwrite').parquet(os.path.join(processed_dir, 'processed_for_model.parquet'))\n",
    "train_df.write.mode('overwrite').parquet(os.path.join(processed_dir, 'train_df.parquet'))\n",
    "test_df.write.mode('overwrite').parquet(os.path.join(processed_dir, 'test_df.parquet'))\n",
    "\n",
    "pipeline_model.write().overwrite().save(os.path.join(processed_dir, 'preprocessing_pipeline'))\n",
    "\n",
    "print('Preprocessing complete – data splits and pipeline saved.')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
