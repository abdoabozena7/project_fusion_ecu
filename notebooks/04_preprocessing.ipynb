{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa32c5a",
   "metadata": {},
   "source": [
    "# 04 – Data Preprocessing (PySpark ML)\n",
    "\n",
    "This notebook prepares the fused data for machine learning.  We encode categorical features, scale numerical features, and split the data into training and testing sets.  We also handle class imbalance by oversampling the minority class using the `imbalanced-learn` library after converting to Pandas.  Our goal is to create a balanced and well‑structured dataset for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d11776df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: d:\\projects\\Ai\\project_fusion_ecu\n",
      "Processed dir: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\n",
      "Full fused data shape: (500000, 25)\n",
      "Target: label\n",
      "User ID: user\n",
      "Time col: time_stamp\n",
      "Dropped rows with null user/target/time: 0\n",
      "After dropna shape: (500000, 25)\n",
      "Temporal sampled to last 200000 rows.\n",
      "Working dataframe shape: (200000, 25)\n",
      "Removed leakage columns: ['clk', 'nonclk', 'time_stamp_str']\n",
      "Columns after leakage removal (before split): 22\n",
      "\n",
      "Temporal split 80/20:\n",
      "cutoff time_stamp = 1494648979\n",
      "train_df shape: (160000, 22)\n",
      "test_df shape : (40000, 22)\n",
      "\n",
      "Positive rate:\n",
      "train pos_rate: 0.04765625\n",
      "test  pos_rate: 0.0457\n",
      "\n",
      "X_train shape (raw): (160000, 13)\n",
      "X_test shape  (raw): (40000, 13)\n",
      "y_train: (160000,) pos_rate: 0.04765625\n",
      "y_test : (40000,) pos_rate: 0.0457\n",
      "\n",
      "Numeric columns: 5\n",
      "Categorical columns: 8\n",
      "\n",
      "Computed scale_pos_weight: 19.9836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:397: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.70378486 -0.70378486 -0.70378486 ... -0.70378486  1.42088877\n",
      "  1.42088877]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train_enc.loc[:, numeric_cols] = X_train_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:397: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.8939619  0.8939619  0.8939619 ...  0.8939619  0.8939619 -1.1186159]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train_enc.loc[:, numeric_cols] = X_train_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:397: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.8939619  0.8939619  0.8939619 ...  0.8939619  0.8939619 -1.1186159]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train_enc.loc[:, numeric_cols] = X_train_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:397: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.8939619  0.8939619  0.8939619 ...  0.8939619  0.8939619 -1.1186159]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train_enc.loc[:, numeric_cols] = X_train_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:397: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.8939619  0.8939619  0.8939619 ...  0.8939619  0.8939619 -1.1186159]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train_enc.loc[:, numeric_cols] = X_train_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:397: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.8939619  0.8939619  0.8939619 ...  0.8939619  0.8939619 -1.1186159]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train_enc.loc[:, numeric_cols] = X_train_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:397: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.72640465 -0.72640465 -0.72640465 ... -0.72640465 -0.72640465\n",
      "  1.05498129]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train_enc.loc[:, numeric_cols] = X_train_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:397: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.74765277 -0.74765277 -0.74765277 ... -0.74765277 -0.74765277\n",
      " -0.15102511]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train_enc.loc[:, numeric_cols] = X_train_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:397: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.82441914 -0.82441914 -0.82441914 ... -0.82441914 -0.82441914\n",
      "  0.35084604]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train_enc.loc[:, numeric_cols] = X_train_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:397: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.86855524 -0.86855524 -0.86855524 ... -0.86855524 -0.86855524\n",
      "  1.01737902]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train_enc.loc[:, numeric_cols] = X_train_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:397: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.84791165 -0.84791165 -0.84791165 ... -0.84791165 -0.84791165\n",
      "  0.79966345]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train_enc.loc[:, numeric_cols] = X_train_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:397: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.86673694 -0.86673694 -0.86673694 ... -0.86673694 -0.86673694\n",
      "  1.23846254]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train_enc.loc[:, numeric_cols] = X_train_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:398: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 1.42088877 -0.70378486 -0.70378486 ... -0.70378486  1.42088877\n",
      "  1.42088877]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test_enc.loc[:, numeric_cols]  = X_test_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:398: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.8939619  0.8939619 -1.1186159 ...  0.8939619  0.8939619  0.8939619]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test_enc.loc[:, numeric_cols]  = X_test_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:398: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.8939619  0.8939619 -1.1186159 ...  0.8939619  0.8939619  0.8939619]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test_enc.loc[:, numeric_cols]  = X_test_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:398: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.8939619  0.8939619 -1.1186159 ...  0.8939619  0.8939619  0.8939619]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test_enc.loc[:, numeric_cols]  = X_test_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:398: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.8939619  0.8939619 -1.1186159 ...  0.8939619  0.8939619  0.8939619]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test_enc.loc[:, numeric_cols]  = X_test_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:398: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.8939619  0.8939619 -1.1186159 ...  0.8939619  0.8939619  0.8939619]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test_enc.loc[:, numeric_cols]  = X_test_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:398: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.72640465 -0.72640465  1.05498129 ... -0.72640465 -0.72640465\n",
      " -0.72640465]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test_enc.loc[:, numeric_cols]  = X_test_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:398: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.74765277 -0.74765277 -0.15102511 ... -0.74765277 -0.74765277\n",
      " -0.74765277]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test_enc.loc[:, numeric_cols]  = X_test_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:398: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.82441914 -0.82441914  0.35084604 ... -0.82441914 -0.82441914\n",
      " -0.82441914]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test_enc.loc[:, numeric_cols]  = X_test_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:398: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.86855524 -0.86855524  1.01737902 ... -0.86855524 -0.86855524\n",
      " -0.86855524]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test_enc.loc[:, numeric_cols]  = X_test_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:398: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.84791165 -0.84791165  0.38776967 ... -0.84791165 -0.84791165\n",
      " -0.84791165]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test_enc.loc[:, numeric_cols]  = X_test_num.astype(np.float64)\n",
      "C:\\Users\\A-plus\\AppData\\Local\\Temp\\ipykernel_14736\\3546869924.py:398: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.86673694 -0.86673694  0.53672938 ... -0.86673694 -0.86673694\n",
      " -0.86673694]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test_enc.loc[:, numeric_cols]  = X_test_num.astype(np.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed X_train shape: (160000, 19)\n",
      "Processed X_test shape : (40000, 19)\n",
      "\n",
      "Fitting XGBoost for feature importance...\n",
      "Saved feature importances to: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\feature_importances.csv\n",
      "Selected Top-K features: 19\n",
      "\n",
      "Saved processed files:\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\X_train_processed.npz\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\X_test_processed.npz\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\y_train.csv\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\y_test.csv\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\feature_importances.csv\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\top_features.csv\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\scale_pos_weight.txt\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\preprocessor.joblib\n",
      "\n",
      "[Preprocessing completed successfully — جاهز لملف التدريب.]\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 03_preprocessing.ipynb / preprocessing.py\n",
    "# Full Preprocessing Code (Temporal split + Safe encoding + No OHE)\n",
    "# ============================================\n",
    "\n",
    "# ============================================\n",
    "# 0. Imports and configuration\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy import sparse\n",
    "from joblib import dump\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ============================================\n",
    "# 1. Paths + Load fused data\n",
    "# ============================================\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "processed_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "Path(processed_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"Processed dir:\", processed_dir)\n",
    "\n",
    "fused_path = os.path.join(processed_dir, \"fused_data.csv\")\n",
    "if not os.path.exists(fused_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Fused data file not found at: {fused_path}\\n\"\n",
    "        \"Run 03_eda_fusion.ipynb first to generate fused_data.csv.\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(fused_path)\n",
    "print(\"Full fused data shape:\", df.shape)\n",
    "\n",
    "# ============================================\n",
    "# 2. Target + user + time columns\n",
    "# ============================================\n",
    "\n",
    "# Target\n",
    "if \"label\" in df.columns:\n",
    "    target_col = \"label\"\n",
    "elif \"clk\" in df.columns:\n",
    "    target_col = \"clk\"\n",
    "else:\n",
    "    raise ValueError('No target column found. Expected \"label\" or \"clk\".')\n",
    "\n",
    "# User id\n",
    "if \"user\" in df.columns:\n",
    "    user_col = \"user\"\n",
    "elif \"userid\" in df.columns:\n",
    "    user_col = \"userid\"\n",
    "elif \"nick\" in df.columns:\n",
    "    user_col = \"nick\"\n",
    "else:\n",
    "    raise ValueError('No user ID column found. Expected \"user\", \"userid\" or \"nick\".')\n",
    "\n",
    "# Time column (for temporal split)\n",
    "time_col = None\n",
    "for cand in [\"time_stamp\", \"timestamp\", \"date_time\", \"datetime\", \"time\"]:\n",
    "    if cand in df.columns:\n",
    "        time_col = cand\n",
    "        break\n",
    "\n",
    "print(\"Target:\", target_col)\n",
    "print(\"User ID:\", user_col)\n",
    "print(\"Time col:\", time_col)\n",
    "\n",
    "# Drop rows with missing target/user/time (time required for temporal split)\n",
    "drop_cols = [user_col, target_col] + ([time_col] if time_col else [])\n",
    "before = len(df)\n",
    "df = df.dropna(subset=drop_cols)\n",
    "print(f\"Dropped rows with null user/target/time: {before - len(df)}\")\n",
    "print(\"After dropna shape:\", df.shape)\n",
    "\n",
    "# Ensure target is int 0/1\n",
    "df[target_col] = df[target_col].astype(int)\n",
    "\n",
    "# ============================================\n",
    "# 3. Optional: keep only most recent N rows (temporal tail)\n",
    "# ============================================\n",
    "\n",
    "MAX_ROWS = 200_000  # you used this in output (last 200k rows)\n",
    "if time_col is not None:\n",
    "    # Sort by time ascending then take last MAX_ROWS\n",
    "    df = df.sort_values(time_col).reset_index(drop=True)\n",
    "    if len(df) > MAX_ROWS:\n",
    "        df = df.iloc[-MAX_ROWS:].copy()\n",
    "        print(f\"Temporal sampled to last {MAX_ROWS} rows.\")\n",
    "else:\n",
    "    # If no time col, random sample\n",
    "    if len(df) > MAX_ROWS:\n",
    "        df = df.sample(n=MAX_ROWS, random_state=42).copy()\n",
    "        print(f\"Random sampled to {MAX_ROWS} rows (no time col).\")\n",
    "\n",
    "print(\"Working dataframe shape:\", df.shape)\n",
    "\n",
    "# ============================================\n",
    "# 4. Remove leakage columns (STRICT)\n",
    "# ============================================\n",
    "\n",
    "leakage_cols = [\n",
    "    \"clk\",\n",
    "    \"nonclk\",\n",
    "    \"noclk\",\n",
    "    \"impressions\",\n",
    "    \"time_stamp_str\",\n",
    "    # NOTE: time_col is used only for split then removed from X\n",
    "]\n",
    "removed_cols = []\n",
    "for c in leakage_cols:\n",
    "    if c in df.columns and c != target_col:\n",
    "        df.drop(columns=[c], inplace=True)\n",
    "        removed_cols.append(c)\n",
    "\n",
    "# If you created \"impressions\" in EDA (clk+nonclk), remove it (already in list)\n",
    "if removed_cols:\n",
    "    print(\"Removed leakage columns:\", removed_cols)\n",
    "\n",
    "print(\"Columns after leakage removal (before split):\", len(df.columns))\n",
    "\n",
    "# ============================================\n",
    "# 5. Temporal split 80/20 (or stratified if no time_col)\n",
    "# ============================================\n",
    "\n",
    "if time_col is not None:\n",
    "    df = df.sort_values(time_col).reset_index(drop=True)\n",
    "    split_idx = int(len(df) * 0.8)\n",
    "    train_df = df.iloc[:split_idx].copy()\n",
    "    test_df  = df.iloc[split_idx:].copy()\n",
    "\n",
    "    cutoff = df.iloc[split_idx][time_col]\n",
    "    print(\"\\nTemporal split 80/20:\")\n",
    "    print(f\"cutoff {time_col} = {cutoff}\")\n",
    "else:\n",
    "    # fallback\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=0.2,\n",
    "        stratify=df[target_col],\n",
    "        random_state=42,\n",
    "    )\n",
    "    print(\"\\nStratified split 80/20 (no time column).\")\n",
    "\n",
    "print(\"train_df shape:\", train_df.shape)\n",
    "print(\"test_df shape :\", test_df.shape)\n",
    "\n",
    "print(\"\\nPositive rate:\")\n",
    "print(\"train pos_rate:\", train_df[target_col].mean())\n",
    "print(\"test  pos_rate:\", test_df[target_col].mean())\n",
    "\n",
    "# ============================================\n",
    "# 6. Build X/y (drop target + keep time only for later optional feature engineering)\n",
    "# ============================================\n",
    "\n",
    "def drop_id_like_cols(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop ID-like and key columns that can cause leakage or meaningless ordering.\n",
    "    This is stricter and safer than manual lists.\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r\"_id$\",          # any *_id\n",
    "        r\"^id$\",\n",
    "        r\"^user$\",\n",
    "        r\"^userid$\",\n",
    "        r\"^nick$\",\n",
    "        r\"^adgroup_id$\",\n",
    "        r\"^campaign_id$\",\n",
    "        r\"^customer$\",\n",
    "        r\"^pid$\",\n",
    "    ]\n",
    "    drop_cols = []\n",
    "    for c in X.columns:\n",
    "        c_low = c.lower().strip()\n",
    "        if any(re.search(p, c_low) for p in patterns):\n",
    "            drop_cols.append(c)\n",
    "    return X.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "def build_xy(sub_df: pd.DataFrame, target: str, time_col: str | None) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    y = sub_df[target].astype(int)\n",
    "    X = sub_df.drop(columns=[target], errors=\"ignore\")\n",
    "\n",
    "    # Drop time column from features (used only for split)\n",
    "    if time_col is not None and time_col in X.columns:\n",
    "        X = X.drop(columns=[time_col], errors=\"ignore\")\n",
    "\n",
    "    # Drop ID-like cols\n",
    "    X = drop_id_like_cols(X)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = build_xy(train_df, target_col, time_col)\n",
    "X_test,  y_test  = build_xy(test_df,  target_col, time_col)\n",
    "\n",
    "print(\"\\nX_train shape (raw):\", X_train.shape)\n",
    "print(\"X_test shape  (raw):\", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape, \"pos_rate:\", y_train.mean())\n",
    "print(\"y_test :\", y_test.shape,  \"pos_rate:\", y_test.mean())\n",
    "\n",
    "# ============================================\n",
    "# 7. Feature typing (IMPORTANT): treat \"numeric-looking categories\"\n",
    "# ============================================\n",
    "\n",
    "# Heuristic: treat these known code columns as categorical even if numeric dtype\n",
    "force_categorical = set([\n",
    "    \"cms_segid\", \"cms_group_id\", \"final_gender_code\", \"age_level\",\n",
    "    \"shopping_level\", \"occupation\", \"cate_id\", \"brand\", \"pid\",\n",
    "    \"pvalue_level\", \"new_user_class_level\"\n",
    "])\n",
    "\n",
    "# Make sure forced categorical exist\n",
    "force_categorical = {c for c in force_categorical if c in X_train.columns}\n",
    "\n",
    "# Identify categorical: objects + forced categorical\n",
    "categorical_cols = list(X_train.select_dtypes(include=[\"object\", \"category\"]).columns)\n",
    "for c in force_categorical:\n",
    "    if c not in categorical_cols:\n",
    "        categorical_cols.append(c)\n",
    "\n",
    "# Numeric = everything else\n",
    "numeric_cols = [c for c in X_train.columns if c not in categorical_cols]\n",
    "\n",
    "print(\"\\nNumeric columns:\", len(numeric_cols))\n",
    "print(\"Categorical columns:\", len(categorical_cols))\n",
    "\n",
    "# ============================================\n",
    "# 8. Add missing indicators + log1p for behavior counts\n",
    "# ============================================\n",
    "\n",
    "# Missing indicators for key features (based on your fusion enrichment rates)\n",
    "key_missing_cols = [c for c in [\"price\", \"age_level\", \"final_gender_code\", \"shopping_level\", \"pvalue_level\", \"cms_segid\"] if c in X_train.columns]\n",
    "for c in key_missing_cols:\n",
    "    X_train[f\"is_missing_{c}\"] = X_train[c].isna().astype(int)\n",
    "    X_test[f\"is_missing_{c}\"]  = X_test[c].isna().astype(int)\n",
    "\n",
    "# Update numeric/categorical lists (indicators are numeric)\n",
    "for c in [f\"is_missing_{x}\" for x in key_missing_cols]:\n",
    "    if c in X_train.columns and c not in numeric_cols:\n",
    "        numeric_cols.append(c)\n",
    "\n",
    "# Log1p for behavior counts (these should be numeric)\n",
    "for c in [\"pv\", \"cart\", \"fav\", \"buy\"]:\n",
    "    if c in X_train.columns:\n",
    "        X_train[c] = pd.to_numeric(X_train[c], errors=\"coerce\")\n",
    "        X_test[c]  = pd.to_numeric(X_test[c], errors=\"coerce\")\n",
    "        X_train[c] = np.log1p(X_train[c])\n",
    "        X_test[c]  = np.log1p(X_test[c])\n",
    "\n",
    "# ============================================\n",
    "# 9. Compute scale_pos_weight (neg/pos) on TRAIN ONLY\n",
    "# ============================================\n",
    "\n",
    "pos_count = int(y_train.sum())\n",
    "neg_count = int(len(y_train) - pos_count)\n",
    "scale_pos_weight = (neg_count / pos_count) if pos_count > 0 else 1.0\n",
    "print(f\"\\nComputed scale_pos_weight: {scale_pos_weight:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# 10. Categorical encoding (NO OHE)\n",
    "#     - High-cardinality: KFold target encoding (train only, leak-safe)\n",
    "#     - Low-cardinality: label encoding\n",
    "# ============================================\n",
    "\n",
    "HIGH_CARD_THRESHOLD = 10\n",
    "SMOOTHING = 20\n",
    "N_SPLITS_TE = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "global_mean = float(y_train.mean())\n",
    "\n",
    "X_train_enc = X_train.copy()\n",
    "X_test_enc  = X_test.copy()\n",
    "\n",
    "# Ensure categorical columns are treated as strings for mapping stability\n",
    "for c in categorical_cols:\n",
    "    X_train_enc[c] = X_train_enc[c].astype(\"string\")\n",
    "    X_test_enc[c]  = X_test_enc[c].astype(\"string\")\n",
    "\n",
    "high_card_cols = []\n",
    "low_card_cols  = []\n",
    "for c in categorical_cols:\n",
    "    nunq = X_train_enc[c].nunique(dropna=True)\n",
    "    if nunq > HIGH_CARD_THRESHOLD:\n",
    "        high_card_cols.append(c)\n",
    "    else:\n",
    "        low_card_cols.append(c)\n",
    "\n",
    "# --- Low-card: label encoding ---\n",
    "low_card_mappings = {}\n",
    "for c in low_card_cols:\n",
    "    cats = X_train_enc[c].dropna().unique().tolist()\n",
    "    # stable ordering\n",
    "    cats = sorted([str(x) for x in cats])\n",
    "    mapping = {cat: i for i, cat in enumerate(cats)}\n",
    "    low_card_mappings[c] = mapping\n",
    "\n",
    "    X_train_enc[c] = X_train_enc[c].map(mapping).fillna(-1).astype(int)\n",
    "    X_test_enc[c]  = X_test_enc[c].map(mapping).fillna(-1).astype(int)\n",
    "\n",
    "# --- High-card: KFold target encoding (leak-safe) ---\n",
    "high_card_mappings = {}\n",
    "\n",
    "if len(high_card_cols) > 0:\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS_TE, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    for c in high_card_cols:\n",
    "        # out-of-fold encoding for training\n",
    "        oof = pd.Series(index=X_train_enc.index, dtype=float)\n",
    "\n",
    "        for tr_idx, val_idx in skf.split(X_train_enc, y_train):\n",
    "            tr_x = X_train_enc.iloc[tr_idx]\n",
    "            tr_y = y_train.iloc[tr_idx]\n",
    "\n",
    "            stats = (\n",
    "                pd.DataFrame({c: tr_x[c], \"target\": tr_y.values})\n",
    "                .groupby(c)[\"target\"]\n",
    "                .agg([\"count\", \"mean\"])\n",
    "            )\n",
    "\n",
    "            # smoothing\n",
    "            enc_map = {}\n",
    "            for cat, row in stats.iterrows():\n",
    "                cnt = row[\"count\"]\n",
    "                mu  = row[\"mean\"]\n",
    "                enc_val = (cnt * mu + SMOOTHING * global_mean) / (cnt + SMOOTHING)\n",
    "                enc_map[str(cat)] = float(enc_val)\n",
    "\n",
    "            val_x = X_train_enc.iloc[val_idx]\n",
    "            oof.iloc[val_idx] = val_x[c].map(enc_map).astype(float)\n",
    "\n",
    "        # Fill unseen/null with global mean\n",
    "        X_train_enc[c] = oof.fillna(global_mean).astype(float)\n",
    "\n",
    "        # Fit final mapping on full training for test transform\n",
    "        stats_full = (\n",
    "            pd.DataFrame({c: X_train_enc.index.map(lambda i: X_train.loc[i, c] if c in X_train.columns else None),\n",
    "                          \"target\": y_train.values})\n",
    "        )\n",
    "        # Rebuild using original string categories from X_train (not oof)\n",
    "        stats_full = pd.DataFrame({c: X_train[c].astype(\"string\"), \"target\": y_train.values}).groupby(c)[\"target\"].agg([\"count\", \"mean\"])\n",
    "\n",
    "        final_map = {}\n",
    "        for cat, row in stats_full.iterrows():\n",
    "            cnt = row[\"count\"]\n",
    "            mu  = row[\"mean\"]\n",
    "            enc_val = (cnt * mu + SMOOTHING * global_mean) / (cnt + SMOOTHING)\n",
    "            final_map[str(cat)] = float(enc_val)\n",
    "\n",
    "        high_card_mappings[c] = {\n",
    "            \"mapping\": final_map,\n",
    "            \"global_mean\": global_mean,\n",
    "            \"smoothing\": SMOOTHING,\n",
    "            \"n_splits\": N_SPLITS_TE,\n",
    "        }\n",
    "\n",
    "        X_test_enc[c] = X_test[c].astype(\"string\").map(final_map).fillna(global_mean).astype(float)\n",
    "\n",
    "# After encoding, all categorical cols become numeric, so add them to numeric_cols if needed\n",
    "for c in categorical_cols:\n",
    "    if c in X_train_enc.columns and c not in numeric_cols:\n",
    "        numeric_cols.append(c)\n",
    "\n",
    "# ============================================\n",
    "# 11. Numeric imputation + scaling (fix dtype warnings)\n",
    "# ============================================\n",
    "\n",
    "# Ensure numeric columns exist in encoded DF (some might be missing due to drops)\n",
    "numeric_cols = [c for c in numeric_cols if c in X_train_enc.columns]\n",
    "\n",
    "# Coerce to numeric safely\n",
    "for c in numeric_cols:\n",
    "    X_train_enc[c] = pd.to_numeric(X_train_enc[c], errors=\"coerce\")\n",
    "    X_test_enc[c]  = pd.to_numeric(X_test_enc[c], errors=\"coerce\")\n",
    "\n",
    "numeric_imputer = SimpleImputer(strategy=\"median\")\n",
    "numeric_scaler  = StandardScaler()\n",
    "\n",
    "X_train_num = numeric_imputer.fit_transform(X_train_enc[numeric_cols].astype(np.float64))\n",
    "X_test_num  = numeric_imputer.transform(X_test_enc[numeric_cols].astype(np.float64))\n",
    "\n",
    "X_train_num = numeric_scaler.fit_transform(X_train_num)\n",
    "X_test_num  = numeric_scaler.transform(X_test_num)\n",
    "\n",
    "# Assign back as float (avoid FutureWarning)\n",
    "X_train_enc.loc[:, numeric_cols] = X_train_num.astype(np.float64)\n",
    "X_test_enc.loc[:, numeric_cols]  = X_test_num.astype(np.float64)\n",
    "\n",
    "# Final processed matrices (keep column order stable)\n",
    "X_train_processed = X_train_enc.copy()\n",
    "X_test_processed  = X_test_enc.copy()\n",
    "\n",
    "# Safety: ensure same columns\n",
    "missing_in_test = [c for c in X_train_processed.columns if c not in X_test_processed.columns]\n",
    "missing_in_train = [c for c in X_test_processed.columns if c not in X_train_processed.columns]\n",
    "if missing_in_test or missing_in_train:\n",
    "    raise ValueError(\n",
    "        f\"Train/Test columns mismatch.\\n\"\n",
    "        f\"Missing in test: {missing_in_test}\\n\"\n",
    "        f\"Missing in train: {missing_in_train}\\n\"\n",
    "    )\n",
    "\n",
    "print(\"\\nProcessed X_train shape:\", X_train_processed.shape)\n",
    "print(\"Processed X_test shape :\", X_test_processed.shape)\n",
    "\n",
    "# ============================================\n",
    "# 12. Feature importance (XGBoost) + optional top-K selection\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nFitting XGBoost for feature importance...\")\n",
    "importance_model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    ")\n",
    "importance_model.fit(X_train_processed, y_train)\n",
    "\n",
    "importances   = importance_model.feature_importances_\n",
    "feature_names = X_train_processed.columns.tolist()\n",
    "\n",
    "importance_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
    "importance_csv_path = os.path.join(processed_dir, \"feature_importances.csv\")\n",
    "importance_df.to_csv(importance_csv_path, index=False)\n",
    "print(f\"Saved feature importances to: {importance_csv_path}\")\n",
    "\n",
    "# Select top-K only if many features\n",
    "TOP_K = 50\n",
    "if len(feature_names) <= TOP_K:\n",
    "    top_features = feature_names[:]   # keep all\n",
    "else:\n",
    "    top_features = importance_df[\"feature\"].iloc[:TOP_K].tolist()\n",
    "\n",
    "print(f\"Selected Top-K features: {len(top_features)}\")\n",
    "\n",
    "X_train_final = X_train_processed[top_features]\n",
    "X_test_final  = X_test_processed[top_features]\n",
    "\n",
    "# ============================================\n",
    "# 13. Save processed outputs\n",
    "# ============================================\n",
    "\n",
    "X_train_path = os.path.join(processed_dir, \"X_train_processed.npz\")\n",
    "X_test_path  = os.path.join(processed_dir, \"X_test_processed.npz\")\n",
    "y_train_path = os.path.join(processed_dir, \"y_train.csv\")\n",
    "y_test_path  = os.path.join(processed_dir, \"y_test.csv\")\n",
    "top_features_path = os.path.join(processed_dir, \"top_features.csv\")\n",
    "scale_pos_weight_path = os.path.join(processed_dir, \"scale_pos_weight.txt\")\n",
    "preprocessor_path = os.path.join(processed_dir, \"preprocessor.joblib\")\n",
    "\n",
    "# Save sparse matrices\n",
    "sparse.save_npz(X_train_path, sparse.csr_matrix(X_train_final.values))\n",
    "sparse.save_npz(X_test_path,  sparse.csr_matrix(X_test_final.values))\n",
    "\n",
    "# Save labels\n",
    "pd.Series(y_train, name=target_col).to_csv(y_train_path, index=False)\n",
    "pd.Series(y_test, name=target_col).to_csv(y_test_path, index=False)\n",
    "\n",
    "# Save scale_pos_weight\n",
    "with open(scale_pos_weight_path, \"w\") as f:\n",
    "    f.write(str(scale_pos_weight))\n",
    "\n",
    "# Save top features list\n",
    "pd.DataFrame({\"feature\": top_features}).to_csv(top_features_path, index=False)\n",
    "\n",
    "# Save preprocessor (for inference reproducibility)\n",
    "preprocessor = {\n",
    "    \"target_col\": target_col,\n",
    "    \"user_col\": user_col,\n",
    "    \"time_col_used_for_split\": time_col,\n",
    "\n",
    "    \"numeric_cols\": numeric_cols,\n",
    "    \"categorical_cols_original\": categorical_cols,\n",
    "\n",
    "    \"missing_indicator_base_cols\": key_missing_cols,\n",
    "    \"log1p_cols\": [c for c in [\"pv\", \"cart\", \"fav\", \"buy\"] if c in X_train.columns],\n",
    "\n",
    "    \"numeric_imputer_strategy\": \"median\",\n",
    "    \"numeric_imputer_statistics\": numeric_imputer.statistics_.tolist(),\n",
    "\n",
    "    \"numeric_scaler_mean\": numeric_scaler.mean_.tolist(),\n",
    "    \"numeric_scaler_scale\": numeric_scaler.scale_.tolist(),\n",
    "\n",
    "    \"high_card_cols\": high_card_cols,\n",
    "    \"high_card_mappings\": high_card_mappings,  # contains mapping + smoothing + global mean\n",
    "\n",
    "    \"low_card_cols\": low_card_cols,\n",
    "    \"low_card_mappings\": low_card_mappings,\n",
    "\n",
    "    \"global_mean\": global_mean,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"top_features\": top_features,\n",
    "}\n",
    "\n",
    "dump(preprocessor, preprocessor_path)\n",
    "\n",
    "print(\"\\nSaved processed files:\")\n",
    "print(\" -\", X_train_path)\n",
    "print(\" -\", X_test_path)\n",
    "print(\" -\", y_train_path)\n",
    "print(\" -\", y_test_path)\n",
    "print(\" -\", importance_csv_path)\n",
    "print(\" -\", top_features_path)\n",
    "print(\" -\", scale_pos_weight_path)\n",
    "print(\" -\", preprocessor_path)\n",
    "\n",
    "print(\"\\n[Preprocessing completed successfully — جاهز لملف التدريب.]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b8847",
   "metadata": {},
   "source": [
    "### Modifications Summary\n",
    "This notebook has been updated to remove SMOTE-based oversampling and One-Hot encoding.\n",
    "Instead, class imbalance is handled by computing `scale_pos_weight` (negative/positive ratio) on the training data.\n",
    "Categorical features are encoded using **Target Encoding** for high-cardinality columns and **Label Encoding** for low-cardinality columns.\n",
    "Numeric features are imputed with the median and standardised.\n",
    "Feature importances are computed with an XGBoost model using the calculated `scale_pos_weight` and the top 50 features are selected.\n",
    "The processed datasets, label files, selected features, feature importances, scale position weight, and a preprocessor mapping are saved to the `data/processed` directory for subsequent notebooks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
