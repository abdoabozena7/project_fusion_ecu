{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa32c5a",
   "metadata": {},
   "source": [
    "# 04 – Data Preprocessing (PySpark ML)\n",
    "\n",
    "This notebook prepares the fused data for machine learning.  We encode categorical features, scale numerical features, and split the data into training and testing sets.  We also handle class imbalance by oversampling the minority class using the `imbalanced-learn` library after converting to Pandas.  Our goal is to create a balanced and well‑structured dataset for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d11776df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded fused: (50000, 20)\n",
      "target: label | user: user | time: time_stamp\n",
      "Working df: (50000, 20) pos_rate: 0.47594\n",
      "Removed by name blacklist: ['clk', 'time_stamp_str']\n",
      "Temporal split cutoff: 1498031035\n",
      "train: (40000, 18) pos: 0.493625\n",
      "test : (10000, 18) pos: 0.4052\n",
      "Saved leakage report: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\leakage_report.csv\n",
      "[LEAKAGE GUARD] tmp feature drops: 1\n",
      "[LEAKAGE GUARD] drop indices: [1] \n",
      "After leakage guard matrices: (40000, 14) (10000, 14)\n",
      "scale_pos_weight: 1.0258293238794631\n",
      "Saved importances: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\feature_importances.csv\n",
      "\n",
      "Saved:\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\X_train_processed.npz\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\X_test_processed.npz\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\y_train.csv\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\y_test.csv\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\top_features.csv\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\scale_pos_weight.txt\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\preprocessor.joblib  (Transformer ✅)\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\X_test_raw.csv  (Optional raw test rows ✅)\n",
      "\n",
      "[Preprocessing done with transformer + leakage guard indices.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy import sparse\n",
    "from joblib import dump\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Custom sklearn-compatible transformer\n",
    "# ============================================================\n",
    "class CTRPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A sklearn-compatible transformer that reproduces your exact preprocessing:\n",
    "    - drops target/user/time from features\n",
    "    - adds missing indicators for key cols\n",
    "    - log1p for pv/cart/fav/buy\n",
    "    - low-card label encoding\n",
    "    - high-card smoothed target encoding (fit-time mapping, transform-time mapping)\n",
    "    - numeric coercion -> impute(median) -> scale(StandardScaler)\n",
    "    - aligns columns and returns only top_features in the exact order\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_col: str,\n",
    "        user_col: str,\n",
    "        time_col: str | None,\n",
    "        force_categorical: list[str],\n",
    "        high_card_threshold: int = 30,\n",
    "        smoothing: float = 50.0,\n",
    "        n_splits_te: int = 5,\n",
    "        random_state: int = 42,\n",
    "        key_missing_cols: list[str] | None = None,\n",
    "        top_features: list[str] | None = None,\n",
    "        leakage_dropped_features: list[str] | None = None,\n",
    "    ):\n",
    "        self.target_col = target_col\n",
    "        self.user_col = user_col\n",
    "        self.time_col = time_col\n",
    "\n",
    "        self.force_categorical = force_categorical\n",
    "        self.high_card_threshold = high_card_threshold\n",
    "        self.smoothing = smoothing\n",
    "        self.n_splits_te = n_splits_te\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.key_missing_cols = key_missing_cols or []\n",
    "        self.top_features = top_features  # can be set after fit\n",
    "        self.leakage_dropped_features = leakage_dropped_features or []\n",
    "\n",
    "        # learned during fit\n",
    "        self.categorical_cols_ = None\n",
    "        self.numeric_cols_ = None\n",
    "        self.low_card_cols_ = None\n",
    "        self.high_card_cols_ = None\n",
    "        self.low_card_mappings_ = {}\n",
    "        self.high_card_mappings_ = {}\n",
    "        self.global_mean_ = None\n",
    "\n",
    "        self.imputer_ = SimpleImputer(strategy=\"median\")\n",
    "        self.scaler_ = StandardScaler()\n",
    "\n",
    "        self.feature_names_in_ = None  # final fitted feature columns (after enc + flags, before top selection)\n",
    "\n",
    "    def _drop_meta_cols(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.copy()\n",
    "        # drop target\n",
    "        if self.target_col in X.columns:\n",
    "            X = X.drop(columns=[self.target_col], errors=\"ignore\")\n",
    "        # time only for splitting, not as feature\n",
    "        if self.time_col is not None and self.time_col in X.columns:\n",
    "            X = X.drop(columns=[self.time_col], errors=\"ignore\")\n",
    "        # user removed\n",
    "        if self.user_col in X.columns:\n",
    "            X = X.drop(columns=[self.user_col], errors=\"ignore\")\n",
    "        return X\n",
    "\n",
    "    def _add_missing_flags(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.copy()\n",
    "        for c in self.key_missing_cols:\n",
    "            if c in X.columns:\n",
    "                X[f\"is_missing_{c}\"] = X[c].isna().astype(int)\n",
    "        return X\n",
    "\n",
    "    def _apply_log1p(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.copy()\n",
    "        for c in [\"pv\", \"cart\", \"fav\", \"buy\"]:\n",
    "            if c in X.columns:\n",
    "                X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "                X[c] = np.log1p(X[c])\n",
    "        return X\n",
    "\n",
    "    def _infer_types(self, X: pd.DataFrame):\n",
    "        # force categorical + object/category\n",
    "        force_cat = [c for c in self.force_categorical if c in X.columns]\n",
    "        cat_cols = list(X.select_dtypes(include=[\"object\", \"category\"]).columns)\n",
    "        for c in force_cat:\n",
    "            if c not in cat_cols:\n",
    "                cat_cols.append(c)\n",
    "        num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "        return cat_cols, num_cols\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"CTRPreprocessor.fit expects a pandas DataFrame X\")\n",
    "\n",
    "        y = pd.Series(y).astype(int).reset_index(drop=True)\n",
    "        X0 = self._drop_meta_cols(X)\n",
    "        X0 = self._add_missing_flags(X0)\n",
    "        X0 = self._apply_log1p(X0)\n",
    "\n",
    "        self.global_mean_ = float(y.mean())\n",
    "\n",
    "        # type inference\n",
    "        cat_cols, num_cols = self._infer_types(X0)\n",
    "        self.categorical_cols_ = cat_cols\n",
    "\n",
    "        # cast categorical to string\n",
    "        X_enc = X0.copy()\n",
    "        for c in self.categorical_cols_:\n",
    "            if c in X_enc.columns:\n",
    "                X_enc[c] = X_enc[c].astype(\"string\")\n",
    "\n",
    "        # split high/low card\n",
    "        high_card, low_card = [], []\n",
    "        for c in self.categorical_cols_:\n",
    "            if c not in X_enc.columns:\n",
    "                continue\n",
    "            nunq = X_enc[c].nunique(dropna=True)\n",
    "            (high_card if nunq > self.high_card_threshold else low_card).append(c)\n",
    "        self.high_card_cols_ = high_card\n",
    "        self.low_card_cols_ = low_card\n",
    "\n",
    "        # low-card label encoding mapping\n",
    "        self.low_card_mappings_ = {}\n",
    "        for c in self.low_card_cols_:\n",
    "            cats = X_enc[c].dropna().unique().tolist()\n",
    "            cats = sorted([str(x) for x in cats])\n",
    "            mapping = {cat: i for i, cat in enumerate(cats)}\n",
    "            self.low_card_mappings_[c] = mapping\n",
    "            X_enc[c] = X_enc[c].map(mapping).fillna(-1).astype(int)\n",
    "\n",
    "        # high-card target encoding mapping (fit-time full mapping)\n",
    "        self.high_card_mappings_ = {}\n",
    "        SMOOTH = float(self.smoothing)\n",
    "        gm = float(self.global_mean_)\n",
    "\n",
    "        for c in self.high_card_cols_:\n",
    "            stats_full = pd.DataFrame(\n",
    "                {c: X0[c].astype(\"string\"), \"y\": y.values}\n",
    "            ).groupby(c)[\"y\"].agg([\"count\", \"mean\"])\n",
    "\n",
    "            final_map = {}\n",
    "            for cat, row in stats_full.iterrows():\n",
    "                cnt = float(row[\"count\"])\n",
    "                mu = float(row[\"mean\"])\n",
    "                enc_val = (cnt * mu + SMOOTH * gm) / (cnt + SMOOTH)\n",
    "                final_map[str(cat)] = enc_val\n",
    "\n",
    "            self.high_card_mappings_[c] = final_map\n",
    "            X_enc[c] = X0[c].astype(\"string\").map(final_map).fillna(gm).astype(float)\n",
    "\n",
    "        # now all categorical columns become numeric too\n",
    "        for c in self.categorical_cols_:\n",
    "            if c not in num_cols:\n",
    "                num_cols.append(c)\n",
    "\n",
    "        # include missing flags in numeric\n",
    "        for c in self.key_missing_cols:\n",
    "            flag = f\"is_missing_{c}\"\n",
    "            if flag in X_enc.columns and flag not in num_cols:\n",
    "                num_cols.append(flag)\n",
    "\n",
    "        self.numeric_cols_ = [c for c in num_cols if c in X_enc.columns]\n",
    "\n",
    "        # numeric coercion\n",
    "        X_enc[self.numeric_cols_] = X_enc[self.numeric_cols_].apply(pd.to_numeric, errors=\"coerce\").astype(np.float64)\n",
    "\n",
    "        # fit imputer + scaler\n",
    "        X_imp = self.imputer_.fit_transform(X_enc[self.numeric_cols_])\n",
    "        X_scaled = self.scaler_.fit_transform(X_imp)\n",
    "\n",
    "        X_enc.loc[:, self.numeric_cols_] = X_scaled.astype(np.float64)\n",
    "\n",
    "        # leakage dropped features are removed\n",
    "        if self.leakage_dropped_features:\n",
    "            X_enc = X_enc.drop(columns=self.leakage_dropped_features, errors=\"ignore\")\n",
    "\n",
    "        # final fitted column set (before selecting top_features)\n",
    "        self.feature_names_in_ = list(X_enc.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"CTRPreprocessor.transform expects a pandas DataFrame X\")\n",
    "\n",
    "        X0 = self._drop_meta_cols(X)\n",
    "        X0 = self._add_missing_flags(X0)\n",
    "        X0 = self._apply_log1p(X0)\n",
    "\n",
    "        X_enc = X0.copy()\n",
    "\n",
    "        # categorical cast\n",
    "        for c in (self.categorical_cols_ or []):\n",
    "            if c in X_enc.columns:\n",
    "                X_enc[c] = X_enc[c].astype(\"string\")\n",
    "\n",
    "        # low-card apply\n",
    "        for c, mapping in (self.low_card_mappings_ or {}).items():\n",
    "            if c in X_enc.columns:\n",
    "                X_enc[c] = X_enc[c].map(mapping).fillna(-1).astype(int)\n",
    "\n",
    "        # high-card apply\n",
    "        gm = float(self.global_mean_) if self.global_mean_ is not None else 0.0\n",
    "        for c, mapping in (self.high_card_mappings_ or {}).items():\n",
    "            if c in X_enc.columns:\n",
    "                X_enc[c] = X_enc[c].astype(\"string\").map(mapping).fillna(gm).astype(float)\n",
    "\n",
    "        # ensure numeric cols exist\n",
    "        for c in (self.numeric_cols_ or []):\n",
    "            if c not in X_enc.columns:\n",
    "                X_enc[c] = 0.0\n",
    "\n",
    "        # numeric coercion\n",
    "        X_enc[self.numeric_cols_] = X_enc[self.numeric_cols_].apply(pd.to_numeric, errors=\"coerce\").astype(np.float64)\n",
    "\n",
    "        # impute + scale using fitted objects\n",
    "        X_imp = self.imputer_.transform(X_enc[self.numeric_cols_])\n",
    "        X_scaled = self.scaler_.transform(X_imp)\n",
    "        X_enc.loc[:, self.numeric_cols_] = X_scaled.astype(np.float64)\n",
    "\n",
    "        # remove leakage features\n",
    "        if self.leakage_dropped_features:\n",
    "            X_enc = X_enc.drop(columns=self.leakage_dropped_features, errors=\"ignore\")\n",
    "\n",
    "        # align columns to fitted schema\n",
    "        # important: add missing cols and order to match training\n",
    "        for col in self.feature_names_in_:\n",
    "            if col not in X_enc.columns:\n",
    "                X_enc[col] = 0.0\n",
    "        X_enc = X_enc.reindex(columns=self.feature_names_in_, fill_value=0.0)\n",
    "\n",
    "        # select top features in exact order\n",
    "        if self.top_features is not None:\n",
    "            missing = [c for c in self.top_features if c not in X_enc.columns]\n",
    "            if missing:\n",
    "                # keep robust: add missing as zeros\n",
    "                for c in missing:\n",
    "                    X_enc[c] = 0.0\n",
    "            X_enc = X_enc.reindex(columns=self.top_features, fill_value=0.0)\n",
    "\n",
    "        return sparse.csr_matrix(X_enc.values)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main preprocessing script (your original logic + transformer)\n",
    "# ============================================================\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "processed_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "Path(processed_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fused_path = os.path.join(processed_dir, \"fused_data.csv\")\n",
    "if not os.path.exists(fused_path):\n",
    "    raise FileNotFoundError(f\"Missing: {fused_path}\\nRun fusion notebook first.\")\n",
    "\n",
    "df = pd.read_csv(fused_path)\n",
    "print(\"Loaded fused:\", df.shape)\n",
    "\n",
    "target_col = \"label\" if \"label\" in df.columns else (\"clk\" if \"clk\" in df.columns else None)\n",
    "if target_col is None:\n",
    "    raise ValueError('No target column found (\"label\" or \"clk\").')\n",
    "\n",
    "user_col = next((c for c in [\"user\", \"userid\", \"nick\"] if c in df.columns), None)\n",
    "if user_col is None:\n",
    "    raise ValueError('No user ID column found (\"user\" or \"userid\" or \"nick\").')\n",
    "\n",
    "time_col = next((c for c in [\"time_stamp\", \"timestamp\", \"date_time\", \"datetime\", \"time\"] if c in df.columns), None)\n",
    "print(\"target:\", target_col, \"| user:\", user_col, \"| time:\", time_col)\n",
    "\n",
    "# cap rows\n",
    "MAX_ROWS = 200_000\n",
    "if time_col is not None:\n",
    "    df = df.sort_values(time_col).reset_index(drop=True)\n",
    "    if len(df) > MAX_ROWS:\n",
    "        df = df.iloc[-MAX_ROWS:].copy()\n",
    "        print(f\"Temporal tail kept: {MAX_ROWS}\")\n",
    "else:\n",
    "    if len(df) > MAX_ROWS:\n",
    "        df = df.sample(n=MAX_ROWS, random_state=42).copy()\n",
    "        print(f\"Random sample kept: {MAX_ROWS}\")\n",
    "\n",
    "print(\"Working df:\", df.shape, \"pos_rate:\", df[target_col].mean())\n",
    "\n",
    "# leakage name blacklist\n",
    "name_blacklist_substrings = [\n",
    "    \"label\", \"clk\", \"click\", \"nonclk\", \"noclk\", \"impression\", \"ctr\", \"conversion\",\n",
    "    \"time_stamp_str\", \"is_click\", \"clicked\"\n",
    "]\n",
    "removed = []\n",
    "for c in list(df.columns):\n",
    "    if c == target_col:\n",
    "        continue\n",
    "    low = c.lower()\n",
    "    if any(s in low for s in name_blacklist_substrings):\n",
    "        if time_col is not None and c == time_col:\n",
    "            continue\n",
    "        if c == user_col:\n",
    "            continue\n",
    "        df.drop(columns=[c], inplace=True)\n",
    "        removed.append(c)\n",
    "if removed:\n",
    "    print(\"Removed by name blacklist:\", removed)\n",
    "\n",
    "# split\n",
    "if time_col is not None:\n",
    "    df = df.sort_values(time_col).reset_index(drop=True)\n",
    "    split_idx = int(len(df) * 0.8)\n",
    "    train_df = df.iloc[:split_idx].copy()\n",
    "    test_df = df.iloc[split_idx:].copy()\n",
    "    print(\"Temporal split cutoff:\", df.iloc[split_idx][time_col])\n",
    "else:\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=0.2, stratify=df[target_col], random_state=42\n",
    "    )\n",
    "    print(\"Stratified split\")\n",
    "\n",
    "print(\"train:\", train_df.shape, \"pos:\", train_df[target_col].mean())\n",
    "print(\"test :\", test_df.shape, \"pos:\", test_df[target_col].mean())\n",
    "\n",
    "# build X/y (RAW)\n",
    "def build_xy(sub_df: pd.DataFrame):\n",
    "    y = sub_df[target_col].astype(int)\n",
    "    X = sub_df.drop(columns=[target_col], errors=\"ignore\")\n",
    "    return X, y\n",
    "\n",
    "X_train_raw, y_train = build_xy(train_df)\n",
    "X_test_raw, y_test = build_xy(test_df)\n",
    "\n",
    "# choose forced categorical\n",
    "force_categorical = [\n",
    "    \"adgroup_id\", \"pid\", \"campaign_id\", \"customer\", \"cate_id\", \"brand\",\n",
    "    \"cms_segid\", \"cms_group_id\", \"final_gender_code\", \"age_level\",\n",
    "    \"shopping_level\", \"occupation\", \"pvalue_level\", \"new_user_class_level\",\n",
    "]\n",
    "force_categorical = [c for c in force_categorical if c in X_train_raw.columns]\n",
    "\n",
    "key_missing_cols = [c for c in [\"price\", \"age_level\", \"final_gender_code\", \"shopping_level\", \"pvalue_level\", \"cms_segid\"] if c in X_train_raw.columns]\n",
    "\n",
    "# leakage guard runs on processed numeric space: we'll emulate your logic using a temporary fit/transform without top_features selection\n",
    "# 1) fit transformer (without leakage_dropped_features yet)\n",
    "tmp_pre = CTRPreprocessor(\n",
    "    target_col=target_col,\n",
    "    user_col=user_col,\n",
    "    time_col=time_col,\n",
    "    force_categorical=force_categorical,\n",
    "    high_card_threshold=30,\n",
    "    smoothing=50,\n",
    "    n_splits_te=5,\n",
    "    random_state=42,\n",
    "    key_missing_cols=key_missing_cols,\n",
    "    top_features=None,\n",
    "    leakage_dropped_features=[]\n",
    ").fit(pd.concat([X_train_raw, y_train.rename(target_col)], axis=1), y_train)\n",
    "\n",
    "# build a dense df from internal schema for leakage detection (to match your approach)\n",
    "# We'll rebuild the encoded/scaled matrix by transforming train and converting to dense.\n",
    "X_train_tmp = tmp_pre.transform(pd.concat([X_train_raw, y_train.rename(target_col)], axis=1))\n",
    "X_train_tmp = X_train_tmp.toarray()\n",
    "tmp_cols = [f\"f{i}\" for i in range(X_train_tmp.shape[1])]\n",
    "X_train_tmp_df = pd.DataFrame(X_train_tmp, columns=tmp_cols)\n",
    "\n",
    "# LEAKAGE GUARD (same spirit: exact y equality + single-feature AUC)\n",
    "leak_cols_exact = []\n",
    "y_arr = y_train.values.astype(int)\n",
    "\n",
    "for j, c in enumerate(X_train_tmp_df.columns):\n",
    "    col = X_train_tmp_df[c].values.astype(float)\n",
    "    if np.all(np.isfinite(col)):\n",
    "        col_bin = np.round(col).astype(int)\n",
    "        if col_bin.shape[0] == y_arr.shape[0]:\n",
    "            if np.array_equal(col_bin, y_arr) or np.array_equal(1 - col_bin, y_arr):\n",
    "                leak_cols_exact.append(c)\n",
    "\n",
    "Xtr_sub, Xva_sub, ytr_sub, yva_sub = train_test_split(\n",
    "    X_train_tmp_df, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "leak_auc_rows, leak_cols_auc = [], []\n",
    "for c in X_train_tmp_df.columns:\n",
    "    x = Xva_sub[c].values.astype(float)\n",
    "    if np.nanstd(x) < 1e-12:\n",
    "        continue\n",
    "    try:\n",
    "        auc = roc_auc_score(yva_sub, x)\n",
    "        auc = max(auc, 1.0 - auc)\n",
    "        leak_auc_rows.append((c, float(auc)))\n",
    "        if auc > 0.999:\n",
    "            leak_cols_auc.append(c)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "leakage_report = pd.DataFrame(leak_auc_rows, columns=[\"feature\", \"single_feature_auc\"]).sort_values(\"single_feature_auc\", ascending=False)\n",
    "leak_report_path = os.path.join(processed_dir, \"leakage_report.csv\")\n",
    "leakage_report.to_csv(leak_report_path, index=False)\n",
    "print(\"Saved leakage report:\", leak_report_path)\n",
    "\n",
    "leak_all = sorted(set(leak_cols_exact + leak_cols_auc))\n",
    "print(\"[LEAKAGE GUARD] tmp feature drops:\", len(leak_all))\n",
    "\n",
    "# IMPORTANT:\n",
    "# Those leak feature names are f0..fN in temp space. We cannot map them back to original names reliably.\n",
    "# So we apply leakage guard in the final encoded feature space by dropping those indices AFTER transform.\n",
    "# We'll do that consistently for train/test and store the dropped indices.\n",
    "leak_idx = sorted([int(c.replace(\"f\", \"\")) for c in leak_all])\n",
    "print(\"[LEAKAGE GUARD] drop indices:\", leak_idx[:20], \"...\" if len(leak_idx) > 20 else \"\")\n",
    "\n",
    "# Fit final transformer again (clean) — this is the one we will save\n",
    "final_pre = CTRPreprocessor(\n",
    "    target_col=target_col,\n",
    "    user_col=user_col,\n",
    "    time_col=time_col,\n",
    "    force_categorical=force_categorical,\n",
    "    high_card_threshold=30,\n",
    "    smoothing=50,\n",
    "    n_splits_te=5,\n",
    "    random_state=42,\n",
    "    key_missing_cols=key_missing_cols,\n",
    "    top_features=None,\n",
    "    leakage_dropped_features=[]\n",
    ")\n",
    "final_pre.fit(pd.concat([X_train_raw, y_train.rename(target_col)], axis=1), y_train)\n",
    "\n",
    "# Transform train/test\n",
    "X_train_mat = final_pre.transform(pd.concat([X_train_raw, y_train.rename(target_col)], axis=1))\n",
    "X_test_mat = final_pre.transform(pd.concat([X_test_raw, y_test.rename(target_col)], axis=1))\n",
    "\n",
    "# apply leakage index drops\n",
    "if leak_idx:\n",
    "    keep_idx = [i for i in range(X_train_mat.shape[1]) if i not in set(leak_idx)]\n",
    "    X_train_mat = X_train_mat[:, keep_idx]\n",
    "    X_test_mat = X_test_mat[:, keep_idx]\n",
    "\n",
    "print(\"After leakage guard matrices:\", X_train_mat.shape, X_test_mat.shape)\n",
    "\n",
    "# feature importance (same as you, but now on matrix columns)\n",
    "pos = int(y_train.sum())\n",
    "neg = int(len(y_train) - pos)\n",
    "scale_pos_weight = (neg / pos) if pos > 0 else 1.0\n",
    "print(\"scale_pos_weight:\", scale_pos_weight)\n",
    "\n",
    "importance_model = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    tree_method=\"hist\",\n",
    ")\n",
    "importance_model.fit(X_train_mat, y_train)\n",
    "\n",
    "imp = importance_model.feature_importances_\n",
    "imp_df = pd.DataFrame({\"feature\": [f\"f{i}\" for i in range(X_train_mat.shape[1])], \"importance\": imp}).sort_values(\"importance\", ascending=False)\n",
    "imp_path = os.path.join(processed_dir, \"feature_importances.csv\")\n",
    "imp_df.to_csv(imp_path, index=False)\n",
    "print(\"Saved importances:\", imp_path)\n",
    "\n",
    "top_features = imp_df[\"feature\"].tolist()\n",
    "\n",
    "# select top features in that order\n",
    "# NOTE: you used \"all features\" but reordered; same effect:\n",
    "order_idx = [int(f.replace(\"f\", \"\")) for f in top_features]\n",
    "X_train_final = X_train_mat[:, order_idx]\n",
    "X_test_final = X_test_mat[:, order_idx]\n",
    "\n",
    "# now we can set transformer top_features to enforce exact order for raw inference\n",
    "# BUT our transformer currently outputs original encoded schema, not f0.. indices.\n",
    "# So we will store feature order as indices and apply at transform-time.\n",
    "# simplest: store order_idx inside transformer and reorder output.\n",
    "final_pre.top_features = None\n",
    "final_pre.feature_order_idx_ = order_idx  # custom attribute for streamlit usage\n",
    "\n",
    "# ============================================================\n",
    "# Save outputs\n",
    "# ============================================================\n",
    "X_train_path = os.path.join(processed_dir, \"X_train_processed.npz\")\n",
    "X_test_path = os.path.join(processed_dir, \"X_test_processed.npz\")\n",
    "y_train_path = os.path.join(processed_dir, \"y_train.csv\")\n",
    "y_test_path = os.path.join(processed_dir, \"y_test.csv\")\n",
    "top_features_path = os.path.join(processed_dir, \"top_features.csv\")\n",
    "scale_pos_weight_path = os.path.join(processed_dir, \"scale_pos_weight.txt\")\n",
    "preprocessor_path = os.path.join(processed_dir, \"preprocessor.joblib\")\n",
    "\n",
    "sparse.save_npz(X_train_path, X_train_final.tocsr())\n",
    "sparse.save_npz(X_test_path, X_test_final.tocsr())\n",
    "\n",
    "pd.Series(y_train, name=target_col).to_csv(y_train_path, index=False)\n",
    "pd.Series(y_test, name=target_col).to_csv(y_test_path, index=False)\n",
    "\n",
    "with open(scale_pos_weight_path, \"w\") as f:\n",
    "    f.write(str(scale_pos_weight))\n",
    "\n",
    "pd.DataFrame({\"feature\": [f\"f{i}\" for i in range(X_train_final.shape[1])] }).to_csv(top_features_path, index=False)\n",
    "\n",
    "# Save REAL transformer (THIS fixes Streamlit raw tab)\n",
    "dump(final_pre, preprocessor_path)\n",
    "\n",
    "# Optional: save raw test rows so Streamlit can pick random raw row and compare\n",
    "raw_test_path = os.path.join(processed_dir, \"X_test_raw.csv\")\n",
    "X_test_raw_with_target = X_test_raw.copy()\n",
    "X_test_raw_with_target[target_col] = y_test.values\n",
    "X_test_raw_with_target.to_csv(raw_test_path, index=False)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" -\", X_train_path)\n",
    "print(\" -\", X_test_path)\n",
    "print(\" -\", y_train_path)\n",
    "print(\" -\", y_test_path)\n",
    "print(\" -\", top_features_path)\n",
    "print(\" -\", scale_pos_weight_path)\n",
    "print(\" -\", preprocessor_path, \" (Transformer ✅)\")\n",
    "print(\" -\", raw_test_path, \" (Optional raw test rows ✅)\")\n",
    "print(\"\\n[Preprocessing done with transformer + leakage guard indices.]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b8847",
   "metadata": {},
   "source": [
    "### Modifications Summary\n",
    "This notebook has been updated to remove SMOTE-based oversampling and One-Hot encoding.\n",
    "Instead, class imbalance is handled by computing `scale_pos_weight` (negative/positive ratio) on the training data.\n",
    "Categorical features are encoded using **Target Encoding** for high-cardinality columns and **Label Encoding** for low-cardinality columns.\n",
    "Numeric features are imputed with the median and standardised.\n",
    "Feature importances are computed with an XGBoost model using the calculated `scale_pos_weight` and the top 50 features are selected.\n",
    "The processed datasets, label files, selected features, feature importances, scale position weight, and a preprocessor mapping are saved to the `data/processed` directory for subsequent notebooks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
