{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa32c5a",
   "metadata": {},
   "source": [
    "# 04 – Data Preprocessing (PySpark ML)\n",
    "\n",
    "This notebook prepares the fused data for machine learning.  We encode categorical features, scale numerical features, and split the data into training and testing sets.  We also handle class imbalance by oversampling the minority class using the `imbalanced-learn` library after converting to Pandas.  Our goal is to create a balanced and well‑structured dataset for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11776df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: d:\\projects\\Ai\\project_fusion_ecu\n",
      "Processed dir: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\n",
      "Full fused data shape: (1000000, 28)\n",
      "Sampled down to 200000 rows.\n",
      "Working dataframe shape: (200000, 28)\n",
      "Target column: label\n",
      "User ID column: user\n",
      "Shape after dropping rows with null user/target: (200000, 28)\n",
      "REMOVED LEAKAGE COLUMN: clk\n",
      "REMOVED LEAKAGE COLUMN: nonclk\n",
      "REMOVED LEAKAGE COLUMN: time_stamp\n",
      "REMOVED LEAKAGE COLUMN: time_stamp_str\n",
      "Columns after leakage removal:\n",
      "['user', 'adgroup_id', 'pid', 'userid', 'cms_segid', 'cms_group_id', 'final_gender_code', 'age_level', 'pvalue_level', 'shopping_level', 'occupation', 'new_user_class_level ', 'adgroup_id.1', ' cate_id', ' campaign_id', ' customer', ' brand ', ' price', 'user.1', 'buy', 'cart', 'fav', 'pv', 'label']\n",
      "Number of unique users: 130800\n",
      "Train_df shape: (159964, 24)\n",
      "Test_df shape: (40036, 24)\n",
      "Dropping ID-like columns: ['user', 'adgroup_id', 'pid', 'userid', 'new_user_class_level ', 'adgroup_id.1', ' campaign_id', ' customer', 'user.1']\n",
      "Dropping ID-like columns: ['user', 'adgroup_id', 'pid', 'userid', 'new_user_class_level ', 'adgroup_id.1', ' campaign_id', ' customer', 'user.1']\n",
      "X_train shape (raw): (159964, 14)\n",
      "X_test shape  (raw): (40036, 14)\n",
      "y_train shape: (159964,)\n",
      "y_test shape: (40036,)\n",
      "\n",
      "y_train distribution:\n",
      "label\n",
      "0    0.950326\n",
      "1    0.049674\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "y_test distribution:\n",
      "label\n",
      "0    0.950944\n",
      "1    0.049056\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Numeric columns: 13\n",
      "Categorical columns: 1\n",
      "\n",
      "Fitting preprocessing pipeline on X_train...\n",
      "X_train_transformed: (159964, 14032)\n",
      "X_test_transformed: (40036, 14032)\n",
      "NaN in X_train_transformed: False\n",
      "NaN in X_test_transformed: False\n",
      "\n",
      "Applying SMOTE to handle class imbalance...\n",
      "X_train_resampled shape: (304036, 14032)\n",
      "y_train_resampled distribution:\n",
      "label\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Saved:\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\X_train_resampled.npz\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\X_test_transformed.npz\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\y_train_resampled.csv\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\y_test.csv\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\preprocessor.joblib\n",
      "\n",
      "[Preprocessing completed successfully – data is ready for model training.]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 0. Imports and configuration\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy import sparse\n",
    "from joblib import dump\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================\n",
    "# 1. Load fused data\n",
    "# ============================================\n",
    "\n",
    "# This notebook is expected in: D:/projects/Ai/project_fusion_ecu/notebooks\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "processed_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"Processed dir:\", processed_dir)\n",
    "\n",
    "fused_path = os.path.join(processed_dir, \"fused_data.csv\")\n",
    "if not os.path.exists(fused_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Fused data file not found at: {fused_path}\\n\"\n",
    "        \"Run 03_eda_fusion.ipynb first to generate fused_data.csv.\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(fused_path)\n",
    "print(\"Full fused data shape:\", df.shape)\n",
    "\n",
    "# ============================================\n",
    "# 2. Optional sampling for faster processing\n",
    "# ============================================\n",
    "\n",
    "MAX_ROWS = 200_000  # adjust if needed\n",
    "\n",
    "if len(df) > MAX_ROWS:\n",
    "    df = df.sample(n=MAX_ROWS, random_state=42)\n",
    "    print(f\"Sampled down to {MAX_ROWS} rows.\")\n",
    "else:\n",
    "    print(\"Using all rows (no sampling).\")\n",
    "\n",
    "print(\"Working dataframe shape:\", df.shape)\n",
    "\n",
    "# ============================================\n",
    "# 3. Choose target and user ID columns\n",
    "# ============================================\n",
    "\n",
    "# Target: prefer 'label' if exists, otherwise 'clk'\n",
    "if \"label\" in df.columns:\n",
    "    target_col = \"label\"\n",
    "elif \"clk\" in df.columns:\n",
    "    target_col = \"clk\"\n",
    "else:\n",
    "    raise ValueError('No target column found. Expected \"label\" or \"clk\".')\n",
    "\n",
    "print(\"Target column:\", target_col)\n",
    "\n",
    "# User ID column for user-level split\n",
    "if \"user\" in df.columns:\n",
    "    user_col = \"user\"\n",
    "elif \"userid\" in df.columns:\n",
    "    user_col = \"userid\"\n",
    "elif \"nick\" in df.columns:\n",
    "    user_col = \"nick\"\n",
    "else:\n",
    "    raise ValueError('No user ID column found. Expected \"user\", \"userid\" or \"nick\".')\n",
    "\n",
    "print(\"User ID column:\", user_col)\n",
    "\n",
    "# Drop rows with missing user/target\n",
    "df = df.dropna(subset=[user_col, target_col])\n",
    "print(\"Shape after dropping rows with null user/target:\", df.shape)\n",
    "\n",
    "# ============================================\n",
    "# 4. Remove leakage columns (VERY IMPORTANT)\n",
    "# ============================================\n",
    "\n",
    "# Columns that directly leak the target or timestamp:\n",
    "# - 'clk'     : original binary click label\n",
    "# - 'nonclk'  : complement of clk, leaks label perfectly\n",
    "# - 'noclk'   : alternative spelling (just in case)\n",
    "# - 'impressions' : if created, clk + nonclk\n",
    "# - time_stamp columns: can be too granular and duplicate user behavior\n",
    "leakage_cols = [\n",
    "    \"clk\",\n",
    "    \"nonclk\",\n",
    "    \"noclk\",\n",
    "    \"impressions\",\n",
    "    \"time_stamp\",\n",
    "    \"time_stamp_str\",\n",
    "]\n",
    "\n",
    "for c in leakage_cols:\n",
    "    if c in df.columns and c != target_col:\n",
    "        print(\"REMOVED LEAKAGE COLUMN:\", c)\n",
    "        df.drop(columns=[c], inplace=True)\n",
    "\n",
    "print(\"Columns after leakage removal:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# ============================================\n",
    "# 5. User-level train/test split\n",
    "# ============================================\n",
    "\n",
    "unique_users = df[user_col].unique()\n",
    "print(\"Number of unique users:\", len(unique_users))\n",
    "\n",
    "train_users, test_users = train_test_split(\n",
    "    unique_users, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_df = df[df[user_col].isin(train_users)].copy()\n",
    "test_df = df[df[user_col].isin(test_users)].copy()\n",
    "\n",
    "print(\"Train_df shape:\", train_df.shape)\n",
    "print(\"Test_df shape:\", test_df.shape)\n",
    "\n",
    "# ============================================\n",
    "# 6. Build X and y (drop IDs)\n",
    "# ============================================\n",
    "\n",
    "def build_xy(sub_df, target_col, user_col):\n",
    "    # Start from full dataframe\n",
    "    X = sub_df.copy()\n",
    "    y = X[target_col].astype(int)\n",
    "    X = X.drop(columns=[target_col])\n",
    "\n",
    "    # Drop ID-like columns (including duplicated names like user.1, adgroup_id.1)\n",
    "    id_keywords = [\"user\", \"userid\", \"nick\", \"adgroup_id\", \"campaign_id\",\n",
    "                   \"customer\", \"pid\"]\n",
    "    id_like_cols = [c for c in X.columns if any(k in c for k in id_keywords)]\n",
    "\n",
    "    if id_like_cols:\n",
    "        print(\"Dropping ID-like columns:\", id_like_cols)\n",
    "        X = X.drop(columns=id_like_cols)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = build_xy(train_df, target_col, user_col)\n",
    "X_test, y_test = build_xy(test_df, target_col, user_col)\n",
    "\n",
    "print(\"X_train shape (raw):\", X_train.shape)\n",
    "print(\"X_test shape  (raw):\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "print(\"\\ny_train distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\ny_test distribution:\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# ============================================\n",
    "# 7. Identify numeric and categorical features\n",
    "# ============================================\n",
    "\n",
    "categorical_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "numeric_cols = [c for c in X_train.columns if c not in categorical_cols]\n",
    "\n",
    "print(\"\\nNumeric columns:\", len(numeric_cols))\n",
    "print(\"Categorical columns:\", len(categorical_cols))\n",
    "\n",
    "# ============================================\n",
    "# 8. Preprocessing pipeline (imputer + encoder + scaler)\n",
    "# ============================================\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 9. Fit preprocessing and transform\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nFitting preprocessing pipeline on X_train...\")\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"X_train_transformed:\", X_train_transformed.shape)\n",
    "print(\"X_test_transformed:\", X_test_transformed.shape)\n",
    "\n",
    "def has_nan(matrix):\n",
    "    if sparse.issparse(matrix):\n",
    "        return np.isnan(matrix.data).any()\n",
    "    else:\n",
    "        return np.isnan(matrix).any()\n",
    "\n",
    "print(\"NaN in X_train_transformed:\", has_nan(X_train_transformed))\n",
    "print(\"NaN in X_test_transformed:\", has_nan(X_test_transformed))\n",
    "\n",
    "if has_nan(X_train_transformed):\n",
    "    raise ValueError(\"NaNs detected in X_train_transformed. Check imputers.\")\n",
    "\n",
    "# ============================================\n",
    "# 10. SMOTE on preprocessed training data\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nApplying SMOTE to handle class imbalance...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(\n",
    "    X_train_transformed, y_train\n",
    ")\n",
    "\n",
    "print(\"X_train_resampled shape:\", X_train_resampled.shape)\n",
    "print(\"y_train_resampled distribution:\")\n",
    "print(pd.Series(y_train_resampled).value_counts(normalize=True))\n",
    "\n",
    "# ============================================\n",
    "# 11. Save NPZ matrices, labels, and preprocessor\n",
    "# ============================================\n",
    "\n",
    "Path(processed_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "X_train_resampled_path = os.path.join(processed_dir, \"X_train_resampled.npz\")\n",
    "X_test_path = os.path.join(processed_dir, \"X_test_transformed.npz\")\n",
    "y_train_resampled_path = os.path.join(processed_dir, \"y_train_resampled.csv\")\n",
    "y_test_path = os.path.join(processed_dir, \"y_test.csv\")\n",
    "preprocessor_path = os.path.join(processed_dir, \"preprocessor.joblib\")\n",
    "\n",
    "sparse.save_npz(X_train_resampled_path, X_train_resampled)\n",
    "sparse.save_npz(X_test_path, X_test_transformed)\n",
    "pd.Series(y_train_resampled, name=target_col).to_csv(y_train_resampled_path, index=False)\n",
    "y_test.to_csv(y_test_path, index=False)\n",
    "dump(preprocessor, preprocessor_path)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" -\", X_train_resampled_path)\n",
    "print(\" -\", X_test_path)\n",
    "print(\" -\", y_train_resampled_path)\n",
    "print(\" -\", y_test_path)\n",
    "print(\" -\", preprocessor_path)\n",
    "\n",
    "print(\"\\n[Preprocessing completed successfully – data is ready for model training.]\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
