{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dccaed4",
   "metadata": {},
   "source": [
    "# 03 – Exploratory Data Analysis & Data Fusion (PySpark)\n",
    "\n",
    "In this notebook we explore the cleaned data and perform data fusion.  We demonstrate **early fusion**, where all relevant tables are joined into one wide table, and **hybrid fusion**, where we engineer features from auxiliary tables and merge them back into the main data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545c36b",
   "metadata": {},
   "source": [
    "## Load cleaned data\n",
    "\n",
    "We load the cleaned datasets saved as Parquet files from the previous step.  Parquet storage is efficient and preserves schema information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "780fad6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/d:/projects/Ai/project_fusion_ecu/data/processed/user_profile_clean.parquet. SQLSTATE: 42K03",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m processed_dir = os.path.join(notebook_dir, \u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mprocessed\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m processed_dir = os.path.abspath(processed_dir)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m user_df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser_profile_clean.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m ad_df = spark.read.parquet(os.path.join(processed_dir, \u001b[33m'\u001b[39m\u001b[33mad_feature_clean.parquet\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     16\u001b[39m click_df = spark.read.parquet(os.path.join(processed_dir, \u001b[33m'\u001b[39m\u001b[33mraw_sample_clean.parquet\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\Ai\\project_fusion_ecu\\.venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:642\u001b[39m, in \u001b[36mDataFrameReader.parquet\u001b[39m\u001b[34m(self, *paths, **options)\u001b[39m\n\u001b[32m    631\u001b[39m int96RebaseMode = options.get(\u001b[33m\"\u001b[39m\u001b[33mint96RebaseMode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m    633\u001b[39m     mergeSchema=mergeSchema,\n\u001b[32m    634\u001b[39m     pathGlobFilter=pathGlobFilter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    639\u001b[39m     int96RebaseMode=int96RebaseMode,\n\u001b[32m    640\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\Ai\\project_fusion_ecu\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\Ai\\project_fusion_ecu\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/d:/projects/Ai/project_fusion_ecu/data/processed/user_profile_clean.parquet. SQLSTATE: 42K03"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.appName('CTR_EDA_Fusion').getOrCreate()\n",
    "\n",
    "# Get the absolute path to the data directory\n",
    "notebook_dir = os.path.dirname(os.path.abspath('03_eda_fusion.ipynb'))\n",
    "processed_dir = os.path.join(notebook_dir, '..', 'data', 'processed')\n",
    "processed_dir = os.path.abspath(processed_dir)\n",
    "\n",
    "user_df = spark.read.parquet(os.path.join(processed_dir, 'user_profile_clean.parquet'))\n",
    "\n",
    "ad_df = spark.read.parquet(os.path.join(processed_dir, 'ad_feature_clean.parquet'))\n",
    "\n",
    "click_df = spark.read.parquet(os.path.join(processed_dir, 'raw_sample_clean.parquet'))\n",
    "\n",
    "behaviour_df = spark.read.parquet(os.path.join(processed_dir, 'behavior_log_clean.parquet'))\n",
    "\n",
    "print('Data loaded:')\n",
    "print('user_df:', user_df.count())\n",
    "print('ad_df:', ad_df.count())\n",
    "print('click_df:', click_df.count())\n",
    "print('behaviour_df:', behaviour_df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb8ee3",
   "metadata": {},
   "source": [
    "## Early fusion\n",
    "\n",
    "We join the user profile and ad feature tables onto the click log.  A left join retains all click records, while adding demographic and ad attributes.  Keys used:\n",
    "\n",
    "- `user` from click log matches `userid` in the user profile.\n",
    "- `adgroup_id` from click log matches `adgroup_id` in the ad feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f53a982e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'click_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Join click log with user profile on user ID\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m click_user = \u001b[43mclick_df\u001b[49m.join(user_df, click_df[\u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m] == user_df[\u001b[33m'\u001b[39m\u001b[33muserid\u001b[39m\u001b[33m'\u001b[39m], how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Join with ad features on adgroup ID\u001b[39;00m\n\u001b[32m      5\u001b[39m full_df = click_user.join(ad_df, \u001b[33m'\u001b[39m\u001b[33madgroup_id\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'click_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Join click log with user profile on user ID\n",
    "click_user = click_df.join(user_df, click_df['user'] == user_df['userid'], how='left')\n",
    "\n",
    "# Join with ad features on adgroup ID\n",
    "full_df = click_user.join(ad_df, 'adgroup_id', how='left')\n",
    "\n",
    "print('Early fused dataframe rows:', full_df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37724ca",
   "metadata": {},
   "source": [
    "## Hybrid fusion features\n",
    "\n",
    "We engineer behavioural features from the behaviour log.  We compute the count of each behaviour type (page view, cart addition, favourite and purchase) per user and merge these aggregated features into the fused table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b72989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot behaviour log to get counts per user for each btag category\n",
    "behaviour_counts = behaviour_df.groupBy('user').pivot('btag').agg(count('*')).fillna(0)\n",
    "\n",
    "# Merge counts into the fused table\n",
    "full_df = full_df.join(behaviour_counts, 'user', how='left')\n",
    "\n",
    "# Fill any missing counts with zero\n",
    "for col_name in ['buy', 'cart', 'fav', 'ipv']:\n",
    "    if col_name in full_df.columns:\n",
    "        full_df = full_df.na.fill({col_name: 0})\n",
    "\n",
    "print('Hybrid fused dataframe rows:', full_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60424da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hybrid fused DataFrame for later use\n",
    "import os\n",
    "processed_dir = os.path.join('..', 'data', 'processed')\n",
    "full_df.write.mode('overwrite').parquet(os.path.join(processed_dir, 'hybrid_fusion.parquet'))\n",
    "print('Hybrid fused data saved to hybrid_fusion.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce6ece",
   "metadata": {},
   "source": [
    "## Sampling for visual exploration\n",
    "\n",
    "For plotting and correlation analysis, we take a random sample of the fused DataFrame and convert it to a Pandas DataFrame.  This is necessary because visualisation libraries expect in‑memory data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c6437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample of 10 000 rows for plotting\n",
    "sample_size = 10000\n",
    "sample_df = full_df.sample(fraction=sample_size / full_df.count(), seed=42).toPandas()\n",
    "\n",
    "# Plot distributions of selected numeric features\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "numeric_cols = ['age_level', 'price', 'buy', 'cart', 'fav', 'ipv']\n",
    "\n",
    "fig, axes = plt.subplots(len(numeric_cols), 2, figsize=(12, 4 * len(numeric_cols)))\n",
    "for i, col_name in enumerate(numeric_cols):\n",
    "    sns.histplot(sample_df[col_name], ax=axes[i, 0], kde=True)\n",
    "    axes[i, 0].set_title(f'Distribution of {col_name}')\n",
    "    sns.boxplot(x=sample_df[col_name], ax=axes[i, 1])\n",
    "    axes[i, 1].set_title(f'Boxplot of {col_name}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00905579",
   "metadata": {},
   "source": [
    "## Correlation analysis\n",
    "\n",
    "We compute the correlation matrix for the numeric features on the sample.  This helps identify multicollinearity among features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bbba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr = sample_df[numeric_cols].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', center=0)\n",
    "plt.title('Correlation Matrix of Numeric Features')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
