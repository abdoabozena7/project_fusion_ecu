{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6e9649",
   "metadata": {},
   "source": [
    "# 03 â€“ Exploratory Data Analysis and Data Fusion (PySpark)\n",
    "\n",
    "This notebook performs exploratory data analysis and demonstrates different fusion strategies.  We investigate distribution of variables, relationships between features and the target, and how to combine data sources.\n",
    "\n",
    "**Fusion strategies:**\n",
    "- **Early Fusion**: Join all relevant tables into a single wide table.\n",
    "- **Hybrid Fusion**: Aggregate behaviour logs into features (counts) and merge them into the click log.\n",
    "\n",
    "We use PySpark for heavy data processing and Pandas for visualisation on sampled data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8396e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n",
      "\n",
      "click_df capped (no count). Showing 3 rows:\n",
      "+----+----------+------+----------+-----+---+-------------------+\n",
      "|user|adgroup_id|rating|time_stamp|label|clk|time_stamp_str     |\n",
      "+----+----------+------+----------+-----+---+-------------------+\n",
      "|307 |7701      |0.5   |1186173318|0    |0  |2007-08-03 23:35:18|\n",
      "|307 |3063      |1.0   |1186173320|0    |0  |2007-08-03 23:35:20|\n",
      "|307 |2169      |1.5   |1186173322|0    |0  |2007-08-03 23:35:22|\n",
      "+----+----------+------+----------+-----+---+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "After Early Fusion (no count). Columns: 13\n",
      "\n",
      "Building time-aware behavior features...\n",
      "Time-aware behavior added. Columns: 18\n",
      "\n",
      "Enrichment checks (using sample to avoid heavy counts):\n",
      "\n",
      "EDA sample shape: (20000, 20)\n",
      "Saved figure: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\figures\\01_label_distribution.png\n",
      "Saved figure: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\figures\\02_ctr_over_time_day.png\n",
      "Saved figure: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\figures\\03_ctr_by_hour.png\n",
      "\n",
      "Saved:\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\fused_train.csv\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\fused_test.csv\n",
      " - d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\fused_data.csv\n",
      "\n",
      "Spark stopped.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "\n",
    "# 1) Settings\n",
    "SEED = 42\n",
    "\n",
    "# EDA sampling only \n",
    "SAMPLE_SIZE_EDA = 20_000\n",
    "\n",
    "MAX_FUSION_ROWS = 50_000     # start safe;\n",
    "\n",
    "# Behavior lookback window before each click time\n",
    "LOOKBACK_DAYS = 7\n",
    "LOOKBACK_SECONDS = LOOKBACK_DAYS * 86400\n",
    "\n",
    "TOP_N = 15\n",
    "FIG_DPI = 140\n",
    "\n",
    "# 2) Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"CTR_Fusion_Visual_EDA_Fixed\")\n",
    "        .master(\"local[2]\")  # safer than local[4]\n",
    "        .config(\"spark.driver.memory\", \"6g\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"60\")\n",
    "        .config(\"spark.python.worker.reuse\", \"true\")\n",
    "        .config(\"spark.network.timeout\", \"800s\")\n",
    "        .config(\"spark.executor.heartbeatInterval\", \"60s\")\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\") # verbose: INFO, WARN, ERROR\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "# 3) Paths\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "processed_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "fig_dir = os.path.join(processed_dir, \"figures\")\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# 4) Load cleaned CSVs\n",
    "user_df = spark.read.csv(os.path.join(processed_dir, \"user_profile_clean.csv\"), header=True, inferSchema=True)\n",
    "ad_df = spark.read.csv(os.path.join(processed_dir, \"ad_feature_clean.csv\"), header=True, inferSchema=True)\n",
    "click_df = spark.read.csv(os.path.join(processed_dir, \"raw_sample_clean.csv\"), header=True, inferSchema=True)\n",
    "behavior_df = spark.read.csv(os.path.join(processed_dir, \"behavior_log_clean.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "def strip_columns(df):\n",
    "    return df.toDF(*[c.strip() for c in df.columns])\n",
    "\n",
    "user_df = strip_columns(user_df)\n",
    "ad_df = strip_columns(ad_df)\n",
    "click_df = strip_columns(click_df)\n",
    "behavior_df = strip_columns(behavior_df)\n",
    "\n",
    "def ensure_user_col(df, candidates=(\"user\", \"userid\", \"nick\")):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            if c != \"user\":\n",
    "                return df.withColumnRenamed(c, \"user\")\n",
    "            return df\n",
    "    return df\n",
    "\n",
    "user_df = ensure_user_col(user_df)\n",
    "click_df = ensure_user_col(click_df)\n",
    "behavior_df = ensure_user_col(behavior_df)\n",
    "\n",
    "# 5) Unify key types \n",
    "if \"user\" in user_df.columns:\n",
    "    user_df = user_df.withColumn(\"user\", F.col(\"user\").cast(\"int\"))\n",
    "if \"user\" in click_df.columns:\n",
    "    click_df = click_df.withColumn(\"user\", F.col(\"user\").cast(\"int\"))\n",
    "if \"user\" in behavior_df.columns:\n",
    "    behavior_df = behavior_df.withColumn(\"user\", F.col(\"user\").cast(\"int\"))\n",
    "\n",
    "if \"adgroup_id\" in ad_df.columns:\n",
    "    ad_df = ad_df.withColumn(\"adgroup_id\", F.col(\"adgroup_id\").cast(\"int\"))\n",
    "if \"adgroup_id\" in click_df.columns:\n",
    "    click_df = click_df.withColumn(\"adgroup_id\", F.col(\"adgroup_id\").cast(\"int\"))\n",
    "\n",
    "# 6) Cap click_df size safely \n",
    "if \"time_stamp\" in click_df.columns:\n",
    "    click_df = click_df.withColumn(\"time_stamp\", F.col(\"time_stamp\").cast(\"long\"))\n",
    "    # take latest N then reorder chronologically\n",
    "    click_df = (\n",
    "        click_df.orderBy(F.col(\"time_stamp\").desc())\n",
    "                .limit(MAX_FUSION_ROWS)\n",
    "                .orderBy(\"time_stamp\")\n",
    "    )\n",
    "else:\n",
    "    click_df = click_df.orderBy(F.rand(seed=SEED)).limit(MAX_FUSION_ROWS)\n",
    "\n",
    "print(\"\\nclick_df capped (no count). Showing 3 rows:\")\n",
    "click_df.show(3, truncate=False)\n",
    "\n",
    "# 7) Early Fusion\n",
    "c = click_df.alias(\"c\")\n",
    "u = user_df.alias(\"u\")\n",
    "a = ad_df.alias(\"a\")\n",
    "\n",
    "click_cols = [F.col(f\"c.{x}\") for x in click_df.columns]\n",
    "user_cols_except_user = [F.col(f\"u.{x}\") for x in user_df.columns if x != \"user\"]\n",
    "\n",
    "click_user_df = (\n",
    "    c.join(u, F.col(\"c.user\") == F.col(\"u.user\"), how=\"left\")\n",
    "     .select(*click_cols, *user_cols_except_user)\n",
    ")\n",
    "\n",
    "cu = click_user_df.alias(\"cu\")\n",
    "cu_cols_except_adgroup = [F.col(f\"cu.{x}\") for x in click_user_df.columns if x != \"adgroup_id\"]\n",
    "ad_cols_except_key = [F.col(f\"a.{x}\") for x in ad_df.columns if x != \"adgroup_id\"]\n",
    "\n",
    "full_df = (\n",
    "    cu.join(a, F.col(\"cu.adgroup_id\") == F.col(\"a.adgroup_id\"), how=\"left\")\n",
    "      .select(\n",
    "          *cu_cols_except_adgroup,\n",
    "          F.col(\"cu.adgroup_id\").alias(\"adgroup_id\"),\n",
    "          *ad_cols_except_key\n",
    "      )\n",
    ")\n",
    "\n",
    "print(\"\\nAfter Early Fusion (no count). Columns:\", len(full_df.columns))\n",
    "\n",
    "\n",
    "can_time_aware = (\n",
    "    (\"time_stamp\" in full_df.columns) and\n",
    "    all(x in behavior_df.columns for x in [\"user\", \"time_stamp\", \"btag\"])\n",
    ")\n",
    "\n",
    "if can_time_aware:\n",
    "    print(\"\\nBuilding time-aware behavior features...\")\n",
    "\n",
    "    beh = behavior_df.select(\n",
    "        F.col(\"user\").cast(\"int\").alias(\"b_user\"),\n",
    "        F.col(\"time_stamp\").cast(\"long\").alias(\"b_time\"),\n",
    "        F.col(\"btag\").cast(\"string\").alias(\"btag\"),\n",
    "    )\n",
    "\n",
    "    f = full_df.select(\n",
    "        \"*\",\n",
    "        F.col(\"user\").cast(\"int\").alias(\"c_user\"),\n",
    "        F.col(\"time_stamp\").cast(\"long\").alias(\"c_time\"),\n",
    "    )\n",
    "\n",
    "    # stable per-row id\n",
    "    f = f.withColumn(\"row_id\", F.monotonically_increasing_id())\n",
    "\n",
    "    joined = (\n",
    "        f.join(\n",
    "            beh,\n",
    "            (F.col(\"c_user\") == F.col(\"b_user\")) &\n",
    "            (F.col(\"b_time\") < F.col(\"c_time\")) &\n",
    "            (F.col(\"b_time\") >= (F.col(\"c_time\") - F.lit(LOOKBACK_SECONDS))),\n",
    "            how=\"left\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # adjust these if your btag values differ\n",
    "    pv_tags = [\"pv\", \"pageview\"]\n",
    "    cart_tags = [\"cart\"]\n",
    "    fav_tags = [\"fav\", \"favorite\"]\n",
    "    buy_tags = [\"buy\"]\n",
    "\n",
    "    agg = (\n",
    "        joined.groupBy(\"row_id\")\n",
    "              .agg(\n",
    "                  F.sum(F.when(F.col(\"btag\").isin(pv_tags), 1).otherwise(0)).alias(f\"pv_{LOOKBACK_DAYS}d\"),\n",
    "                  F.sum(F.when(F.col(\"btag\").isin(cart_tags), 1).otherwise(0)).alias(f\"cart_{LOOKBACK_DAYS}d\"),\n",
    "                  F.sum(F.when(F.col(\"btag\").isin(fav_tags), 1).otherwise(0)).alias(f\"fav_{LOOKBACK_DAYS}d\"),\n",
    "                  F.sum(F.when(F.col(\"btag\").isin(buy_tags), 1).otherwise(0)).alias(f\"buy_{LOOKBACK_DAYS}d\"),\n",
    "              )\n",
    "    )\n",
    "\n",
    "    full_df = (\n",
    "        f.join(agg, on=\"row_id\", how=\"left\")\n",
    "         .drop(\"c_user\", \"c_time\", \"b_user\", \"b_time\", \"btag\")  # safe even if not present\n",
    "         .fillna({\n",
    "             f\"pv_{LOOKBACK_DAYS}d\": 0,\n",
    "             f\"cart_{LOOKBACK_DAYS}d\": 0,\n",
    "             f\"fav_{LOOKBACK_DAYS}d\": 0,\n",
    "             f\"buy_{LOOKBACK_DAYS}d\": 0,\n",
    "         })\n",
    "    )\n",
    "\n",
    "    print(\"Time-aware behavior added. Columns:\", len(full_df.columns))\n",
    "else:\n",
    "    print(\"\\nWarning: cannot compute time-aware behavior (missing columns).\")\n",
    "\n",
    "# 9) Add label + time features\n",
    "if \"clk\" in full_df.columns:\n",
    "    full_df = full_df.withColumn(\"label\", F.col(\"clk\").cast(\"int\"))\n",
    "else:\n",
    "    print(\"Warning: clk missing; label will not be created.\")\n",
    "\n",
    "if \"time_stamp\" in full_df.columns:\n",
    "    full_df = full_df.withColumn(\"hour\", (F.col(\"time_stamp\") / 3600).cast(\"long\") % 24)\n",
    "    full_df = full_df.withColumn(\"day\", (F.col(\"time_stamp\") / 86400).cast(\"long\"))\n",
    "else:\n",
    "    print(\"Warning: time_stamp missing; hour/day will not be created.\")\n",
    "\n",
    "# 10) Join enrichment checks \n",
    "join_checks = []\n",
    "if \"age_level\" in full_df.columns:\n",
    "    join_checks.append((\"User join (age_level not null)\", \"age_level\"))\n",
    "if \"price\" in full_df.columns:\n",
    "    join_checks.append((\"Ad join (price not null)\", \"price\"))\n",
    "if \"cate_id\" in full_df.columns:\n",
    "    join_checks.append((\"Ad join (cate_id not null)\", \"cate_id\"))\n",
    "\n",
    "print(\"\\nEnrichment checks (using sample to avoid heavy counts):\")\n",
    "enrichment = []\n",
    "qc_sample = full_df.orderBy(F.rand(seed=SEED)).limit(20_000)\n",
    "qc_pdf = qc_sample.toPandas()\n",
    "\n",
    "for label_name, col_name in join_checks:\n",
    "    rate = float((~qc_pdf[col_name].isna()).mean()) if col_name in qc_pdf.columns else 0.0\n",
    "    enrichment.append((label_name, rate))\n",
    "    print(\"-\", label_name, \":\", round(rate * 100, 2), \"%\")\n",
    "\n",
    "# 11) Sample for EDA plots\n",
    "sample_sdf = full_df.orderBy(F.rand(seed=SEED)).limit(SAMPLE_SIZE_EDA)\n",
    "sample_pdf = sample_sdf.toPandas()\n",
    "print(\"\\nEDA sample shape:\", sample_pdf.shape)\n",
    "\n",
    "def save_fig(name):\n",
    "    path = os.path.join(fig_dir, name)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=FIG_DPI)\n",
    "    print(\"Saved figure:\", path)\n",
    "    plt.close()\n",
    "\n",
    "def safe_has_cols(pdf, cols):\n",
    "    return all(c in pdf.columns for c in cols)\n",
    "\n",
    "# 1) Class balance\n",
    "if \"label\" in sample_pdf.columns:\n",
    "    counts = sample_pdf[\"label\"].value_counts().sort_index()\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(counts.index.astype(str), counts.values)\n",
    "    plt.title(\"Target distribution: Click vs No-Click\")\n",
    "    plt.xlabel(\"label (0=No Click, 1=Click)\")\n",
    "    plt.ylabel(\"count\")\n",
    "    save_fig(\"01_label_distribution.png\")\n",
    "\n",
    "# 2) CTR by day\n",
    "if safe_has_cols(sample_pdf, [\"day\", \"label\"]):\n",
    "    tmp = sample_pdf.dropna(subset=[\"day\", \"label\"]).copy()\n",
    "    ctr_by_day = tmp.groupby(\"day\")[\"label\"].mean().reset_index().sort_values(\"day\")\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(ctr_by_day[\"day\"], ctr_by_day[\"label\"])\n",
    "    plt.title(\"CTR over time (daily)\")\n",
    "    plt.xlabel(\"day\")\n",
    "    plt.ylabel(\"CTR\")\n",
    "    save_fig(\"02_ctr_over_time_day.png\")\n",
    "\n",
    "# 3) CTR by hour\n",
    "if safe_has_cols(sample_pdf, [\"hour\", \"label\"]):\n",
    "    tmp = sample_pdf.dropna(subset=[\"hour\", \"label\"]).copy()\n",
    "    ctr_by_hour = tmp.groupby(\"hour\")[\"label\"].mean().reset_index().sort_values(\"hour\")\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(ctr_by_hour[\"hour\"].astype(int), ctr_by_hour[\"label\"])\n",
    "    plt.title(\"CTR by hour\")\n",
    "    plt.xlabel(\"hour\")\n",
    "    plt.ylabel(\"CTR\")\n",
    "    save_fig(\"03_ctr_by_hour.png\")\n",
    "\n",
    "# 4) Join enrichment bar\n",
    "if enrichment:\n",
    "    names = [x[0] for x in enrichment]\n",
    "    vals = [x[1] for x in enrichment]\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.barh(names, vals)\n",
    "    plt.title(\"Join enrichment rates (sample-based)\")\n",
    "    plt.xlabel(\"Enrichment Rate\")\n",
    "    plt.xlim(0, 1)\n",
    "    save_fig(\"04_join_enrichment_rates.png\")\n",
    "\n",
    "# 5) CTR by cate_id/brand topN\n",
    "def plot_ctr_topN(pdf, col_cat, top_n=TOP_N, fname=\"ctr_top.png\", title=\"\"):\n",
    "    if not safe_has_cols(pdf, [col_cat, \"label\"]):\n",
    "        return\n",
    "    tmp = pdf[[col_cat, \"label\"]].dropna()\n",
    "    vc = tmp[col_cat].value_counts().head(top_n)\n",
    "    top_cats = set(vc.index.tolist())\n",
    "    tmp2 = tmp[tmp[col_cat].isin(top_cats)]\n",
    "\n",
    "    out = tmp2.groupby(col_cat)[\"label\"].mean().reset_index()\n",
    "    out[\"count\"] = tmp2.groupby(col_cat)[\"label\"].size().values\n",
    "    out = out.sort_values(\"count\", ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    if HAS_SNS:\n",
    "        sns.barplot(data=out, x=col_cat, y=\"label\")\n",
    "    else:\n",
    "        plt.bar(out[col_cat].astype(str), out[\"label\"].values)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(col_cat)\n",
    "    plt.ylabel(\"CTR\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    save_fig(fname)\n",
    "\n",
    "if \"cate_id\" in sample_pdf.columns:\n",
    "    plot_ctr_topN(sample_pdf, \"cate_id\", fname=\"05_ctr_by_cate_topN.png\", title=f\"CTR by cate_id (Top {TOP_N})\")\n",
    "if \"brand\" in sample_pdf.columns:\n",
    "    plot_ctr_topN(sample_pdf, \"brand\", fname=\"06_ctr_by_brand_topN.png\", title=f\"CTR by brand (Top {TOP_N})\")\n",
    "\n",
    "# 6) price vs label\n",
    "if safe_has_cols(sample_pdf, [\"price\", \"label\"]):\n",
    "    tmp = sample_pdf[[\"price\", \"label\"]].dropna()\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    if HAS_SNS:\n",
    "        sns.boxplot(x=\"label\", y=\"price\", data=tmp)\n",
    "    else:\n",
    "        g0 = tmp[tmp[\"label\"] == 0][\"price\"].values\n",
    "        g1 = tmp[tmp[\"label\"] == 1][\"price\"].values\n",
    "        plt.boxplot([g0, g1], labels=[\"0\", \"1\"])\n",
    "        plt.xlabel(\"label\")\n",
    "    plt.title(\"Price distribution by click\")\n",
    "    plt.ylabel(\"price\")\n",
    "    save_fig(\"07_price_vs_label_boxplot.png\")\n",
    "\n",
    "# 12) Save fused data + Temporal split \n",
    "if \"time_stamp\" in full_df.columns:\n",
    "    cutoff = full_df.approxQuantile(\"time_stamp\", [0.8], 0.001)[0]\n",
    "    train_df = full_df.filter(F.col(\"time_stamp\") <= cutoff)\n",
    "    test_df  = full_df.filter(F.col(\"time_stamp\") > cutoff)\n",
    "\n",
    "    train_path = os.path.join(processed_dir, \"fused_train.csv\")\n",
    "    test_path  = os.path.join(processed_dir, \"fused_test.csv\")\n",
    "    fused_path = os.path.join(processed_dir, \"fused_data.csv\")\n",
    "\n",
    "    # Save train/test\n",
    "    train_df.toPandas().to_csv(train_path, index=False)\n",
    "    test_df.toPandas().to_csv(test_path, index=False)\n",
    "\n",
    "    # Also save full \n",
    "    full_df.toPandas().to_csv(fused_path, index=False)\n",
    "\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\" -\", train_path)\n",
    "    print(\" -\", test_path)\n",
    "    print(\" -\", fused_path)\n",
    "else:\n",
    "    fused_path = os.path.join(processed_dir, \"fused_data.csv\")\n",
    "    full_df.toPandas().to_csv(fused_path, index=False)\n",
    "    print(\"\\nSaved:\", fused_path)\n",
    "\n",
    "spark.stop()\n",
    "print(\"\\nSpark stopped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
