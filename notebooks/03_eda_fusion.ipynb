{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6e9649",
   "metadata": {},
   "source": [
    "# 03 â€“ Exploratory Data Analysis and Data Fusion (PySpark)\n",
    "\n",
    "This notebook performs exploratory data analysis and demonstrates different fusion strategies.  We investigate distribution of variables, relationships between features and the target, and how to combine data sources.\n",
    "\n",
    "**Fusion strategies:**\n",
    "- **Early Fusion**: Join all relevant tables into a single wide table.\n",
    "- **Hybrid Fusion**: Aggregate behaviour logs into features (counts) and merge them into the click log.\n",
    "\n",
    "We use PySpark for heavy data processing and Pandas for visualisation on sampled data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe8396e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n",
      "Project root: d:\\projects\\Ai\\project_fusion_ecu\n",
      "Processed dir: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\n",
      "Figures dir: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\figures\n",
      "Rows: user_df 500000 ad_df 500000 click_df 500000 behavior_df 499964\n",
      "After Early Fusion: rows: 500000 cols: 21\n",
      "After Hybrid Fusion (Behavior): rows: 500000 cols: 25\n",
      "\n",
      "Enrichment rates after joins:\n",
      "- User join (age_level not null) : 44.28 %\n",
      "- Ad join (price not null) : 59.56 %\n",
      "- Ad join (cate_id not null) : 59.56 %\n",
      "Sample size for EDA: (20000, 28)\n",
      "Saved figure: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\figures\\01_label_distribution.png\n",
      "Saved figure: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\figures\\02_ctr_over_time_day.png\n",
      "Saved figure: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\figures\\03_ctr_by_hour.png\n",
      "Saved figure: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\figures\\04_join_enrichment_rates.png\n",
      "Saved figure: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\figures\\05_ctr_by_cate_topN.png\n",
      "Saved figure: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\figures\\06_ctr_by_brand_topN.png\n",
      "Saved figure: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\figures\\07_price_vs_label_boxplot.png\n",
      "Saved figure: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\figures\\08_missingness_rates.png\n",
      "Saved figure: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\figures\\09_numeric_correlation_heatmap.png\n",
      "\n",
      "Temporal split 80/20:\n",
      "cutoff time_stamp: 1494565111.0\n",
      "train rows: 399821\n",
      "test rows : 100179\n",
      "Saved: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\fused_train.csv\n",
      "Saved: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\fused_test.csv\n",
      "Saved figure: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\figures\\10_precision_recall_curve.png\n",
      "Saved figure: d:\\projects\\Ai\\project_fusion_ecu\\data\\processed\\figures\\11_score_distribution.png\n",
      "PR Curve and Score Distribution created (optional).\n",
      "Spark stopped.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Single-file: Fusion + Visual EDA + ready outputs\n",
    "# Language: English (comments/strings)\n",
    "# ============================================================\n",
    "# What this notebook does:\n",
    "# 1) Read cleaned CSVs from data/processed\n",
    "# 2) Fusion (Click + User + Ad) + Behavior Aggregation\n",
    "# 3) Join quality checks (Enrichment Rates)\n",
    "# 4) Visual EDA to support preprocessing decisions:\n",
    "#    - Class balance (Bar)\n",
    "#    - CTR over time (Line)\n",
    "#    - CTR by hour (Bar)\n",
    "#    - Join quality (Bar)\n",
    "#    - CTR by category (Top-N) (Bar)\n",
    "#    - price vs label (Boxplot)\n",
    "#    - Missingness (Bar)\n",
    "#    - Correlation heatmap (Numeric)\n",
    "#    - (optional) PR Curve + Score Distribution after baseline model if npz files exist\n",
    "# 5) Save figures in data/processed/figures\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seaborn is optional for visuals\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    HAS_SNS = True\n",
    "except Exception:\n",
    "    HAS_SNS = False\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0) General settings\n",
    "# ============================================================\n",
    "SEED = 42\n",
    "SAMPLE_SIZE = 20000          # sample size for quick EDA in Pandas\n",
    "TOP_N = 15                   # top N categories for cate/brand\n",
    "FIG_DPI = 140\n",
    "\n",
    "# ============================================================\n",
    "# 1) Spark Session\n",
    "# ============================================================\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"CTR_Fusion_Visual_EDA\")\n",
    "        .master(\"local[4]\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "        .config(\"spark.driver.memory\", \"6g\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "# ============================================================\n",
    "# 2) Paths\n",
    "# ============================================================\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "processed_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "fig_dir = os.path.join(processed_dir, \"figures\")\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"Processed dir:\", processed_dir)\n",
    "print(\"Figures dir:\", fig_dir)\n",
    "\n",
    "# ============================================================\n",
    "# 3) Load cleaned CSVs\n",
    "# ============================================================\n",
    "user_df = spark.read.csv(os.path.join(processed_dir, \"user_profile_clean.csv\"), header=True, inferSchema=True)\n",
    "ad_df = spark.read.csv(os.path.join(processed_dir, \"ad_feature_clean.csv\"), header=True, inferSchema=True)\n",
    "click_df = spark.read.csv(os.path.join(processed_dir, \"raw_sample_clean.csv\"), header=True, inferSchema=True)\n",
    "behavior_df = spark.read.csv(os.path.join(processed_dir, \"behavior_log_clean.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "def strip_columns(df):\n",
    "    return df.toDF(*[c.strip() for c in df.columns])\n",
    "\n",
    "user_df = strip_columns(user_df)\n",
    "ad_df = strip_columns(ad_df)\n",
    "click_df = strip_columns(click_df)\n",
    "behavior_df = strip_columns(behavior_df)\n",
    "\n",
    "def ensure_user_col(df, candidates=(\"user\", \"userid\", \"nick\")):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            if c != \"user\":\n",
    "                return df.withColumnRenamed(c, \"user\")\n",
    "            return df\n",
    "    return df\n",
    "\n",
    "user_df = ensure_user_col(user_df)\n",
    "click_df = ensure_user_col(click_df)\n",
    "behavior_df = ensure_user_col(behavior_df)\n",
    "\n",
    "# ============================================================\n",
    "# 4) Unify join key types (important)\n",
    "# ============================================================\n",
    "if \"user\" in user_df.columns:\n",
    "    user_df = user_df.withColumn(\"user\", F.col(\"user\").cast(\"int\"))\n",
    "if \"user\" in click_df.columns:\n",
    "    click_df = click_df.withColumn(\"user\", F.col(\"user\").cast(\"int\"))\n",
    "if \"user\" in behavior_df.columns:\n",
    "    behavior_df = behavior_df.withColumn(\"user\", F.col(\"user\").cast(\"int\"))\n",
    "\n",
    "if \"adgroup_id\" in ad_df.columns:\n",
    "    ad_df = ad_df.withColumn(\"adgroup_id\", F.col(\"adgroup_id\").cast(\"int\"))\n",
    "if \"adgroup_id\" in click_df.columns:\n",
    "    click_df = click_df.withColumn(\"adgroup_id\", F.col(\"adgroup_id\").cast(\"int\"))\n",
    "\n",
    "print(\"Rows:\",\n",
    "      \"user_df\", user_df.count(),\n",
    "      \"ad_df\", ad_df.count(),\n",
    "      \"click_df\", click_df.count(),\n",
    "      \"behavior_df\", behavior_df.count()\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 5) Fusion: Click + User + Ad (avoid duplicate columns)\n",
    "# ============================================================\n",
    "c = click_df.alias(\"c\")\n",
    "u = user_df.alias(\"u\")\n",
    "a = ad_df.alias(\"a\")\n",
    "\n",
    "click_cols = [F.col(f\"c.{x}\") for x in click_df.columns]\n",
    "user_cols_except_user = [F.col(f\"u.{x}\") for x in user_df.columns if x != \"user\"]\n",
    "\n",
    "click_user_df = (\n",
    "    c.join(u, F.col(\"c.user\") == F.col(\"u.user\"), how=\"left\")\n",
    "     .select(*click_cols, *user_cols_except_user)\n",
    ")\n",
    "\n",
    "cu = click_user_df.alias(\"cu\")\n",
    "cu_cols_except_adgroup = [F.col(f\"cu.{x}\") for x in click_user_df.columns if x != \"adgroup_id\"]\n",
    "ad_cols_except_key = [F.col(f\"a.{x}\") for x in ad_df.columns if x != \"adgroup_id\"]\n",
    "\n",
    "full_df = (\n",
    "    cu.join(a, F.col(\"cu.adgroup_id\") == F.col(\"a.adgroup_id\"), how=\"left\")\n",
    "      .select(\n",
    "          *cu_cols_except_adgroup,\n",
    "          F.col(\"cu.adgroup_id\").alias(\"adgroup_id\"),\n",
    "          *ad_cols_except_key\n",
    "      )\n",
    ")\n",
    "\n",
    "print(\"After Early Fusion:\",\n",
    "      \"rows:\", full_df.count(),\n",
    "      \"cols:\", len(full_df.columns)\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 6) Behavior Aggregation (Pivot)\n",
    "# Note: this aggregation is global per-user (not time-aware).\n",
    "# For training, prefer time-aware aggregations, but we use this as general signal.\n",
    "# ============================================================\n",
    "if \"btag\" in behavior_df.columns and \"user\" in behavior_df.columns:\n",
    "    behaviour_counts = (\n",
    "        behavior_df.groupBy(\"user\")\n",
    "        .pivot(\"btag\")\n",
    "        .agg(F.count(\"btag\"))\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    f = full_df.alias(\"f\")\n",
    "    b = behaviour_counts.alias(\"b\")\n",
    "    beh_cols_except_user = [F.col(f\"b.{x}\") for x in behaviour_counts.columns if x != \"user\"]\n",
    "\n",
    "    full_df = (\n",
    "        f.join(b, F.col(\"f.user\") == F.col(\"b.user\"), how=\"left\")\n",
    "         .select(*[F.col(f\"f.{x}\") for x in full_df.columns], *beh_cols_except_user)\n",
    "    )\n",
    "\n",
    "    for x in behaviour_counts.columns:\n",
    "        if x != \"user\" and x in full_df.columns:\n",
    "            full_df = full_df.fillna({x: 0})\n",
    "\n",
    "    print(\"After Hybrid Fusion (Behavior):\",\n",
    "          \"rows:\", full_df.count(),\n",
    "          \"cols:\", len(full_df.columns)\n",
    "    )\n",
    "else:\n",
    "    print(\"Warning: cannot perform Behavior Pivot (btag/user missing).\")\n",
    "\n",
    "# ============================================================\n",
    "# 7) Add label and time features (important for preprocessing)\n",
    "# ============================================================\n",
    "if \"clk\" in full_df.columns:\n",
    "    full_df = full_df.withColumn(\"label\", F.col(\"clk\").cast(\"int\"))\n",
    "else:\n",
    "    print(\"Warning: clk column missing - cannot compute CTR precisely.\")\n",
    "\n",
    "# time features\n",
    "if \"time_stamp\" in full_df.columns:\n",
    "    # hour of day (0-23) and day number\n",
    "    full_df = full_df.withColumn(\"hour\", (F.col(\"time_stamp\") / 3600).cast(\"long\") % 24)\n",
    "    full_df = full_df.withColumn(\"day\", (F.col(\"time_stamp\") / 86400).cast(\"long\"))\n",
    "else:\n",
    "    print(\"Warning: time_stamp missing - time-based visuals will be skipped.\")\n",
    "\n",
    "# ============================================================\n",
    "# 8) Compute join quality (Enrichment Rates)\n",
    "# ============================================================\n",
    "join_checks = []\n",
    "if \"age_level\" in full_df.columns:\n",
    "    join_checks.append((\"User join (age_level not null)\", \"age_level\"))\n",
    "if \"price\" in full_df.columns:\n",
    "    join_checks.append((\"Ad join (price not null)\", \"price\"))\n",
    "if \"cate_id\" in full_df.columns:\n",
    "    join_checks.append((\"Ad join (cate_id not null)\", \"cate_id\"))\n",
    "\n",
    "enrichment = []\n",
    "total_rows = full_df.count()\n",
    "\n",
    "for label_name, col_name in join_checks:\n",
    "    nn = full_df.filter(F.col(col_name).isNotNull()).count()\n",
    "    enrichment.append((label_name, nn / total_rows if total_rows else 0.0))\n",
    "\n",
    "print(\"\\nEnrichment rates after joins:\")\n",
    "for x in enrichment:\n",
    "    print(\"-\", x[0], \":\", round(x[1]*100, 2), \"%\")\n",
    "\n",
    "# ============================================================\n",
    "# 9) Take random sample for Pandas EDA (reduce memory use)\n",
    "# ============================================================\n",
    "sample_sdf = full_df.orderBy(F.rand(seed=SEED)).limit(SAMPLE_SIZE)\n",
    "sample_pdf = sample_sdf.toPandas()\n",
    "print(\"Sample size for EDA:\", sample_pdf.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 10) Helper plotting utils\n",
    "# ============================================================\n",
    "def save_fig(name):\n",
    "    path = os.path.join(fig_dir, name)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=FIG_DPI)\n",
    "    print(\"Saved figure:\", path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def safe_has_cols(pdf, cols):\n",
    "    return all(c in pdf.columns for c in cols)\n",
    "\n",
    "# ============================================================\n",
    "# 11) Visual 1: Class balance (Click vs No-Click)\n",
    "# ============================================================\n",
    "if \"label\" in sample_pdf.columns:\n",
    "    counts = sample_pdf[\"label\"].value_counts().sort_index()\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(counts.index.astype(str), counts.values)\n",
    "    plt.title(\"Target distribution: Click vs No-Click\")\n",
    "    plt.xlabel(\"label (0=No Click, 1=Click)\")\n",
    "    plt.ylabel(\"count\")\n",
    "    save_fig(\"01_label_distribution.png\")\n",
    "else:\n",
    "    print(\"Skipping: label missing.\")\n",
    "\n",
    "# ============================================================\n",
    "# 12) Visual 2: CTR over time (daily)\n",
    "# ============================================================\n",
    "if safe_has_cols(sample_pdf, [\"day\", \"label\"]):\n",
    "    tmp = sample_pdf.dropna(subset=[\"day\", \"label\"]).copy()\n",
    "    ctr_by_day = tmp.groupby(\"day\")[\"label\"].mean().reset_index()\n",
    "    ctr_by_day = ctr_by_day.sort_values(\"day\")\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(ctr_by_day[\"day\"], ctr_by_day[\"label\"])\n",
    "    plt.title(\"CTR over time (daily)\")\n",
    "    plt.xlabel(\"day (derived from time_stamp)\")\n",
    "    plt.ylabel(\"CTR = mean(label)\")\n",
    "    save_fig(\"02_ctr_over_time_day.png\")\n",
    "else:\n",
    "    print(\"Skipping: day/label missing.\")\n",
    "\n",
    "# ============================================================\n",
    "# 13) Visual 3: CTR by hour (Bar)\n",
    "# ============================================================\n",
    "if safe_has_cols(sample_pdf, [\"hour\", \"label\"]):\n",
    "    tmp = sample_pdf.dropna(subset=[\"hour\", \"label\"]).copy()\n",
    "    ctr_by_hour = tmp.groupby(\"hour\")[\"label\"].mean().reset_index().sort_values(\"hour\")\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(ctr_by_hour[\"hour\"].astype(int), ctr_by_hour[\"label\"])\n",
    "    plt.title(\"CTR by hour\")\n",
    "    plt.xlabel(\"hour (0-23)\")\n",
    "    plt.ylabel(\"CTR\")\n",
    "    save_fig(\"03_ctr_by_hour.png\")\n",
    "else:\n",
    "    print(\"Skipping: hour/label missing.\")\n",
    "\n",
    "# ============================================================\n",
    "# 14) Visual 4: Join enrichment (Bar)\n",
    "# ============================================================\n",
    "if enrichment:\n",
    "    names = [x[0] for x in enrichment]\n",
    "    vals = [x[1] for x in enrichment]\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.barh(names, vals)\n",
    "    plt.title(\"Join enrichment rates\")\n",
    "    plt.xlabel(\"Enrichment Rate\")\n",
    "    plt.xlim(0, 1)\n",
    "    save_fig(\"04_join_enrichment_rates.png\")\n",
    "\n",
    "# ============================================================\n",
    "# 15) Visual 5: CTR by cate_id (Top-N by count)\n",
    "# ============================================================\n",
    "def plot_ctr_topN(pdf, col_cat, top_n=TOP_N, fname=\"ctr_top.png\", title=\"\"):\n",
    "    if not safe_has_cols(pdf, [col_cat, \"label\"]):\n",
    "        print(f\"Skipping: {col_cat} or label missing.\")\n",
    "        return\n",
    "\n",
    "    tmp = pdf[[col_cat, \"label\"]].dropna()\n",
    "    # top categories by volume to avoid misleading small-sample rates\n",
    "    vc = tmp[col_cat].value_counts().head(top_n)\n",
    "    top_cats = set(vc.index.tolist())\n",
    "    tmp2 = tmp[tmp[col_cat].isin(top_cats)]\n",
    "\n",
    "    ctr = tmp2.groupby(col_cat)[\"label\"].mean()\n",
    "    cnt = tmp2.groupby(col_cat)[\"label\"].size()\n",
    "\n",
    "    out = pd.DataFrame({\"count\": cnt, \"ctr\": ctr}).reset_index()\n",
    "    out = out.sort_values(\"count\", ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    if HAS_SNS:\n",
    "        sns.barplot(data=out, x=col_cat, y=\"ctr\")\n",
    "    else:\n",
    "        plt.bar(out[col_cat].astype(str), out[\"ctr\"].values)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(col_cat)\n",
    "    plt.ylabel(\"CTR (mean label)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    save_fig(fname)\n",
    "\n",
    "if \"cate_id\" in sample_pdf.columns:\n",
    "    plot_ctr_topN(\n",
    "        sample_pdf,\n",
    "        \"cate_id\",\n",
    "        top_n=TOP_N,\n",
    "        fname=\"05_ctr_by_cate_topN.png\",\n",
    "        title=f\"CTR by cate_id (Top {TOP_N} by count)\"\n",
    "    )\n",
    "\n",
    "if \"brand\" in sample_pdf.columns:\n",
    "    plot_ctr_topN(\n",
    "        sample_pdf,\n",
    "        \"brand\",\n",
    "        top_n=TOP_N,\n",
    "        fname=\"06_ctr_by_brand_topN.png\",\n",
    "        title=f\"CTR by brand (Top {TOP_N} by count)\"\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# 16) Visual 6: price vs label (Boxplot)\n",
    "# ============================================================\n",
    "if safe_has_cols(sample_pdf, [\"price\", \"label\"]):\n",
    "    tmp = sample_pdf[[\"price\", \"label\"]].dropna()\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    if HAS_SNS:\n",
    "        sns.boxplot(x=\"label\", y=\"price\", data=tmp)\n",
    "    else:\n",
    "        # simple fallback without seaborn\n",
    "        g0 = tmp[tmp[\"label\"] == 0][\"price\"].values\n",
    "        g1 = tmp[tmp[\"label\"] == 1][\"price\"].values\n",
    "        plt.boxplot([g0, g1], labels=[\"0\", \"1\"])\n",
    "        plt.xlabel(\"label\")\n",
    "\n",
    "    plt.title(\"Price distribution by click\")\n",
    "    plt.ylabel(\"price\")\n",
    "    save_fig(\"07_price_vs_label_boxplot.png\")\n",
    "else:\n",
    "    print(\"Skipping: price/label missing.\")\n",
    "\n",
    "# ============================================================\n",
    "# 17) Visual 7: Missingness per column\n",
    "# ============================================================\n",
    "missing_rate = (sample_pdf.isna().mean().sort_values(ascending=False))\n",
    "missing_rate = missing_rate[missing_rate > 0]\n",
    "\n",
    "if len(missing_rate) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(missing_rate.index[::-1], missing_rate.values[::-1])\n",
    "    plt.title(\"Missingness rate per column\")\n",
    "    plt.xlabel(\"Missing Rate\")\n",
    "    save_fig(\"08_missingness_rates.png\")\n",
    "else:\n",
    "    print(\"No notable missing values in the sample.\")\n",
    "\n",
    "# ============================================================\n",
    "# 18) Visual 8: Correlation heatmap for numeric features\n",
    "# ============================================================\n",
    "numeric_cols = [c for c in sample_pdf.columns if pd.api.types.is_numeric_dtype(sample_pdf[c])]\n",
    "if len(numeric_cols) >= 2:\n",
    "    corr = sample_pdf[numeric_cols].corr()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    if HAS_SNS:\n",
    "        sns.heatmap(corr, annot=False, center=0, cmap=\"coolwarm\")\n",
    "    else:\n",
    "        plt.imshow(corr.values)\n",
    "        plt.colorbar()\n",
    "        plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "        plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "\n",
    "    plt.title(\"Correlation heatmap for numeric features\")\n",
    "    save_fig(\"09_numeric_correlation_heatmap.png\")\n",
    "else:\n",
    "    print(\"Skipping: not enough numeric columns for correlation.\")\n",
    "\n",
    "# ============================================================\n",
    "# 19) (optional) Save fused data + temporal train/test split\n",
    "# ============================================================\n",
    "if \"time_stamp\" in full_df.columns:\n",
    "    cutoff = full_df.approxQuantile(\"time_stamp\", [0.8], 0.001)[0]\n",
    "    train_df = full_df.filter(F.col(\"time_stamp\") <= cutoff)\n",
    "    test_df  = full_df.filter(F.col(\"time_stamp\") > cutoff)\n",
    "\n",
    "    print(\"\\nTemporal split 80/20:\")\n",
    "    print(\"cutoff time_stamp:\", cutoff)\n",
    "    print(\"train rows:\", train_df.count())\n",
    "    print(\"test rows :\", test_df.count())\n",
    "\n",
    "    # Save CSV (use Pandas to avoid Windows/Hadoop issues)\n",
    "    train_path = os.path.join(processed_dir, \"fused_train.csv\")\n",
    "    test_path = os.path.join(processed_dir, \"fused_test.csv\")\n",
    "\n",
    "    train_df.toPandas().to_csv(train_path, index=False)\n",
    "    test_df.toPandas().to_csv(test_path, index=False)\n",
    "    print(\"Saved:\", train_path)\n",
    "    print(\"Saved:\", test_path)\n",
    "else:\n",
    "    fused_path = os.path.join(processed_dir, \"fused_data.csv\")\n",
    "    full_df.toPandas().to_csv(fused_path, index=False)\n",
    "    print(\"Saved fused_data:\", fused_path)\n",
    "\n",
    "# ============================================================\n",
    "# 20) (optional) PR Curve + Score Distribution if processed training files exist\n",
    "# ============================================================\n",
    "try:\n",
    "    from scipy import sparse\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "    xtr_path = os.path.join(processed_dir, \"X_train_processed.npz\")\n",
    "    xte_path = os.path.join(processed_dir, \"X_test_processed.npz\")\n",
    "    ytr_path = os.path.join(processed_dir, \"y_train.csv\")\n",
    "    yte_path = os.path.join(processed_dir, \"y_test.csv\")\n",
    "\n",
    "    if all(os.path.exists(p) for p in [xtr_path, xte_path, ytr_path, yte_path]):\n",
    "        X_train = sparse.load_npz(xtr_path)\n",
    "        X_test  = sparse.load_npz(xte_path)\n",
    "        y_train = pd.read_csv(ytr_path).squeeze().astype(int)\n",
    "        y_test  = pd.read_csv(yte_path).squeeze().astype(int)\n",
    "\n",
    "        model = LogisticRegression(max_iter=3000, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        probas = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(y_test, probas)\n",
    "        ap = average_precision_score(y_test, probas)\n",
    "\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        plt.plot(recall, precision)\n",
    "        plt.title(f\"Precision-Recall Curve (Baseline LR) | AP={ap:.4f}\")\n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        save_fig(\"10_precision_recall_curve.png\")\n",
    "\n",
    "        # score distribution for pos vs neg\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.hist(probas[y_test == 0], bins=50, alpha=0.7, label=\"No Click (0)\")\n",
    "        plt.hist(probas[y_test == 1], bins=50, alpha=0.7, label=\"Click (1)\")\n",
    "        plt.title(\"Predicted probability distribution (Baseline LR)\")\n",
    "        plt.xlabel(\"Predicted Probability\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.legend()\n",
    "        save_fig(\"11_score_distribution.png\")\n",
    "\n",
    "        print(\"PR Curve and Score Distribution created (optional).\")\n",
    "    else:\n",
    "        print(\"Skipping PR Curve: processed npz/y files missing.\")\n",
    "except Exception as e:\n",
    "    print(\"Skipping PR Curve due to error:\", str(e))\n",
    "\n",
    "# ============================================================\n",
    "# 21) Stop Spark\n",
    "# ============================================================\n",
    "spark.stop()\n",
    "print(\"Spark stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
